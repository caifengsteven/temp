import pandas as pd
import numpy as np
import datetime as dt
import pdblp
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVR
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
import re
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import os
import pickle
from scipy.sparse import csr_matrix, hstack
warnings.filterwarnings('ignore')

# Download NLTK resources if not already downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class FactorizationMachine:
    """
    Implementation of a Factorization Machine model with configurable order and factors
    Based on the paper "Exploiting social media with higher-order Factorization Machines"
    """
    
    def __init__(self, d=3, k_values=None, learning_rate=0.01, 
                 regularization=0.01, epochs=100, random_state=42):
        """
        Initialize the FM model
        
        Parameters:
        -----------
        d : int
            Highest order of interactions to model
        k_values : list
            List of integers representing the number of factors for each order
            e.g., [1, 5, 3] means linear weights, 5 factors for 2nd order, 3 factors for 3rd order
        learning_rate : float
            Learning rate for stochastic gradient descent
        regularization : float
            Regularization parameter
        epochs : int
            Number of training epochs
        random_state : int
            Random seed for reproducibility
        """
        self.d = d
        
        # Default k values if not provided
        if k_values is None:
            self.k_values = [1] + [5] * (d - 1)
        else:
            if len(k_values) != d:
                raise ValueError(f"k_values must have length {d}, got {len(k_values)}")
            self.k_values = k_values
        
        self.learning_rate = learning_rate
        self.regularization = regularization
        self.epochs = epochs
        self.random_state = random_state
        np.random.seed(random_state)
        
        # Model parameters to be learned
        self.w0 = None    # Global bias
        self.weights = []  # Weights for each order
    
    def _init_model_parameters(self, n_features):
        """Initialize model parameters"""
        self.w0 = 0.0
        
        # Initialize weights for each order
        self.weights = []
        for l in range(1, self.d + 1):
            if l == 1 and self.k_values[l-1] == 1:
                # Linear weights
                w_l = np.zeros(n_features)
            else:
                # Latent factors for interactions
                w_l = np.random.normal(0, 0.1, (n_features, self.k_values[l-1]))
            self.weights.append(w_l)
    
    def fit(self, X, y):
        """
        Fit the FM model to the training data
        
        Parameters:
        -----------
        X : array-like or sparse matrix, shape = [n_samples, n_features]
            Training data
        y : array-like, shape = [n_samples]
            Target values
        """
        # Convert input to appropriate format if needed
        if isinstance(X, pd.DataFrame):
            X = X.values
        elif isinstance(X, csr_matrix):
            X = X
        else:
            X = np.array(X)
        
        n_samples, n_features = X.shape
        
        # Initialize model parameters
        self._init_model_parameters(n_features)
        
        # Training loop
        for epoch in range(self.epochs):
            # Shuffle data
            idx = np.random.permutation(n_samples)
            X_shuffled = X[idx]
            y_shuffled = y[idx]
            
            # Stochastic gradient descent
            for i in range(n_samples):
                x_i = X_shuffled[i]
                
                # Sparse data handling
                if isinstance(X, csr_matrix):
                    indices = X[i].indices
                    x_i = np.zeros(n_features)
                    x_i[indices] = X[i].data
                
                # Calculate prediction for this sample
                y_pred = self.predict_single(x_i)
                
                # Calculate error
                error = y_pred - y_shuffled[i]
                
                # Update global bias
                self.w0 -= self.learning_rate * (error + self.regularization * self.w0)
                
                # Update weights for each order
                for l in range(1, self.d + 1):
                    if l == 1 and self.k_values[l-1] == 1:
                        # Update linear weights
                        for j in range(n_features):
                            if x_i[j] != 0:  # Skip zero features for efficiency
                                self.weights[l-1][j] -= self.learning_rate * (error * x_i[j] + 
                                                                          self.regularization * self.weights[l-1][j])
                    else:
                        # Update interaction weights
                        for j in range(n_features):
                            if x_i[j] != 0:  # Skip zero features for efficiency
                                for f in range(self.k_values[l-1]):
                                    # Calculate gradient for this factor
                                    sum_term = self._calculate_factor_gradient(x_i, j, l, f)
                                    
                                    # Update factor
                                    self.weights[l-1][j, f] -= self.learning_rate * (error * sum_term * x_i[j] + 
                                                                                  self.regularization * self.weights[l-1][j, f])
    
    def _calculate_factor_gradient(self, x, j, order, factor):
        """Helper function to calculate gradient for a specific factor"""
        # This is a simplified implementation for demonstration
        # For a full implementation, see the paper reference
        
        if order == 2:
            # Second-order gradient
            sum_term = 0
            for j2 in range(len(x)):
                if j2 != j and x[j2] != 0:
                    sum_term += x[j2] * self.weights[order-1][j2, factor]
            return sum_term
        else:
            # Higher-order gradient - simplified approximation
            return np.sum([x_j * self.weights[order-1][j_idx, factor] 
                           for j_idx, x_j in enumerate(x) if j_idx != j and x_j != 0])
    
    def predict(self, X):
        """
        Predict using the FM model
        
        Parameters:
        -----------
        X : array-like or sparse matrix, shape = [n_samples, n_features]
            Samples
            
        Returns:
        --------
        y_pred : array, shape = [n_samples]
            Predicted target values
        """
        if isinstance(X, pd.DataFrame):
            X = X.values
        elif isinstance(X, csr_matrix):
            X = X
        else:
            X = np.array(X)
        
        n_samples = X.shape[0]
        y_pred = np.zeros(n_samples)
        
        for i in range(n_samples):
            x_i = X[i]
            
            # Sparse data handling
            if isinstance(X, csr_matrix):
                indices = X[i].indices
                x_i = np.zeros(X.shape[1])
                x_i[indices] = X[i].data
            
            y_pred[i] = self.predict_single(x_i)
        
        return y_pred
    
    def predict_single(self, x):
        """Predict for a single sample x"""
        # Start with global bias
        pred = self.w0
        
        # Add prediction components for each order
        for l in range(1, self.d + 1):
            if l == 1 and self.k_values[l-1] == 1:
                # Linear terms
                linear_term = np.sum(x * self.weights[l-1])
                pred += linear_term
            else:
                # Interaction terms - simplified for demonstration
                interaction_term = self._calculate_interaction_term(x, l)
                pred += interaction_term
        
        return pred
    
    def _calculate_interaction_term(self, x, order):
        """Calculate interaction term for a specific order"""
        # Simplified implementation for demonstration
        # For a full implementation, see the paper reference
        
        if order == 2:
            # Second-order interactions
            sum_term = 0
            for f in range(self.k_values[order-1]):
                factor_sum = 0
                for j in range(len(x)):
                    if x[j] != 0:
                        factor_sum += x[j] * self.weights[order-1][j, f]
                sum_term += factor_sum ** 2
            return 0.5 * sum_term
        else:
            # Higher-order interactions - simplified approximation
            return np.sum([np.sum(x[j] * self.weights[order-1][j, f] for j in range(len(x)) if x[j] != 0) ** order
                           for f in range(self.k_values[order-1])]) / order
    
    def save(self, filepath):
        """Save the model to a file"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'd': self.d,
                'k_values': self.k_values,
                'w0': self.w0,
                'weights': self.weights,
                'learning_rate': self.learning_rate,
                'regularization': self.regularization,
                'epochs': self.epochs,
                'random_state': self.random_state
            }, f)
    
    @classmethod
    def load(cls, filepath):
        """Load a model from a file"""
        with open(filepath, 'rb') as f:
            model_dict = pickle.load(f)
        
        model = cls(
            d=model_dict['d'],
            k_values=model_dict['k_values'],
            learning_rate=model_dict['learning_rate'],
            regularization=model_dict['regularization'],
            epochs=model_dict['epochs'],
            random_state=model_dict['random_state']
        )
        
        model.w0 = model_dict['w0']
        model.weights = model_dict['weights']
        
        return model


class AdaptiveOrderFM:
    """
    Implementation of the Adaptive-Order algorithm for Factorization Machines
    Based on Algorithm 1 in the paper
    """
    
    def __init__(self, max_order=8, max_factors_per_order=20, 
                 learning_rate=0.01, regularization=0.01, epochs=100, 
                 random_state=42):
        """
        Initialize the adaptive-order FM
        
        Parameters:
        -----------
        max_order : int
            Maximum order to consider
        max_factors_per_order : int
            Maximum number of factors per order to consider
        learning_rate : float
            Learning rate for SGD
        regularization : float
            Regularization parameter
        epochs : int
            Number of training epochs
        random_state : int
            Random seed
        """
        self.max_order = max_order
        self.max_factors_per_order = max_factors_per_order
        self.learning_rate = learning_rate
        self.regularization = regularization
        self.epochs = epochs
        self.random_state = random_state
        self.best_model = None
    
    def fit(self, X, y):
        """
        Fit the adaptive-order FM to the data
        
        Parameters:
        -----------
        X : array-like or sparse matrix, shape = [n_samples, n_features]
            Training data
        y : array-like, shape = [n_samples]
            Target values
        """
        print("Step A: Determining highest order and ratio between values of k...")
        
        # Step A: Determine highest order d and ratio between values of k
        d = 1
        e1 = self._eval_a(X, y, [1])  # FM(1) = SVM with linear kernel
        
        best_order_error = e1
        best_order = d
        
        while d < self.max_order:
            # Create model with one factor at the next higher order
            k_values = [0] * (d + 1)
            k_values[0] = 1  # Always include linear weights
            k_values[d] = 1  # One factor at the highest order
            
            # Evaluate model
            ed_next = self._eval_a(X, y, k_values)
            
            # Check if adding higher order improves performance
            if d > 1 and best_order_error < ed_next:
                break
            
            if ed_next < best_order_error:
                best_order_error = ed_next
                best_order = d + 1
            
            d += 1
        
        d = max(3, best_order)  # Ensure d >= 3 as per the paper
        print(f"Highest order determined: {d}")
        
        # Calculate relevance scores
        relevance_scores = []
        for l in range(2, d + 1):
            # Create model with one factor at order l
            k_values = [0] * (d + 1)
            k_values[0] = 1  # Linear weights
            k_values[l - 1] = 1  # One factor at order l
            
            # Evaluate model
            el = self._eval_a(X, y, k_values[:d])
            
            # Calculate relevance score
            r_l = max(el - e1, 0)
            relevance_scores.append(r_l)
        
        # Square relevance scores to increase spread of ratios
        relevance_scores = [r**2 for r in relevance_scores]
        
        print("Step B: Determining h and values of k...")
        
        # Step B: Determine h and values of k
        h = 1
        sum_relevance = sum(relevance_scores) if sum(relevance_scores) > 0 else 1
        
        # Initial k values
        k_values = [1]  # Linear weights
        for l in range(2, d + 1):
            k_l = max(int(relevance_scores[l-2] / sum_relevance * h + 0.5), 1)
            k_values.append(k_l)
        
        best_rmse = self._eval_b(X, y, k_values)
        best_k_values = k_values[:]
        
        while h <= self.max_factors_per_order:
            h = h * 2  # Multiply by 2 to spread number of model parameters
            
            # Calculate new k values
            k_values_new = [1]  # Linear weights
            for l in range(2, d + 1):
                k_l = max(int(relevance_scores[l-2] / sum_relevance * h + 0.5), 1)
                k_values_new.append(k_l)
            
            # Evaluate model with new k values
            current_rmse = self._eval_b(X, y, k_values_new)
            
            # Check if increasing factors improves performance
            if best_rmse <= current_rmse:
                break
            
            best_rmse = current_rmse
            best_k_values = k_values_new[:]
        
        print(f"Best k values determined: {best_k_values}")
        
        # Step C: Determine final model parameters
        print("Step C: Training final model...")
        self.best_model = FactorizationMachine(
            d=len(best_k_values),
            k_values=best_k_values,
            learning_rate=self.learning_rate,
            regularization=self.regularization,
            epochs=self.epochs,
            random_state=self.random_state
        )
        self.best_model.fit(X, y)
        
        return self
    
    def _eval_a(self, X, y, k_values):
        """
        Evaluation function for Step A
        Returns median RMSE from 10-fold cross validation
        """
        # Prepare 10 disjoint, equally-sized subsets
        n_samples = X.shape[0]
        fold_size = n_samples // 10
        np.random.seed(self.random_state)
        indices = np.random.permutation(n_samples)
        
        rmse_values = []
        
        for i in range(10):
            # Create validation subset
            start_idx = i * fold_size
            end_idx = (i + 1) * fold_size if i < 9 else n_samples
            val_indices = indices[start_idx:end_idx]
            train_indices = np.concatenate([indices[:start_idx], indices[end_idx:]])
            
            # Split data
            X_train = X[train_indices]
            y_train = y[train_indices]
            X_val = X[val_indices]
            y_val = y[val_indices]
            
            # Train model
            model = FactorizationMachine(
                d=len(k_values),
                k_values=k_values,
                learning_rate=self.learning_rate,
                regularization=self.regularization,
                epochs=self.epochs // 2,  # Fewer epochs for cross-validation
                random_state=self.random_state + i
            )
            model.fit(X_train, y_train)
            
            # Predict and calculate RMSE
            y_pred = model.predict(X_val)
            rmse = np.sqrt(np.mean((y_val - y_pred) ** 2))
            rmse_values.append(rmse)
        
        # Return median RMSE
        return np.median(rmse_values)
    
    def _eval_b(self, X, y, k_values):
        """
        Evaluation function for Step B
        Returns RMSE from 80/20 validation
        """
        # 80/20 split
        n_samples = X.shape[0]
        n_train = int(0.8 * n_samples)
        np.random.seed(self.random_state)
        indices = np.random.permutation(n_samples)
        
        train_indices = indices[:n_train]
        val_indices = indices[n_train:]
        
        # Split data
        X_train = X[train_indices]
        y_train = y[train_indices]
        X_val = X[val_indices]
        y_val = y[val_indices]
        
        # Train model
        model = FactorizationMachine(
            d=len(k_values),
            k_values=k_values,
            learning_rate=self.learning_rate,
            regularization=self.regularization,
            epochs=self.epochs // 2,  # Fewer epochs for faster evaluation
            random_state=self.random_state
        )
        model.fit(X_train, y_train)
        
        # Predict and calculate RMSE
        y_pred = model.predict(X_val)
        rmse = np.sqrt(np.mean((y_val - y_pred) ** 2))
        
        return rmse
    
    def predict(self, X):
        """
        Predict using the best model
        
        Parameters:
        -----------
        X : array-like or sparse matrix, shape = [n_samples, n_features]
            Samples
            
        Returns:
        --------
        y_pred : array, shape = [n_samples]
            Predicted target values
        """
        if self.best_model is None:
            raise ValueError("Model not fitted yet. Call fit() first.")
        
        return self.best_model.predict(X)
    
    def save(self, filepath):
        """Save the model to a file"""
        if self.best_model is None:
            raise ValueError("Model not fitted yet. Call fit() first.")
        
        self.best_model.save(filepath)
    
    @classmethod
    def load(cls, filepath):
        """Load a model from a file"""
        model = cls()
        model.best_model = FactorizationMachine.load(filepath)
        return model


class SVMModel:
    """Simple wrapper for LinearSVR to match FM interface"""
    
    def __init__(self, learning_rate=0.01, regularization=0.01, epochs=100, random_state=42):
        self.model = LinearSVR(
            epsilon=0.0,
            tol=1e-4,
            C=1.0/regularization,
            random_state=random_state,
            max_iter=epochs
        )
    
    def fit(self, X, y):
        self.model.fit(X, y)
        return self
    
    def predict(self, X):
        return self.model.predict(X)
    
    def save(self, filepath):
        with open(filepath, 'wb') as f:
            pickle.dump(self.model, f)
    
    @classmethod
    def load(cls, filepath):
        model = cls()
        with open(filepath, 'rb') as f:
            model.model = pickle.load(f)
        return model


class TweetProcessor:
    """Process tweets for use in the FM model"""
    
    def __init__(self):
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))
        self.vectorizer = None
    
    def preprocess_tweet(self, text):
        """Preprocess a single tweet"""
        # Convert to lowercase
        text = text.lower()
        
        # Remove URLs, usernames, punctuation, numbers
        text = re.sub(r'http\S+', '', text)  # Remove URLs
        text = re.sub(r'@\S+', '', text)  # Remove usernames
        text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
        text = re.sub(r'\d+', '', text)  # Remove numbers
        
        # Remove stop words and stem words
        words = nltk.word_tokenize(text)
        words = [self.stemmer.stem(word) for word in words if word not in self.stop_words]
        
        return ' '.join(words)
    
    def process_tweets(self, tweets_df):
        """Process a DataFrame of tweets"""
        # Extract relevant columns
        processed_tweets = []
        
        for _, row in tweets_df.iterrows():
            # Check if tweet is in English
            if 'lang' in row and row['lang'] != 'en':
                continue
            
            # Preprocess text
            processed_text = self.preprocess_tweet(row['text'])
            
            # Add to processed tweets
            processed_tweets.append({
                'timestamp': row['timestamp'],
                'text': processed_text,
                'ticker': row['ticker'] if 'ticker' in row else None
            })
        
        return pd.DataFrame(processed_tweets)
    
    def fit_vectorizer(self, texts, max_features=10000):
        """Fit vectorizer on texts"""
        self.vectorizer = CountVectorizer(
            binary=True,  # Binary weights
            max_features=max_features
        )
        self.vectorizer.fit(texts)
        return self
    
    def transform_tweets(self, texts):
        """Transform texts to feature matrix"""
        if self.vectorizer is None:
            raise ValueError("Vectorizer not fitted yet. Call fit_vectorizer() first.")
        
        # Transform texts to document-term matrix
        X = self.vectorizer.transform(texts)
        return X
    
    def get_feature_names(self):
        """Get feature names from vectorizer"""
        if self.vectorizer is None:
            raise ValueError("Vectorizer not fitted yet. Call fit_vectorizer() first.")
        
        return self.vectorizer.get_feature_names_out()
    
    def save(self, filepath):
        """Save the vectorizer to a file"""
        if self.vectorizer is None:
            raise ValueError("Vectorizer not fitted yet. Call fit_vectorizer() first.")
        
        with open(filepath, 'wb') as f:
            pickle.dump(self.vectorizer, f)
    
    @classmethod
    def load(cls, filepath):
        """Load a vectorizer from a file"""
        processor = cls()
        with open(filepath, 'rb') as f:
            processor.vectorizer = pickle.load(f)
        return processor


class FactorizationMachineStrategy:
    """
    Implementation of the trading strategy using Factorization Machines
    """
    
    def __init__(self, formation_days=40, top_stocks=5, model_type='AFM',
                 window_size=10, bollinger_std=2, min_tweets=80,
                 transaction_cost=0.0002):
        """
        Initialize the trading strategy
        
        Parameters:
        -----------
        formation_days : int
            Number of days in the formation period
        top_stocks : int
            Number of top stocks to trade
        model_type : str
            Type of model to use ('AFM', 'TFM', 'SFM', or 'SVM')
        window_size : int
            Window size for Bollinger Bands (in minutes)
        bollinger_std : float
            Number of standard deviations for Bollinger Bands
        min_tweets : int
            Minimum number of tweets required for a stock to be considered
        transaction_cost : float
            Transaction cost per share per half turn
        """
        self.formation_days = formation_days
        self.top_stocks = top_stocks
        self.model_type = model_type
        self.window_size = window_size
        self.bollinger_std = bollinger_std
        self.min_tweets = min_tweets
        self.transaction_cost = transaction_cost
        
        # Initialize Bloomberg connection
        self.conn = None
        
        # Initialize components
        self.tweet_processor = TweetProcessor()
        self.models = {}  # Dictionary to store models for each stock
        self.top_stock_list = []  # List of top stocks to trade
        
        # Storage for results
        self.daily_returns = []
        self.transactions = []
    
    def connect_to_bloomberg(self):
        """Connect to Bloomberg"""
        print("Connecting to Bloomberg...")
        try:
            self.conn = pdblp.BCon(timeout=60000)  # 60-second timeout
            self.conn.start()
            print("Connected to Bloomberg")
            return True
        except Exception as e:
            print(f"Failed to connect to Bloomberg: {e}")
            return False
    
    def get_bloomberg_data(self, tickers, fields, start_date, end_date, interval=1):
        """
        Get data from Bloomberg
        
        Parameters:
        -----------
        tickers : list
            List of ticker symbols
        fields : list
            List of fields to request
        start_date : str
            Start date in format 'YYYYMMDD'
        end_date : str
            End date in format 'YYYYMMDD'
        interval : int
            Data interval in minutes
            
        Returns:
        --------
        data : DataFrame
            Bloomberg data
        """
        if self.conn is None:
            if not self.connect_to_bloomberg():
                return None
        
        try:
            if interval > 1:
                # Get daily data
                data = self.conn.bdh(tickers, fields, start_date, end_date)
            else:
                # Get intraday data
                data = self.conn.bdib(tickers, fields, start_date, end_date, interval=1)
            
            return data
        except Exception as e:
            print(f"Error getting Bloomberg data: {e}")
            return None
    
    def get_sp500_constituents(self, date):
        """
        Get S&P 500 constituents for a given date
        
        Parameters:
        -----------
        date : str
            Date in format 'YYYYMMDD'
            
        Returns:
        --------
        constituents : list
            List of S&P 500 constituents
        """
        # This is a simplified approach - in practice, you would need
        # a database of historical S&P 500 constituents for each date
        
        # For demonstration, we'll get the current constituents
        if self.conn is None:
            if not self.connect_to_bloomberg():
                return []
        
        try:
            # Get index members
            data = self.conn.ref(['SPX Index'], 'INDX_MEMBERS')
            constituents = data['value'][0]
            return constituents
        except Exception as e:
            print(f"Error getting S&P 500 constituents: {e}")
            return []
    
    def get_minute_data(self, tickers, date):
        """
        Get minute-by-minute price data for a list of tickers
        
        Parameters:
        -----------
        tickers : list
            List of ticker symbols
        date : str
            Date in format 'YYYYMMDD'
            
        Returns:
        --------
        prices : DataFrame
            Minute-by-minute prices
        """
        if self.conn is None:
            if not self.connect_to_bloomberg():
                return None
        
        try:
            # Format date for Bloomberg
            start_datetime = f"{date} 09:30:00"
            end_datetime = f"{date} 16:00:00"
            
            # Get intraday bar data
            data = self.get_bloomberg_data(tickers, ['TRADE'], start_datetime, end_datetime)
            
            # Process data
            if data is not None and not data.empty:
                # Pivot to get clean minute-by-minute prices
                prices = data['close'].unstack(level=0)
                
                # Forward fill missing values
                prices = prices.fillna(method='ffill')
                
                return prices
            else:
                print(f"No minute data retrieved for {date}")
                return None
        except Exception as e:
            print(f"Error getting minute data: {e}")
            return None
    
    def get_tweets(self, start_date, end_date, tickers=None):
        """
        Get tweets for a given time period
        
        Parameters:
        -----------
        start_date : str
            Start date in format 'YYYYMMDD'
        end_date : str
            End date in format 'YYYYMMDD'
        tickers : list or None
            List of ticker symbols to filter tweets by
            
        Returns:
        --------
        tweets : DataFrame
            DataFrame of tweets
        """
        # In a real implementation, you would query a Twitter API or database
        # For demonstration, we'll simulate some tweets
        
        print(f"Getting tweets from {start_date} to {end_date}...")
        
        # Convert dates to datetime
        start_dt = dt.datetime.strptime(start_date, "%Y%m%d")
        end_dt = dt.datetime.strptime(end_date, "%Y%m%d")
        
        # Create date range
        dates = [start_dt + dt.timedelta(days=i) for i in range((end_dt - start_dt).days + 1)]
        
        # Generate sample tweets
        tweets = []
        
        # Common financial terms for simulation
        financial_terms = [
            "buy", "sell", "hold", "upgrade", "downgrade", "earnings", "revenue", 
            "profit", "loss", "dividend", "growth", "decline", "beat", "miss",
            "target", "price", "rating", "bullish", "bearish", "analyst", 
            "report", "guidance", "forecast", "stock", "market", "trading"
        ]
        
        # If no tickers provided, use a default set
        if tickers is None or len(tickers) == 0:
            tickers = ["AAPL", "MSFT", "AMZN", "GOOGL", "META", "TSLA", "NVDA", "JPM"]
        
        # For each date
        for date in dates:
            # Only consider trading days
            if date.weekday() >= 5:  # Skip weekends
                continue
            
            # Generate 200-500 tweets per day
            n_tweets = np.random.randint(200, 500)
            
            for _ in range(n_tweets):
                # Random ticker
                ticker = np.random.choice(tickers)
                
                # Random time during trading hours
                hour = np.random.randint(9, 16)
                minute = np.random.randint(0, 60)
                if hour == 9 and minute < 30:
                    minute = np.random.randint(30, 60)
                if hour == 16:
                    minute = 0
                
                timestamp = date.replace(hour=hour, minute=minute)
                
                # Random tweet text
                n_terms = np.random.randint(3, 8)
                terms = np.random.choice(financial_terms, n_terms, replace=True)
                text = f"{ticker} {' '.join(terms)}"
                
                # Add some randomness to text
                sentiment = np.random.choice(["positive", "negative", "neutral"], p=[0.4, 0.3, 0.3])
                if sentiment == "positive":
                    text += " great opportunity bullish"
                elif sentiment == "negative":
                    text += " concerning bearish"
                
                # Add to tweets
                tweets.append({
                    "timestamp": timestamp,
                    "text": text,
                    "ticker": ticker,
                    "lang": "en"
                })
        
        return pd.DataFrame(tweets)
    
    def prepare_formation_data(self, tweets_df, prices_df, target_minutes=20):
        """
        Prepare data for the formation period
        
        Parameters:
        -----------
        tweets_df : DataFrame
            DataFrame of tweets
        prices_df : DataFrame
            DataFrame of minute-by-minute prices
        target_minutes : int
            Number of minutes ahead to predict
            
        Returns:
        --------
        X : sparse matrix
            Document-term matrix
        y : array
            Target returns
        tickers : list
            List of tickers corresponding to each tweet
        timestamps : list
            List of timestamps corresponding to each tweet
        """
        # Process tweets
        processed_tweets = self.tweet_processor.process_tweets(tweets_df)
        
        # Fit vectorizer if needed
        if self.tweet_processor.vectorizer is None:
            self.tweet_processor.fit_vectorizer(processed_tweets['text'])
        
        # Transform tweets to document-term matrix
        X = self.tweet_processor.transform_tweets(processed_tweets['text'])
        
        # Calculate target returns
        y = []
        tickers = []
        timestamps = []
        
        for _, row in processed_tweets.iterrows():
            ticker = row['ticker']
            timestamp = row['timestamp']
            
            if ticker is None or ticker not in prices_df.columns:
                continue
            
            # Get current price
            if timestamp not in prices_df.index:
                continue
            current_price = prices_df.loc[timestamp, ticker]
            
            # Get future price
            future_timestamp = timestamp + dt.timedelta(minutes=target_minutes)
            if future_timestamp not in prices_df.index:
                continue
            future_price = prices_df.loc[future_timestamp, ticker]
            
            # Calculate return
            target_return = (future_price - current_price) / current_price
            
            # Add to target list
            y.append(target_return)
            tickers.append(ticker)
            timestamps.append(timestamp)
        
        # Filter X to only include rows with valid targets
        X = X[range(len(y))]
        
        return X, np.array(y), tickers, timestamps
    
    def train_models(self, X, y, tickers):
        """
        Train models for each ticker
        
        Parameters:
        -----------
        X : sparse matrix
            Document-term matrix
        y : array
            Target returns
        tickers : list
            List of tickers corresponding to each sample
            
        Returns:
        --------
        root_relative_squared_errors : dict
            Dictionary of root relative squared errors for each ticker
        """
        # Group data by ticker
        ticker_data = {}
        for i in range(len(tickers)):
            ticker = tickers[i]
            if ticker not in ticker_data:
                ticker_data[ticker] = {'X_indices': [], 'y': []}
            
            ticker_data[ticker]['X_indices'].append(i)
            ticker_data[ticker]['y'].append(y[i])
        
        # Train models for each ticker with enough data
        root_relative_squared_errors = {}
        
        for ticker, data in ticker_data.items():
            # Check if enough tweets
            if len(data['X_indices']) < self.min_tweets:
                continue
            
            print(f"Training model for {ticker} with {len(data['X_indices'])} tweets...")
            
            # Get data for this ticker
            X_ticker = X[data['X_indices']]
            y_ticker = np.array(data['y'])
            
            # Split into train and validation sets
            n_samples = len(data['X_indices'])
            n_train = int(0.8 * n_samples)
            
            indices = np.random.permutation(n_samples)
            train_idx = indices[:n_train]
            val_idx = indices[n_train:]
            
            X_train, X_val = X_ticker[train_idx], X_ticker[val_idx]
            y_train, y_val = y_ticker[train_idx], y_ticker[val_idx]
            
            # Train model based on type
            if self.model_type == 'AFM':
                model = AdaptiveOrderFM(
                    max_order=8,
                    max_factors_per_order=20,
                    learning_rate=0.01,
                    regularization=0.01,
                    epochs=100,
                    random_state=42
                )
            elif self.model_type == 'TFM':
                model = FactorizationMachine(
                    d=3,
                    k_values=[1, 1, 1],
                    learning_rate=0.01,
                    regularization=0.01,
                    epochs=100,
                    random_state=42
                )
            elif self.model_type == 'SFM':
                model = FactorizationMachine(
                    d=2,
                    k_values=[1, 1],
                    learning_rate=0.01,
                    regularization=0.01,
                    epochs=100,
                    random_state=42
                )
            else:  # SVM
                model = SVMModel(
                    learning_rate=0.01,
                    regularization=0.01,
                    epochs=100,
                    random_state=42
                )
            
            # Fit model
            model.fit(X_train, y_train)
            
            # Predict on validation set
            y_pred = model.predict(X_val)
            
            # Calculate root relative squared error
            mse = np.mean((y_val - y_pred) ** 2)
            rmse = np.sqrt(mse)
            
            # Relative squared error
            y_mean = np.mean(y_val)
            relative_squared_error = np.sum((y_val - y_pred) ** 2) / np.sum((y_val - y_mean) ** 2)
            root_relative_squared_error = np.sqrt(relative_squared_error)
            
            # Store model and error
            self.models[ticker] = model
            root_relative_squared_errors[ticker] = root_relative_squared_error
        
        return root_relative_squared_errors
    
    def select_top_stocks(self, root_relative_squared_errors):
        """
        Select top stocks based on root relative squared error
        
        Parameters:
        -----------
        root_relative_squared_errors : dict
            Dictionary of root relative squared errors for each ticker
            
        Returns:
        --------
        top_stocks : list
            List of top stock tickers
        """
        # Sort stocks by error (lower is better)
        sorted_stocks = sorted(root_relative_squared_errors.items(), key=lambda x: x[1])
        
        # Select top stocks
        top_stocks = [ticker for ticker, error in sorted_stocks[:self.top_stocks]]
        
        return top_stocks
    
    def trade_day(self, date, prices_df, tweets_df):
        """
        Execute trading strategy for a single day
        
        Parameters:
        -----------
        date : str
            Date in format 'YYYYMMDD'
        prices_df : DataFrame
            DataFrame of minute-by-minute prices
        tweets_df : DataFrame
            DataFrame of tweets
            
        Returns:
        --------
        daily_return : float
            Return for the day
        transactions : list
            List of transactions
        """
        print(f"Trading for {date}...")
        
        # Process tweets for the day
        processed_tweets = self.tweet_processor.process_tweets(tweets_df)
        
        # Initialize storage for day's transactions and position
        day_transactions = []
        positions = {}
        open_trades = {}
        
        # For each minute
        for timestamp, minute_data in prices_df.iterrows():
            # Get tweets for this minute
            minute_tweets = processed_tweets[processed_tweets['timestamp'] == timestamp]
            
            # For each target stock
            for ticker in self.top_stock_list:
                if ticker not in prices_df.columns:
                    continue
                
                # Get tweets for this stock at this minute
                stock_tweets = minute_tweets[minute_tweets['ticker'] == ticker]
                
                if len(stock_tweets) == 0:
                    continue
                
                # Make prediction for each tweet
                predictions = []
                
                for _, tweet in stock_tweets.iterrows():
                    # Transform tweet to feature vector
                    X = self.tweet_processor.transform_tweets([tweet['text']])
                    
                    # Predict return
                    if ticker in self.models:
                        prediction = self.models[ticker].predict(X)[0]
                        predictions.append(prediction)
                
                if len(predictions) == 0:
                    continue
                
                # Average predictions
                avg_prediction = np.mean(predictions)
                
                # Calculate Bollinger Bands
                # Get historical returns
                historical_returns = []
                
                for i in range(self.window_size):
                    if timestamp - dt.timedelta(minutes=i+1) in prices_df.index:
                        prev_price = prices_df.loc[timestamp - dt.timedelta(minutes=i+1), ticker]
                        curr_price = prices_df.loc[timestamp - dt.timedelta(minutes=i), ticker]
                        hist_return = (curr_price - prev_price) / prev_price
                        historical_returns.append(hist_return)
                
                if len(historical_returns) < self.window_size:
                    continue
                
                # Calculate mean and standard deviation
                mean_return = np.mean(historical_returns)
                std_return = np.std(historical_returns)
                
                # Bollinger Bands
                upper_band = mean_return + self.bollinger_std * std_return
                lower_band = mean_return - self.bollinger_std * std_return
                
                # Sum of historical returns and prediction
                sum_returns = sum(historical_returns) + avg_prediction
                
                # Check if we should open a trade
                # Only trade if between 9:30 AM and 3:40 PM
                trading_hours = timestamp.time() >= dt.time(9, 30) and timestamp.time() <= dt.time(15, 40)
                
                if trading_hours:
                    # Check for entry signals
                    if sum_returns > upper_band:  # Stock is undervalued
                        # Long position
                        if ticker not in positions:
                            positions[ticker] = 0
                        
                        # Open trade if not already open
                        if ticker not in open_trades:
                            # Buy 1 USD worth of stock
                            price = prices_df.loc[timestamp, ticker]
                            shares = 1.0 / price
                            positions[ticker] += shares
                            
                            # Record trade
                            open_trades[ticker] = {
                                'entry_time': timestamp,
                                'entry_price': price,
                                'shares': shares,
                                'direction': 'long'
                            }
                            
                            day_transactions.append({
                                'timestamp': timestamp,
                                'ticker': ticker,
                                'action': 'buy',
                                'price': price,
                                'shares': shares,
                                'value': 1.0,
                                'prediction': avg_prediction
                            })
                    
                    elif sum_returns < lower_band:  # Stock is overvalued
                        # Short position
                        if ticker not in positions:
                            positions[ticker] = 0
                        
                        # Open trade if not already open
                        if ticker not in open_trades:
                            # Sell short 1 USD worth of stock
                            price = prices_df.loc[timestamp, ticker]
                            shares = 1.0 / price
                            positions[ticker] -= shares
                            
                            # Record trade
                            open_trades[ticker] = {
                                'entry_time': timestamp,
                                'entry_price': price,
                                'shares': shares,
                                'direction': 'short'
                            }
                            
                            day_transactions.append({
                                'timestamp': timestamp,
                                'ticker': ticker,
                                'action': 'sell_short',
                                'price': price,
                                'shares': shares,
                                'value': 1.0,
                                'prediction': avg_prediction
                            })
                
                # Check if we should close a trade (20 minutes after opening)
                if ticker in open_trades:
                    entry_time = open_trades[ticker]['entry_time']
                    if timestamp - entry_time >= dt.timedelta(minutes=20):
                        # Close position
                        price = prices_df.loc[timestamp, ticker]
                        shares = open_trades[ticker]['shares']
                        direction = open_trades[ticker]['direction']
                        
                        if direction == 'long':
                            # Sell shares
                            positions[ticker] -= shares
                            action = 'sell'
                        else:
                            # Buy back shares
                            positions[ticker] += shares
                            action = 'buy_to_cover'
                        
                        # Calculate trade value
                        trade_value = shares * price
                        
                        # Record trade
                        day_transactions.append({
                            'timestamp': timestamp,
                            'ticker': ticker,
                            'action': action,
                            'price': price,
                            'shares': shares,
                            'value': trade_value
                        })
                        
                        # Remove from open trades
                        del open_trades[ticker]
        
        # Close any remaining open trades at the end of the day
        for ticker, trade in open_trades.items():
            if ticker not in prices_df.columns:
                continue
            
            # Get closing price
            close_time = prices_df.index[-1]
            close_price = prices_df.loc[close_time, ticker]
            
            shares = trade['shares']
            direction = trade['direction']
            
            if direction == 'long':
                # Sell shares
                positions[ticker] -= shares
                action = 'sell'
            else:
                # Buy back shares
                positions[ticker] += shares
                action = 'buy_to_cover'
            
            # Calculate trade value
            trade_value = shares * close_price
            
            # Record trade
            day_transactions.append({
                'timestamp': close_time,
                'ticker': ticker,
                'action': action,
                'price': close_price,
                'shares': shares,
                'value': trade_value
            })
        
        # Calculate daily return
        daily_return = self.calculate_daily_return(day_transactions)
        
        return daily_return, day_transactions
    
    def calculate_daily_return(self, transactions):
        """
        Calculate daily return based on transactions
        
        Parameters:
        -----------
        transactions : list
            List of transactions
            
        Returns:
        --------
        daily_return : float
            Return for the day
        """
        if not transactions:
            return 0.0
        
        # Group transactions by ticker
        ticker_trades = {}
        
        for trade in transactions:
            ticker = trade['ticker']
            if ticker not in ticker_trades:
                ticker_trades[ticker] = []
            
            ticker_trades[ticker].append(trade)
        
        # Calculate return for each ticker
        ticker_returns = []
        
        for ticker, trades in ticker_trades.items():
            # Group trades into round trips
            round_trips = []
            open_positions = []
            
            for trade in trades:
                action = trade['action']
                
                if action in ['buy', 'sell_short']:
                    # Opening position
                    open_positions.append(trade)
                else:
                    # Closing position
                    if open_positions:
                        # Match with the oldest open position
                        open_trade = open_positions.pop(0)
                        
                        # Calculate return
                        entry_price = open_trade['price']
                        exit_price = trade['price']
                        shares = trade['shares']
                        
                        if open_trade['action'] == 'buy':
                            # Long position
                            trade_return = (exit_price - entry_price) / entry_price
                        else:
                            # Short position
                            trade_return = (entry_price - exit_price) / entry_price
                        
                        # Subtract transaction costs
                        trade_return -= 2 * self.transaction_cost
                        
                        round_trips.append({
                            'ticker': ticker,
                            'entry_time': open_trade['timestamp'],
                            'exit_time': trade['timestamp'],
                            'entry_price': entry_price,
                            'exit_price': exit_price,
                            'shares': shares,
                            'return': trade_return
                        })
            
            # Calculate average return for ticker
            if round_trips:
                ticker_return = np.mean([trip['return'] for trip in round_trips])
                ticker_returns.append(ticker_return)
        
        # Calculate average return across tickers
        if ticker_returns:
            daily_return = np.mean(ticker_returns)
        else:
            daily_return = 0.0
        
        return daily_return
    
    def run_backtest(self, start_date, end_date):
        """
        Run backtest for a given time period
        
        Parameters:
        -----------
        start_date : str
            Start date in format 'YYYYMMDD'
        end_date : str
            End date in format 'YYYYMMDD'
            
        Returns:
        --------
        daily_returns : list
            List of daily returns
        """
        print(f"Running backtest from {start_date} to {end_date}...")
        
        # Convert dates to datetime
        start_dt = dt.datetime.strptime(start_date, "%Y%m%d")
        end_dt = dt.datetime.strptime(end_date, "%Y%m%d")
        
        # Create date range
        all_dates = [start_dt + dt.timedelta(days=i) for i in range((end_dt - start_dt).days + 1)]
        trading_dates = [date for date in all_dates if date.weekday() < 5]  # Skip weekends
        
        # For each trading date
        daily_returns = []
        
        for i, trading_date in enumerate(trading_dates):
            date_str = trading_date.strftime("%Y%m%d")
            print(f"Processing day {i+1}/{len(trading_dates)}: {date_str}")
            
            # Define formation period
            if i < self.formation_days:
                # Not enough data for formation yet
                continue
            
            # Formation period dates
            formation_start = trading_dates[i - self.formation_days]
            formation_end = trading_dates[i - 1]
            formation_start_str = formation_start.strftime("%Y%m%d")
            formation_end_str = formation_end.strftime("%Y%m%d")
            
            # Get S&P 500 constituents
            constituents = self.get_sp500_constituents(date_str)
            
            # Get tweets for formation period
            formation_tweets = self.get_tweets(formation_start_str, formation_end_str, constituents)
            
            # Get minute-by-minute prices for formation period
            formation_prices = self.get_minute_data(constituents, formation_start_str)
            for date in trading_dates[i - self.formation_days + 1:i]:
                date_str = date.strftime("%Y%m%d")
                prices = self.get_minute_data(constituents, date_str)
                if prices is not None:
                    formation_prices = pd.concat([formation_prices, prices])
            
            if formation_prices is None:
                print(f"No price data available for formation period")
                continue
            
            # Prepare data for model training
            X, y, tickers, timestamps = self.prepare_formation_data(formation_tweets, formation_prices)
            
            if len(y) == 0:
                print(f"No valid data for formation period")
                continue
            
            # Train models
            root_relative_squared_errors = self.train_models(X, y, tickers)
            
            # Select top stocks
            self.top_stock_list = self.select_top_stocks(root_relative_squared_errors)
            print(f"Selected top stocks: {self.top_stock_list}")
            
            # Get prices for trading day
            trading_prices = self.get_minute_data(self.top_stock_list, date_str)
            
            if trading_prices is None:
                print(f"No price data available for trading day {date_str}")
                continue
            
            # Get tweets for trading day
            trading_tweets = self.get_tweets(date_str, date_str, self.top_stock_list)
            
            # Execute trading strategy
            daily_return, transactions = self.trade_day(date_str, trading_prices, trading_tweets)
            
            # Store results
            daily_returns.append({
                'date': date_str,
                'return': daily_return
            })
            self.transactions.extend(transactions)
            
            print(f"Daily return: {daily_return:.4f}")
        
        # Store daily returns
        self.daily_returns = daily_returns
        
        return daily_returns
    
    def plot_results(self):
        """Plot cumulative returns"""
        if not self.daily_returns:
            print("No results to plot")
            return
        
        # Convert daily returns to DataFrame
        returns_df = pd.DataFrame(self.daily_returns)
        returns_df['date'] = pd.to_datetime(returns_df['date'])
        returns_df = returns_df.set_index('date')
        
        # Calculate cumulative returns
        returns_df['cumulative_return'] = (1 + returns_df['return']).cumprod() - 1
        
        # Plot results
        plt.figure(figsize=(12, 6))
        plt.plot(returns_df.index, returns_df['cumulative_return'] * 100)
        plt.title(f'Cumulative Returns for {self.model_type} Strategy')
        plt.xlabel('Date')
        plt.ylabel('Cumulative Return (%)')
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def calculate_performance_metrics(self):
        """Calculate performance metrics"""
        if not self.daily_returns:
            print("No results to calculate metrics")
            return {}
        
        # Convert daily returns to numpy array
        returns = np.array([day['return'] for day in self.daily_returns])
        
        # Calculate metrics
        total_days = len(returns)
        profitable_days = np.sum(returns >= 0)
        hit_rate = profitable_days / total_days if total_days > 0 else 0
        
        avg_return = np.mean(returns) if returns.size > 0 else 0
        std_return = np.std(returns) if returns.size > 0 else 0
        
        # Annualize metrics
        annual_factor = 252
        annual_return = (1 + avg_return) ** annual_factor - 1
        annual_volatility = std_return * np.sqrt(annual_factor)
        
        # Sharpe ratio (assuming risk-free rate of 0)
        sharpe_ratio = annual_return / annual_volatility if annual_volatility > 0 else 0
        
        # Calculate maximum drawdown
        cumulative_returns = (1 + returns).cumprod() - 1
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = running_max - cumulative_returns
        max_drawdown = np.max(drawdown) if drawdown.size > 0 else 0
        
        # Return metrics
        metrics = {
            'total_days': total_days,
            'profitable_days': profitable_days,
            'hit_rate': hit_rate,
            'avg_daily_return': avg_return,
            'std_daily_return': std_return,
            'annual_return': annual_return,
            'annual_volatility': annual_volatility,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown
        }
        
        # Print metrics
        print("\nPerformance Metrics:")
        print(f"Total Days: {total_days}")
        print(f"Profitable Days: {profitable_days} ({hit_rate:.2%})")
        print(f"Average Daily Return: {avg_return:.4f} ({avg_return:.2%})")
        print(f"Daily Volatility: {std_return:.4f} ({std_return:.2%})")
        print(f"Annualized Return: {annual_return:.4f} ({annual_return:.2%})")
        print(f"Annualized Volatility: {annual_volatility:.4f} ({annual_volatility:.2%})")
        print(f"Sharpe Ratio: {sharpe_ratio:.4f}")
        print(f"Maximum Drawdown: {max_drawdown:.4f} ({max_drawdown:.2%})")
        
        return metrics


# Example usage
if __name__ == "__main__":
    # Create strategy
    afm_strategy = FactorizationMachineStrategy(
        formation_days=40,
        top_stocks=5,
        model_type='AFM',  # 'AFM', 'TFM', 'SFM', or 'SVM'
        window_size=10,
        bollinger_std=2,
        min_tweets=80,
        transaction_cost=0.0002
    )
    
    # Run backtest
    start_date = "20230101"
    end_date = "20230131"
    daily_returns = afm_strategy.run_backtest(start_date, end_date)
    
    # Plot results
    afm_strategy.plot_results()
    
    # Calculate performance metrics
    metrics = afm_strategy.calculate_performance_metrics()