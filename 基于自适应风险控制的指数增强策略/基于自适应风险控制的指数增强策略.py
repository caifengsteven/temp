# -*- coding: utf-8 -*-
"""
Created on Wed Apr 29 13:48:14 2020

@author: Asus
"""

'''
å¯¼è¯»
A.ç ”ç©¶ç›®çš„ï¼šæŒ‡æ•°å¢å¼ºåŸºé‡‘åŠ›æ±‚åœ¨å¯¹åŸºå‡†æŒ‡æ•°è·Ÿè¸ªçš„åŒæ—¶å®ç°è¶…é¢æ”¶ç›Šï¼Œåœ¨å¸‚åœºç»å†äº†2017å¹´å¸‚å€¼ã€åè½¬ç­‰å› å­çš„å¤§å¹…æ³¢åŠ¨åŠå›æ’¤ï¼Œé€šè¿‡ç»„åˆä¼˜åŒ–æ¥æ„å»ºæŒ‡æ•°å¢å¼ºç»„åˆçš„æ–¹å¼å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç»„åˆä¼˜åŒ–æ¨¡å‹çš„æœ€å¤§ä¼˜åŠ¿æ˜¯å¯ä»¥è¿›è¡Œçµæ´»çš„é£é™©æ§åˆ¶ï¼Œèƒ½å¤Ÿåœ¨æœ€å¤§åŒ–ç»„åˆæ”¶ç›Šçš„åŒæ—¶æ»¡è¶³ä¸€ç³»åˆ—çš„é£é™©æ§åˆ¶çº¦æŸæ¡ä»¶ï¼Œä½¿å¾—ç»„åˆèƒ½å¤Ÿåœ¨è·Ÿè¸ªåŸºå‡†æŒ‡æ•°çš„åŸºç¡€ä¸Šå®ç°ç¨³å®šè¶…é¢ã€‚æ”¶ç›Šé¢„æµ‹æ¨¡å‹å’Œé£é™©æ§åˆ¶æ¨¡å‹æ˜¯ç»„åˆä¼˜åŒ–æ¨¡å‹çš„ä¸¤ä¸ªé‡è¦çš„æ„æˆéƒ¨åˆ†ï¼Œæœ¬æ–‡å‚è€ƒå¤©é£è¯åˆ¸ï¼šã€ŠåŸºäºè‡ªé€‚åº”é£é™©æ§åˆ¶çš„æŒ‡æ•°å¢å¼ºç­–ç•¥ã€‹ã€å¤©é£è¯åˆ¸ï¼šã€Šå› å­æ­£äº¤å…¨æ”»ç•¥ï¼Œç†è®ºã€æ¡†æ¶ä¸å®è·µã€‹ï¼Œæˆ‘ä»¬å°†ä¸¤ç¯‡ç ”æŠ¥ä¸­çš„å› å­æ¨¡å‹ä¸é£é™©æ§åˆ¶æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œé€šè¿‡ä¸€ç§è‡ªé€‚åº”æ§åˆ¶è·Ÿè¸ªè¯¯å·®çš„æ–¹æ³•åœ¨Aè‚¡æ„å»ºæŒ‡æ•°å¢å¼ºç­–ç•¥è¿›è¡Œå®è¯ï¼Œå®ç°åœ¨ä¸åŒé£æ ¼æŒ‡æ•°ä¸Šçš„ç¨³å¥è¶…é¢æ”¶ç›Šã€‚

B.ç ”ç©¶ç»“è®ºï¼šå®è¯è¡¨æ˜ï¼Œé€šè¿‡ä¼˜çŸ¿å¹³å°åŠç ”æŠ¥æ¨¡å‹è¿›è¡Œå®è¯ï¼Œæˆ‘ä»¬ç»“åˆæ”¶ç›Šé¢„æµ‹æ¨¡å‹ã€é£é™©æ§åˆ¶æ¨¡å‹ã€è‡ªé€‚åº”é£é™©æ§åˆ¶æ–¹æ³•ä¸‹çš„ç­–ç•¥èƒ½åœ¨å„ç§å¸‚åœºé£æ ¼ä¸‹è·å–ç¨³å®šçš„è¶…é¢æ”¶ç›Šï¼Œå…¶ä¸­ï¼Œ

è‡ªé€‚åº”é£é™©æ§åˆ¶ä¸‹çš„æ²ªæ·±300æŒ‡æ•°å¢å¼ºç»„åˆä»2010å¹´åˆå›æµ‹è‡³2018å¹´8æœˆåº•ï¼Œå¹´åŒ–è¶…é¢æ”¶ç›Š8.23%ï¼Œç›¸å¯¹æœ€å¤§å›æ’¤4.01%ï¼Œæ”¶ç›Šå›æ’¤æ¯”2.05ï¼Œå¤æ™®æ¯”ç‡2.48ï¼Œè·Ÿè¸ªè¯¯å·®3.31%ï¼›
è‡ªé€‚åº”é£é™©æ§åˆ¶ä¸‹çš„ä¸­è¯500æŒ‡æ•°å¢å¼ºç»„åˆä»2010å¹´åˆå›æµ‹è‡³2018å¹´8æœˆåº•ï¼Œå¹´åŒ–è¶…é¢æ”¶ç›Š13.23%ï¼Œç›¸å¯¹æœ€å¤§å›æ’¤3.23%ï¼Œæ”¶ç›Šå›æ’¤æ¯”4.10ï¼Œå¤æ™®æ¯”ç‡3.99ï¼Œè·Ÿè¸ªè¯¯å·®3.32%ã€‚
C.æ–‡ç« ç»“æ„ï¼šæœ¬æ–‡å…±åˆ†ä¸º3ä¸ªéƒ¨åˆ†ï¼Œå…·ä½“å¦‚ä¸‹

ä¸€ã€æ•°æ®å‡†å¤‡åŠå¤„ç†ï¼šè¿™éƒ¨åˆ†ä¸»è¦åŒ…æ‹¬è‚¡ç¥¨æ± çš„ç•Œå®šï¼Œæ‰€ç”¨é€‰è‚¡å› å­çš„æ„é€ ã€é¢„å¤„ç†ç­‰

äºŒã€æ”¶ç›Šé¢„æµ‹æ¨¡å‹ï¼šè¯¥éƒ¨åˆ†ä¸»è¦æ˜¯åˆ©ç”¨å¯¹ç§°æ­£äº¤æ–¹æ³•ã€å› å­æƒé‡åå‘å½’é›¶ä»¥åŠICIRåŠ æƒæ–¹æ³•å¯¹å› å­è¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°å¤åˆå› å­ï¼Œä½œä¸ºå¯¹è‚¡ç¥¨çš„æ”¶ç›Šé¢„æµ‹

ä¸‰ã€é£é™©æ§åˆ¶æ¨¡å‹ä¸ç»„åˆä¼˜åŒ–ï¼šè¯¥éƒ¨åˆ†ä¸»è¦æ˜¯æ ¹æ®æ”¶ç›Šé¢„æµ‹æ¨¡å‹åŠä¸€ç³»åˆ—é£é™©çº¦æŸé€šè¿‡ç»„åˆä¼˜åŒ–æ–¹æ³•è·å¾—æ¯æœŸç»„åˆæƒé‡å¹¶ç”¨ä¼˜çŸ¿å¹³å°è¿›è¡Œå›æµ‹

D.è¿è¡Œæ—¶é—´è¯´æ˜

ä¸€ã€æ•°æ®å‡†å¤‡åŠå¤„ç†ï¼Œéœ€è¦50åˆ†é’Ÿå·¦å³

äºŒã€æ”¶ç›Šé¢„æµ‹æ¨¡å‹ï¼Œéœ€è¦5åˆ†é’Ÿå·¦å³

ä¸‰ã€é£é™©æ§åˆ¶æ¨¡å‹ä¸ç»„åˆä¼˜åŒ–ï¼Œéœ€è¦60åˆ†é’Ÿå·¦å³

 æ³¨æ„äº‹é¡¹ 

 ç¬¬ä¸‰éƒ¨åˆ†æ¶‰åŠå›æµ‹åŠå¤šè¿›ç¨‹ä¼˜åŒ–ï¼Œæ¶ˆè€—èµ„æºè¾ƒå¤§ï¼Œéœ€è¦é‡å¯ç ”ç©¶ç¯å¢ƒä»¥é‡Šæ”¾èµ„æº

ä¹‹å‰çš„æ•°æ®éƒ½è¿›è¡Œäº†å­˜å‚¨ï¼Œç¬¬ä¸‰éƒ¨åˆ†çš„ä»£ç å¯ä»¥ç›´æ¥è¿è¡Œè€Œä¸éœ€è¦é‡è·‘ç¬¬ä¸€ã€äºŒéƒ¨åˆ†çš„ä»£ç 

é‡å¯ç ”ç©¶ç¯å¢ƒçš„æ­¥éª¤ä¸ºï¼š

ç½‘é¡µç‰ˆï¼šå…ˆç‚¹å‡»å·¦ä¸Šè§’çš„â€œNotebookâ€å›¾æ ‡ï¼Œç„¶åç‚¹å‡»å·¦ä¸‹è§’çš„â€œå†…å­˜å ç”¨x%â€å›¾æ ‡ï¼Œåœ¨å¼¹æ¡†ä¸­ç‚¹å‡»é‡å¯ç ”ç©¶ç¯å¢ƒ
å®¢æˆ·ç«¯ï¼šç‚¹å‡»å·¦ä¸‹è§’çš„â€œå†…å­˜x%â€, åœ¨å¼¹æ¡†ä¸­ç‚¹å‡»é‡å¯ç ”ç©¶ç¯å¢ƒ
(æ·±åº¦æŠ¥å‘Šç‰ˆæƒå½’ä¼˜çŸ¿æ‰€æœ‰ï¼Œç¦æ­¢ç›´æ¥è½¬è½½æˆ–ç¼–è¾‘åè½¬è½½ã€‚)

   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡åŠå¤„ç†
è¯¥éƒ¨åˆ†è€—æ—¶ 50åˆ†é’Ÿ(ä¸»è¦æ˜¯å› å­çš„æ„é€ ç¯èŠ‚éœ€è¦40åˆ†é’Ÿï¼Œå…¶ä»–åˆè®¡10åˆ†é’Ÿ)
è¯¥éƒ¨åˆ†å†…å®¹ä¸ºï¼š

æ¯æœŸè‚¡ç¥¨æ± çš„é€‰å–ï¼ˆå‰”é™¤STåŠä¸Šå¸‚ä¸æ»¡åŠå¹´çš„æ–°è‚¡ï¼‰

é€‰è‚¡å› å­çš„æ„é€ ï¼Œå¯¹å› å­çš„é¢„å¤„ç†ï¼ˆå»æå€¼ã€ä¸­æ€§åŒ–ã€æ ‡å‡†åŒ–ï¼‰

(æ·±åº¦æŠ¥å‘Šç‰ˆæƒå½’ä¼˜çŸ¿æ‰€æœ‰ï¼Œç¦æ­¢ç›´æ¥è½¬è½½æˆ–ç¼–è¾‘åè½¬è½½ã€‚)

   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
1.1 æ¯æœŸè‚¡ç¥¨æ± çš„é€‰å–

ç”Ÿæˆçš„è‚¡ç¥¨æ± æ–‡ä»¶å­˜å‚¨åœ¨enhance_strategy_data/stock_pool.csv

è‚¡ç¥¨æ± å±æ€§ä¸ºdateï¼Œcodeï¼ŒåŒºé—´ä¸ºæ—¥æœŸä¸ºï¼ˆ20080102 - 20180903ï¼‰ä¹‹é—´çš„æ¯æœˆæœˆåˆ

'''


# coding: utf-8

import numpy as np
import pandas as pd
import datetime
import time
import os
import copy
import cvxpy as cvx
import matplotlib.pyplot as plt
import statsmodels.api as sm
import scipy.stats as st
import seaborn as sns
from multiprocessing import Pool
import cPickle as pickle
from CAL.PyCAL import *    # CAL.PyCALä¸­åŒ…å«font
universe = set_universe('A')
cal = Calendar('China.SSE')

# æ—¶é—´æ ¼å¼è½¬å˜å‡½æ•°
def time_change(x):
    y = datetime.datetime.strptime(x, '%Y-%m-%d')
    y = y.strftime('%Y%m%d')
    return y

# è·å–å›æµ‹åŒºé—´çš„äº¤æ˜“æ—¥ã€æœˆæœ«ä»¥åŠæœˆåˆæ—¶é—´
def get_trade_list(start_date, end_date):
    """
    Args:
        start_date: æ—¶é—´åŒºé—´èµ·ç‚¹
        end_date: æ—¶é—´åŒºé—´ç»ˆç‚¹
    Returns: 
        trade_list: æ—¶é—´åŒºé—´å†…çš„äº¤æ˜“æ—¥
        month_end: æœˆæœ«æ—¶é—´
        month_start: æœˆåˆæ—¶é—´
    """
    cal_dates = DataAPI.TradeCalGet(exchangeCD=u"XSHG", beginDate=start_date, endDate=end_date).sort('calendarDate')
    cal_dates = cal_dates[cal_dates['isOpen'] == 1]
    trade_list = cal_dates['calendarDate'].values.tolist()
    trade_list = [time_change(x) for x in trade_list]
    month_end = cal_dates[cal_dates['isMonthEnd'] == 1]
    month_end = month_end['calendarDate'].values.tolist()
    month_end = [time_change(x) for x in month_end]
    cal = Calendar('China.SSE')
    month_start = [cal.advanceDate(x, '1B').strftime('%Y%m%d') for x in month_end]
    return trade_list, month_end, month_start

# å‰”é™¤STè‚¡ç¥¨
def st_remove(source_universe, st_date=None):
    """
    Args:
        source_universe (list of str): éœ€è¦è¿›è¡Œç­›é€‰çš„è‚¡ç¥¨åˆ—è¡¨
        st_date (datetime): è¿›è¡Œç­›é€‰çš„æ—¥æœŸ,é»˜è®¤ä¸ºè°ƒç”¨å½“å¤©
    Returns:
        list: å»æ‰STè‚¡ç¥¨ä¹‹åçš„è‚¡ç¥¨åˆ—è¡¨
    """
    st_date = st_date if st_date is not None else datetime.datetime.now().strftime('%Y%m%d')
    df_ST = DataAPI.SecSTGet(secID=source_universe, beginDate=st_date, endDate=st_date, field=['secID'])
    return [s for s in source_universe if s not in list(df_ST['secID'])]

# å‰”é™¤æŸä¸ªæ—¥æœŸå‰å¤šå°‘ä¸ªäº¤æ˜“æ—¥,ä¹‹åä¸Šå¸‚çš„æ–°è‚¡
def new_remove(ticker,tradeDate= None,day = 1):
    """
    Args:
        ticker (list of str): éœ€è¦è¿›è¡Œç­›é€‰çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ— åç¼€ï¼‰
        tradeDate (datetime): è¿›è¡Œç­›é€‰çš„æ—¥æœŸ,é»˜è®¤ä¸ºè°ƒç”¨å½“å¤©
        day (int): å‘å‰æ¼‚ç§»çš„äº¤æ˜“æ—¥çš„ä¸ªæ•°
    Returns:
        list: å»æ‰æ–°è‚¡è‚¡ç¥¨ä¹‹åçš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ— åç¼€ï¼‰
    """
    tradeDate = tradeDate if tradeDate is not None else datetime.datetime.now()
    period = '-' + str(day) + 'B'
    pastDate = cal.advanceDate(tradeDate,period)
    pastDate = pastDate.strftime("%Y-%m-%d")
    ipo_date = DataAPI.SecIDGet(partyID=u"",assetClass=u"e",ticker=ticker,cnSpell=u"",field=u"ticker,listDate",pandas="1")
    remove_list = ipo_date[ipo_date['listDate'] > pastDate]['ticker'].tolist()
    return [stk for stk in ticker if stk not in remove_list]

# å°†è‚¡ç¥¨ä»£ç è½¬åŒ–ä¸ºè‚¡ç¥¨å†…éƒ¨ç¼–ç 
def ticker2secID(ticker):
    """
    Args:
        tickers (list): éœ€è¦è½¬åŒ–çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
    Returns:
        list: è½¬åŒ–ä¸ºå†…éƒ¨ç¼–ç çš„è‚¡ç¥¨ç¼–ç åˆ—è¡¨
    """
    universe = DataAPI.EquGet(equTypeCD=u"A",listStatusCD="L,S,DE,UN",field=u"ticker,secID",pandas="1") # è·å–æ‰€æœ‰çš„Aè‚¡ï¼ˆåŒ…æ‹¬å·²é€€å¸‚ï¼‰
    universe = dict(universe.set_index('ticker')['secID'])
    if isinstance(ticker, list):
        res = []
        for i in ticker:
            if i in universe:
                res.append(universe[i])
            else:
                print i, ' åœ¨universeä¸­ä¸å­˜åœ¨ï¼Œæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„secIDï¼'
        return res
    else:
        raise ValueError('ticker should be listï¼')

# è·å–è‚¡ç¥¨æ± å‡½æ•°
def get_stock_pool(date, N):
    """
    Args:
        date: æœˆåˆæ—¶é—´
        N: æ–°è‚¡çš„å®šä¹‰æ—¶é—´
    Returns:
        stock_pool: æ­¤æœˆåˆçš„è‚¡ç¥¨æ± 
    """
    univ=DynamicUniverse('A')
    all_code = univ.preview(date,skip_halted=False)
    all_code_not_ST = st_remove(all_code, st_date=date)
    ticker = [x.split('.')[0] for x in all_code_not_ST]
    all_code_need = new_remove(ticker, tradeDate=date, day=N)
    code = ticker2secID(all_code_need)
    df = pd.DataFrame({'code': code})
    df['date'] = date
    df = df[['date', 'code']]
    return df


# è‚¡ç¥¨æ± æ–‡ä»¶å­˜æ”¾ç›®å½•ï¼Œå¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œç¨‹åºè‡ªåŠ¨æ–°å»ºä¸€ä¸ª
raw_data_dir = "./enhance_strategy_data"
if not os.path.exists(raw_data_dir):
    os.mkdir(raw_data_dir)

# è·å–æ¯æœˆåˆçš„è‚¡ç¥¨æ± (start_date - end_date)
tic = time.time()
path = 'enhance_strategy_data/'
start_date = '20071220'
end_date = '20180831'        
trade_list, month_end, month_start = get_trade_list(start_date, end_date)
N = 180
all_stock = []
for date in month_start:
    stock = get_stock_pool(date, N)
    all_stock.append(stock)

all_stock = pd.concat(all_stock)
all_stock.to_csv(path + 'stock_pool.csv', index=False)
toc = time.time()
print('***********è‚¡ç¥¨æ± ç¤ºä¾‹************')
print(all_stock.head(10).to_html())
print ("\n ----- Computation time = " + str((toc - tic)) + "s")


'''


1.2 å› å­æ„é€ åŠé¢„å¤„ç†

å› å­æ„é€ 

è€ƒè™‘åˆ°å› å­çš„å…¨é¢æ€§å’Œä»£è¡¨æ€§ï¼Œæˆ‘ä»¬ä»è§„æ¨¡ã€ä¼°å€¼ã€æˆé•¿ã€ç›ˆåˆ©ã€æŠ€æœ¯ã€æµåŠ¨æ€§ã€æ³¢åŠ¨ç­‰ç»´åº¦æ¥ç­›é€‰å…·æœ‰é•¿æœŸç¨³å®šé€‰è‚¡èƒ½åŠ›çš„å› å­ï¼Œå› å­é›†åˆå¦‚ä¸‹ï¼š å›¾ç‰‡æ³¨é‡Š
æˆ‘ä»¬å°†å› å­çš„è®¡ç®—åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼š

ç¬¬ä¸€éƒ¨åˆ†æ˜¯è´¢åŠ¡ç›¸å…³å› å­çš„è®¡ç®—ï¼Œè®¡ç®—æ–¹æ³•åŒ…å«åœ¨FinancialFactorç±»ä¸­ï¼›
ç¬¬äºŒéƒ¨åˆ†æ˜¯æŠ€æœ¯å› å­çš„è®¡ç®—ï¼Œæˆ‘ä»¬åœ¨æœˆæœ«å®Œæˆå•æœŸå› å­çš„è®¡ç®—ï¼›
åœ¨å¾—åˆ°åŸå§‹å› å­åï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªå› å­è¿›è¡Œå¦‚ä¸‹å¤„ç†ï¼š

Step1: ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸šçš„ä¸­ä½æ•°å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¡«å……ï¼›
Step2: é‡‡ç”¨MADï¼ˆMedian Absolute Deviation ç»å¯¹ä¸­ä½æ•°æ³•ï¼‰è¿›è¡Œè¾¹ç•Œå‹ç¼©å¤„ç†ï¼Œå‰”é™¤å¼‚å¸¸å€¼ï¼›
Step3: å¯¹é™¤LNCAPä»¥å¤–çš„å…¶ä»–å› å­è¿›è¡Œå¸‚å€¼ + è¡Œä¸šçš„ä¸­æ€§åŒ–ï¼Œå¯¹LNCAPåšè¡Œä¸šä¸­æ€§åŒ–ï¼›
Step4: å¯¹ç¬¬äºŒæ­¥çš„æ®‹å·®é¡¹åšz-scoreæ ‡å‡†åŒ–å¤„ç†ï¼Œ
Step5: è®¡ç®—æ¯ä¸ªè‚¡ç¥¨çš„æ¬¡æœˆæ”¶ç›Šæ•°æ®å¤‡ç”¨ã€‚ factor_standå°±æ˜¯åšå®Œé¢„å¤„ç†åçš„æ ‡å‡†åŒ–å› å­æ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼šï¼ˆåˆ—æ•°å¤ªå¤šæˆªå–éƒ¨åˆ†ï¼‰ å›¾ç‰‡æ³¨é‡Š


'''

# ç¬¬ä¸€éƒ¨åˆ†ï¼šè´¢åŠ¡ç›¸å…³å› å­è®¡ç®—
class FinancialFactor(object):
    """
    è®¡ç®—è´¢åŠ¡ç›¸å…³å› å­
    """

    def __init__(self, income_statement, asset_statement, end_date_list):
        self.income_statement = income_statement  # åˆ©æ¶¦åŠæ”¶å…¥ç›¸å…³æ•°æ®
        self.asset_statement = asset_statement  # èµ„äº§æ•°æ®
        self.end_date_list = end_date_list

    @classmethod
    def cal_signal(cls, df, columns, end_date_list, shift=True):
        '''
        è®¡ç®—ä¸šç»©æƒŠå–œå› å­, V = (Qt - Qt-4)/std(delta(Q))
        delta(Q)ä¸ºè¿‡å»12æœŸçš„ Qt-Qt-4çš„æ ‡å‡†å·®ï¼Œè¿‡å»12æœŸä¸åŒ…å«å½“æœŸ
        :param df:è‡³å°‘åŒ…æ‹¬: secID, publishDate, endDate, [columns]
        :param columns: list, ç”¨æ¥è®¡ç®—æƒŠå–œå› å­çš„ä¼šè®¡ç§‘ç›®
        :param end_date_list: æ—¶é—´åŒºé—´
        :param shift: è®¡ç®—æ—¶å€™æ˜¯å¦è¿›è¡Œæ¼‚ç§»
        :return: å› å­å€¼dataframe, åˆ—ä¸º publishDate, [columns], publishDateæ ¼å¼å’Œè¾“å…¥ä¸€è‡´
        '''
        df1 = df.copy()
        df1.sort_values(by=['publishDate', 'endDate'], ascending=False, inplace=True)
        df2 = df.set_index('publishDate')[columns]
        date_list = df1['publishDate'].unique()
        date_list.sort()

        for date in sorted(date_list):
            tmp = df1[df1.publishDate <= date]
            tmp.drop_duplicates(subset=['endDate'], inplace=True, keep='first')

            tmp = tmp.sort_values(by='endDate', ascending=False).set_index('endDate')
            report_end_date = tmp.index[0]
            report_date_list = end_date_list[end_date_list <= report_end_date][-13:][::-1]
            tmp = tmp.reindex(report_date_list).head(13)

            tmp[columns] = tmp[columns].diff(-4)
            for column in columns:
                sigma = tmp[column][1:].std() if len(tmp[column].dropna()) >= 4 else np.NaN
                if shift:
                    df2.loc[date, column] = (tmp[column].iloc[0] - tmp[column].iloc[1:].mean()) / sigma
                else:
                    df2.loc[date, column] = tmp[column].iloc[0] / sigma
        df2 = df2.reset_index()
        return df2[['publishDate'] + columns]

    @classmethod
    def cal_yoy_signal(cls, df, columns):
        '''
        è®¡ç®—åŒæ¯”å¢é•¿ç‡ï¼Œ value = (Qt-Q(t-4))/abs(Q(t-4))
        :param df: è‡³å°‘åŒ…æ‹¬: secID, publishDate, endDate, [columns]
        :param columns: åˆ—åï¼Œç”¨æ¥è®¡ç®—åŒæ¯”çš„ä¼šè®¡ç§‘ç›®ï¼Œlistæ ¼å¼
        :return: dataframeï¼Œ åˆ—ä¸º: secID, publishDate, [columns]
        returnçš„columnsè™½ç„¶å’Œè¾“å…¥åŒåï¼Œä½†å€¼ä¸ºåŒæ¯”å€¼ï¼Œ publishDateæ ¼å¼ä¸º"%Y-%m-%d"
        '''
        df1 = df.copy()
        # è½¬æˆintç±»å‹ï¼Œä¾¿äºè¿›è¡Œrollingè®¡ç®—
        df1['endDate'] = df1['endDate'].apply(lambda x: int(x.replace("-", "")))
        df1['publishDate'] = df1['publishDate'].apply(lambda x: int(x.replace("-", "")))
        # å»å¹´åŒæœŸendDate
        df1['pre_end_date'] = df1['endDate'] - 10000
        # éœ€è¦åˆå¹¶çš„å»å¹´åŒæœŸå€¼
        m_df = df1[['publishDate', 'endDate'] + columns].rename(columns={"publishDate": "pre_pub",
                                                                         'endDate': 'pre_end_date'})

        df1 = df1.merge(m_df, on=['pre_end_date'], how='inner', suffixes=['', "_pre"])

        df1['max_pub_date'] = np.max(df1[['publishDate', 'pre_pub']], axis=1)
        # åŒä¸€ä¸ªå‘å¸ƒæ—¥æœŸï¼Œä¿ç•™æœ€å¤§çš„endDateçš„å€¼
        df1.sort_values(by=['max_pub_date', 'endDate', 'pre_pub'], ascending=True, inplace=True)
        df1 = df1.drop_duplicates(subset=['max_pub_date'], keep='last')

        # å¾—åˆ°æœ€è¿‘8æ¡è®°å½•å¯¹åº”çš„æœ€å¤§max_endDate
        df1['max_end_date'] = df1['endDate'].rolling(window=8, min_periods=1).max()
        # å–endDateä¸ºæœ€æ–°çš„è®°å½•
        df1 = df1[df1['endDate'] == df1['max_end_date']]
        # è®¡ç®—å› å­å€¼
        for column in columns:
            pre_value_col = column + "_pre"
            df1[column] = (df1[column] - df1[pre_value_col]) / abs(df1[pre_value_col])
        # å°†publishDateè½¬æˆ '%Y-%m-%d'æ ¼å¼
        df1 = df1[['secID', 'max_pub_date'] + columns].rename(columns={"max_pub_date": "publishDate"})
        df1['publishDate'] = df1['publishDate'].apply(lambda x: "%s-%s-%s" % (str(x)[:4], str(x)[4:6], str(x)[6:8]))
        return df1

    def cal_sue_sur(self):
        """
        ä¸å¸¦æ¼‚ç§»é¡¹çš„ä¸šç»©æƒŠå–œå› å­
        :return: sueå’Œsurçš„DataFrame, å…¬å‘Šæ—¥å‘å¸ƒåè®¡ç®—çš„å› å­æ•°æ®
        åˆ—ä¸ºï¼š secID, publishDate, signal
        """
        su_df = self.income_statement.groupby(by='secID').apply(lambda x: FinancialFactor.cal_signal(x,
                                                                                                     ['NIncomeAttrP',
                                                                                                      'revenue'],
                                                                                                     self.end_date_list,
                                                                                                     False))
        if 'secID' in su_df.columns:
            su_df = su_df.drop('secID', axis=1)
        su_df = su_df.reset_index()
        su_df.drop_duplicates(subset=['secID', 'publishDate'], inplace=True, keep='first')
        sue = su_df[['secID', 'publishDate', 'NIncomeAttrP']].dropna()
        sue = sue[['secID', 'publishDate', 'NIncomeAttrP']].rename(columns={"NIncomeAttrP":'signal'})
        sur = su_df[['secID', 'publishDate', 'revenue']].dropna()
        sur = sur[['secID', 'publishDate', 'revenue']].rename(columns={"revenue": 'signal'})
        return sue, sur

    def cal_growth_yoy(self):
        """
        è®¡ç®—å‡€åˆ©æ¶¦å¢é•¿ç‡å•å­£åº¦åŒæ¯”å› å­
        :return:DataFrame, å…¬å‘Šæ—¥å‘å¸ƒåè®¡ç®—çš„å› å­æ•°æ®
        """
        growth_yoy = self.income_statement.groupby(by='secID').apply(lambda x: FinancialFactor.cal_yoy_signal(x,
                                                                                                              [
                                                                                                                  'NIncomeAttrP',
                                                                                                                  'revenue']))
        profit_growth_yoy = growth_yoy[['secID', 'publishDate', 'NIncomeAttrP']].dropna().rename(
            columns={"NIncomeAttrP": "signal"})
        sales_growth_yoy = growth_yoy[['secID', 'publishDate', 'revenue']].dropna().rename(
            columns={"revenue": "signal"})
        profit_growth_yoy.reset_index(drop=True, inplace=True)
        sales_growth_yoy.reset_index(drop=True, inplace=True)
        return profit_growth_yoy, sales_growth_yoy

    @classmethod
    def cal_latest_pit_num(cls, df, col):
        '''
        å–è´¢åŠ¡æ•°æ®ä¸­æœ€æ–°çš„colåˆ—çš„å€¼ï¼ˆæœ€å¤§çš„endDate), endDateæŒ‡è´¢åŠ¡å‘å¸ƒæœŸï¼Œå¦‚2018-03-30
        :param df: è´¢åŠ¡æ•°æ®dataframeï¼Œè‡³å°‘åŒ…æ‹¬ publishDate, endDate, col
        :param col: è´¢åŠ¡ç§‘ç›®
        :return: æ¯ä¸ªå…¬å‘Šæ—¥å¯¹åº”çš„æœ€æ–°endDateçš„colå€¼
        dataframeæ ¼å¼ï¼Œåˆ—ä¸º publishDate,max_end_date, col, publishDateæ ¼å¼ä¸º"%Y-%m-%d"
        '''
        df1 = df[['publishDate', 'endDate', col]]
        # è½¬æˆintç±»å‹ï¼Œä¾¿äºè¿›è¡Œrollingè®¡ç®—
        df1['endDate'] = df1['endDate'].apply(lambda x: int(x.replace("-", "")))
        df1['publishDate'] = df1['publishDate'].apply(lambda x: int(x.replace("-", "")))
        # æ ¹æ®å‘å¸ƒæ—¥ã€endDateå‡åº
        df1.sort_values(by=['publishDate', 'endDate'], ascending=True, inplace=True)
        # å¾—åˆ°æœ€è¿‘8æ¡è®°å½•å¯¹åº”çš„æœ€å¤§max_endDate
        df1['max_end_date'] = df1['endDate'].rolling(window=8, min_periods=1).max()
        # åˆå¹¶max_end_dateçš„æ•°å€¼
        merge_df = df1[['publishDate', 'endDate', col]]
        merge_df.columns = ['m_pubdate', 'max_end_date', 'signal']
        df1 = df1.merge(merge_df, on=['max_end_date'], how='left')
        # å‰”é™¤æœªæ¥çš„æ•°æ®
        df1 = df1[df1.publishDate >= df1.m_pubdate]
        # å–å·²å‘å¸ƒçš„æœ€æ–°endDateçš„å€¼
        df1.sort_values(by=['publishDate', 'max_end_date', 'm_pubdate'], ascending=True, inplace=True)
        df1.drop_duplicates(subset=['publishDate'], keep='last', inplace=True)
        # å–endDateä¸ºæœ€æ–°çš„è®°å½•
        df1['publishDate'] = df1['publishDate'].apply(lambda x: "%s-%s-%s" % (str(x)[:4], str(x)[4:6], str(x)[6:8]))
        df1['max_end_date'] = df1['max_end_date'].apply(lambda x: "%s-%s-%s" % (str(x)[:4], str(x)[4:6], str(x)[6:8]))
        return df1[['publishDate', 'max_end_date', 'signal']]

    @classmethod
    def cal_latest_2pit_mean(cls, df, col):
        '''
        å–è´¢åŠ¡æ•°æ®ä¸­æœ€æ–°çš„ä¸¤ä¸ªcolåˆ—çš„å€¼çš„å‡å€¼ï¼ˆæœ€å¤§çš„endDateå’Œæ¬¡å¤§endDate), endDateæŒ‡è´¢åŠ¡å‘å¸ƒæœŸï¼Œå¦‚2018-03-30
        :param df: è´¢åŠ¡æ•°æ®dataframeï¼Œè‡³å°‘åŒ…æ‹¬ publishDate, endDate, col
        :param col: è´¢åŠ¡ç§‘ç›®
        :return: æ¯ä¸ªå…¬å‘Šæ—¥å¯¹åº”çš„æœ€æ–°endDateçš„colå€¼
        dataframeæ ¼å¼ï¼Œåˆ—ä¸º publishDate,max_end_date,col, publishDateæ ¼å¼ä¸º"%Y-%m-%d"
        '''
        df1 = df[['publishDate', 'endDate', col]]
        # è½¬æˆintç±»å‹ï¼Œä¾¿äºè¿›è¡Œrollingè®¡ç®—
        df1['endDate'] = df1['endDate'].apply(lambda x: int(x.replace("-", "")))
        df1['publishDate'] = df1['publishDate'].apply(lambda x: int(x.replace("-", "")))
        # æ ¹æ®å‘å¸ƒæ—¥ã€endDateå‡åº
        df1.sort_values(by=['publishDate', 'endDate'], ascending=True, inplace=True)
        # å¾—åˆ°æœ€è¿‘8æ¡è®°å½•å¯¹åº”çš„æœ€å¤§max_endDate
        df1['max_end_date'] = df1['endDate'].rolling(window=8, min_periods=1).max()
        # åˆå¹¶max_end_dateçš„æ•°å€¼
        merge_df = df1[['publishDate', 'endDate', col]]
        merge_df.columns = ['m_pubdate', 'max_end_date', 'signal']
        df1 = df1.merge(merge_df, on=['max_end_date'], how='left')
        # å‰”é™¤æœªæ¥çš„æ•°æ®
        df1 = df1[df1.publishDate >= df1.m_pubdate]
        # å–å·²å‘å¸ƒçš„æœ€æ–°endDateçš„å€¼
        df1.sort_values(by=['publishDate', 'max_end_date', 'm_pubdate'], ascending=True, inplace=True)
        df1.drop_duplicates(subset=['publishDate'], keep='last', inplace=True)

        def calc_prev_enddate(x):
            '''
            è®¡ç®—ä¸Šä¸€ä¸ªè´¢æŠ¥çš„enddate, xä¸ºintï¼Œå¦‚20180330
            è¿”å›å€¼ä¸ºintç±»å‹
            '''
            c_m = str(x)[4:6]
            c_y = str(x)[:4]
            if c_m == '03':
                prev_enddate = int("%s1231" % (int(c_y) - 1))
            elif c_m == '06':
                prev_enddate = int("%s0331" % c_y)
            elif c_m == '09':
                prev_enddate = int("%s0630" % c_y)
            elif c_m == '12':
                prev_enddate = int("%s0930" % c_y)
            else:
                raise Exception("not valid month, %s" % c_m)
            return prev_enddate

        # æ¬¡å¤§end_dateçš„å€¼
        df1['prev_end_date'] = df1['max_end_date'].apply(lambda x: calc_prev_enddate(x))
        # åˆå¹¶æ¬¡å¤§end_dateçš„å€¼
        merge_df.columns = ['prev_m_pubdate', 'prev_end_date', 'prev_signal']
        df1 = df1.merge(merge_df, on=['prev_end_date'], how='left')
        # å‰”é™¤æœªæ¥çš„æ•°æ®
        df1 = df1[df1.publishDate >= df1.prev_m_pubdate]
        # å–å·²å‘å¸ƒçš„æœ€æ–°endDateçš„å€¼
        df1.sort_values(by=['publishDate', 'prev_end_date', 'prev_m_pubdate'], ascending=True, inplace=True)
        df1.drop_duplicates(subset=['publishDate'], keep='last', inplace=True)
        # å–è¿‘æœŸä¸¤ä¸ªå€¼çš„å‡å€¼
        df1['signal'] = df1[['signal', 'prev_signal']].mean(axis=1)
        # å–endDateä¸ºæœ€æ–°çš„è®°å½•
        df1['publishDate'] = df1['publishDate'].apply(lambda x: "%s-%s-%s" % (str(x)[:4], str(x)[4:6], str(x)[6:8]))
        df1['max_end_date'] = df1['max_end_date'].apply(lambda x: "%s-%s-%s" % (str(x)[:4], str(x)[4:6], str(x)[6:8]))
        return df1[['publishDate', 'max_end_date', 'signal']]

    def cal_roe(self):
        """
        è®¡ç®—ROEç›¸å…³å› å­
        :return:DataFrame, å…¬å‘Šæ—¥å‘å¸ƒåè®¡ç®—çš„å› å­æ•°æ®
        åˆ—ä¸ºï¼š'secID', 'publishDate', 'roe', 'delta_roe'
        """
        # è®¡ç®—æ¯ä¸ªå…¬å‘Šæ—¥å¯¹åº”çš„å‡€åˆ©æ¶¦å€¼ï¼ˆæœ€æ–°çš„è´¢æŠ¥æœŸ)
        numerator = self.income_statement.groupby(by='secID').apply(
            lambda x: FinancialFactor.cal_latest_pit_num(x, 'NIncomeAttrP'))
        if 'secID' in numerator.columns:
            numerator = numerator.drop('secID', axis=1)
        numerator = numerator.reset_index()
        numerator = numerator.rename(columns={'signal': "signal_NIAttrP", 'max_end_date': "endDate"}).dropna()

        # è®¡ç®—æ¯ä¸ªå…¬å‘Šæ—¥å¯¹åº”çš„å‡€èµ„äº§å€¼(æœ€æ–°çš„è´¢æŠ¥æœŸ)
        denominator = self.asset_statement.groupby(by='secID').apply(
            lambda x: FinancialFactor.cal_latest_2pit_mean(x, 'TEquityAttrP'))
        if 'secID' in denominator.columns:
            denominator = denominator.drop('secID', axis=1)
        denominator = denominator.reset_index()
        denominator = denominator.rename(columns={'signal': "signal_TEAttrP", 'max_end_date': "endDate"}).dropna()

        df = pd.merge(numerator, denominator, on=['secID', 'publishDate', 'endDate'], how='left')
        df['roe'] = df['signal_NIAttrP'] / df['signal_TEAttrP']
        df = df.dropna()
        # å»å¹´åŒæœŸçš„roe
        df['pre_endDate'] = df['endDate'].apply(lambda x: "%s%s" % (int(x[:4]) - 1, x[4:]))
        merge_df = df[['secID', 'publishDate', 'endDate', 'roe']]
        merge_df.columns = ['secID', 'pre_pubdate', 'pre_endDate', 'pre_roe']
        # åˆå¹¶å»å¹´çš„å€¼
        df = df.merge(merge_df, on=['secID', 'pre_endDate'], how='left')
        df = df[(df.publishDate >= df.pre_pubdate) | (df.pre_pubdate.isnull())]
        df.sort_values(by=['secID', 'publishDate', 'endDate', 'pre_pubdate'], ascending=True, inplace=True)
        df.drop_duplicates(subset=['secID', 'publishDate'], keep='last', inplace=True)

        df['delta_roe'] = df['roe'] - df['pre_roe']
        df = df[['secID', 'publishDate', 'roe', 'delta_roe']]
        df.reset_index(drop=True, inplace=True)
        return df
    
def fill_factor(df, name):
    """
    å¤„ç†è´¢åŠ¡å› å­æ•°æ®çš„æ ¼å¼
    """
    df = df.pivot(index='publishDate', columns='secID', values='signal').loc[trade_date_list, :].fillna(method='ffill').loc[month_date_list, :].unstack().reset_index()
    df.columns = ['secID', 'publishDate', name]
    return df


# ç¬¬ä¸€éƒ¨åˆ†ï¼šè´¢åŠ¡ç›¸å…³å› å­è®¡ç®—
tic = time.time()
# åˆ©æ¶¦åŠæ”¶å…¥æ•°æ®
income_data = DataAPI.FdmtISQPITGet(field=u"secID,publishDate,endDate,NIncomeAttrP,NIncome,revenue", pandas="1")
income_data = income_data[income_data['secID'].str[0].isin(['0', '3', '6'])]
# èµ„äº§æ•°æ®
asset_data = DataAPI.FdmtBSGet(field=u"secID,publishDate,endDate,TEquityAttrP,TShEquity", pandas="1")
asset_data = asset_data[asset_data['secID'].str[0].isin(['0', '3', '6'])]  
    
date_list = np.array(sorted(income_data['endDate'].unique()))
financial_factor = FinancialFactor(income_data, asset_data, date_list)
#ã€€æŠ«éœ²æœŸå› å­è®¡ç®—    
sue, sur = financial_factor.cal_sue_sur()

profit_growth_yoy, sales_growth_yoy = financial_factor.cal_growth_yoy()

earning_factor = financial_factor.cal_roe()
    
# å°†PITçš„å› å­æ•°æ®è½¬æˆæœˆæœ«å› å­å€¼
calendar = DataAPI.TradeCalGet(exchangeCD='XSHG', beginDate='20070101', endDate='20180831')
calendar = calendar[calendar['isOpen'] == 1]
trade_date_list = calendar['calendarDate'].tolist()

month_date_list = calendar[calendar['isMonthEnd'] == 1]['calendarDate']
month_date_list = month_date_list[month_date_list > '2007-01-01'].tolist()

# ä¸šç»©æƒŠå–œå› å­
sue = fill_factor(sue, 'sue')
sur = fill_factor(sur, 'sur')
profit_growth_yoy = fill_factor(profit_growth_yoy, 'profit_growth_yoy')
sales_growth_yoy = fill_factor(sales_growth_yoy, 'sales_growth_yoy')
# ç›ˆåˆ©ç›¸å…³å› å­
roe = fill_factor(earning_factor[['secID', 'publishDate', 'roe']].rename(columns={"roe": "signal"}), 'roe')
delta_roe = fill_factor(earning_factor[['secID', 'publishDate', 'delta_roe']].rename(columns={"delta_roe": "signal"}), 'delta_roe')
# æˆé•¿å’Œç›ˆåˆ©å› å­æ•´åˆ
growth = sue.merge(sur, on=['secID', 'publishDate']).merge(profit_growth_yoy, on=['secID', 'publishDate']). \
    merge(sales_growth_yoy, on=['secID', 'publishDate'])
growth.columns = ['code', 'date', 'sue', 'sur', 'profit_growth_yoy', 'sales_growth_yoy']
growth['date'] = map(time_change, growth['date'])
earning = roe.merge(delta_roe, on=['secID', 'publishDate'])
earning.columns = ['code', 'date', 'roe', 'delta_roe']
earning['date'] = map(time_change, earning['date'])

toc = time.time()
print ("\n ----- Financial factor Computation time = " + str((toc - tic)) + "s")


# è®¡ç®—å¯¹æ•°å¸‚å€¼å› å­
def cal_lnmkt(date, code):
    """
    Args:
        date: æ—¥æœŸ
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
    Returns:
        mkt: å¯¹æ•°å¸‚å€¼å› å­æ•°æ®,dataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼Œå› å­å€¼
    """   
    mkt = DataAPI.MktEqudGet(tradeDate=date,secID=code,field=u"tradeDate,secID,marketValue",pandas="1")
    mkt.columns = ['date', 'code', 'mkt']
    mkt['mkt'] = np.log(mkt['mkt'])
    mkt['date'] = map(time_change, mkt['date'])
    return mkt

# ä¼°å€¼å› å­ï¼šBPã€EPTTMã€SPTTM
def cal_value_factor(date, code):
    """
    Args:
        date: æ—¥æœŸ
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
    Returns: 
        value: ä¼°å€¼å› å­æ•°æ®ï¼Œdataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼ŒBPï¼ŒEPTTMï¼ŒSPTTM
    """
    temp = DataAPI.MktStockFactorsOneDayProGet(tradeDate=date,secID=code,field=u"tradeDate,secID,PB,PE,PS",pandas="1")
    temp['BP'] = 1.0 / temp['PB']
    temp['EPTTM'] =  1.0 / temp['PE']
    temp['SPTTM'] = 1.0 / temp['PS']
    value = temp[['tradeDate', 'secID', 'BP', 'EPTTM', 'SPTTM']]
    value.columns = ['date', 'code', 'BP', 'EPTTM', 'SPTTM']
    value['date'] = map(time_change, value['date'])
    return value

# æŠ€æœ¯å› å­ï¼šä¸€æœˆã€ä¸‰æœˆåè½¬
def cal_reverse(date, code):
    """
    Args:
        date: æ—¥æœŸ
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
    Returns:
        ret: åè½¬å› å­æ•°æ®ï¼Œdataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼Œå› å­å€¼
    """
    cal = Calendar('China.SSE')
    pre_20 = cal.advanceDate(date, '-20B').strftime('%Y%m%d')
    pre_60 = cal.advanceDate(date, '-60B').strftime('%Y%m%d')
    close_20 = DataAPI.MktEqudAdjAfGet(secID=code,tradeDate=pre_20,field=u"secID,closePrice",pandas="1")
    close_20.columns = ['code', 'close_20']
    close_60 = DataAPI.MktEqudAdjAfGet(secID=code,tradeDate=pre_60,field=u"secID,closePrice",pandas="1")
    close_60.columns = ['code', 'close_60']
    close = DataAPI.MktEqudAdjAfGet(secID=code,tradeDate=date,field=u"tradeDate,secID,closePrice",pandas="1")
    close.columns = ['date', 'code', 'close']
    ret = close.merge(close_20, on='code').merge(close_60, on='code')
    ret['ret_20'] = ret['close'] / ret['close_20'] - 1
    ret['ret_60'] = ret['close'] / ret['close_60'] - 1 
    reverse = ret[['date', 'code', 'ret_20', 'ret_60']]
    reverse['date'] = map(time_change, reverse['date'])
    return reverse

# è·å–æŸä¸ªæ—¶ç‚¹è‚¡ç¥¨æ‰€å±è¡Œä¸šï¼ˆç”³ä¸‡ä¸€çº§è¡Œä¸šï¼Œä¸å›å¡«ï¼‰
def get_industry(date, code):
    """
    Args:
        date: æœˆåˆæ—¶é—´
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
    Returns:
        indu: ç”³ä¸‡ä¸€çº§è¡Œä¸šå› å­æ•°æ®,dataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼Œè¡Œä¸šå
    """ 
    cal = Calendar('China.SSE')
    end = cal.advanceDate(date, '-1B').strftime('%Y%m%d') 
    indu = DataAPI.MdSwBackGet(secID=code,field=u"secID,isNew,oldTypeName,industryName1,intoDate,outDate",pandas="1")
    indu['outDate'].fillna('2050-01-01', inplace=True)
    indu['intoDate'] = map(time_change, indu['intoDate'])
    indu['outDate'] = map(time_change, indu['outDate'])
    indu = indu[(indu['intoDate']<=end) & (indu['outDate']>end)]
    indu.drop_duplicates(subset=['secID'], inplace=True)
    indu['date'] = end
    indu = indu[['date', 'secID', 'industryName1']]
    indu.columns = ['date', 'code', 'industry']
    return indu

# æµåŠ¨æ€§å› å­ï¼š éæµåŠ¨æ€§å†²å‡»ï¼Œä¸€æœˆæ—¥å‡æ¢æ‰‹ï¼Œä¸‰æœˆæ—¥å‡æ¢æ‰‹
def cal_liquidity(date, code, mkt_info):
    """
    Args:
        date: æœˆåˆæ—¥æœŸ
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
        mkt_info: è¡Œæƒ…æ•°æ®
    Returns:
        liquidity: æµåŠ¨æ€§å› å­æ•°æ®ï¼Œdataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼ŒILLIQï¼Œturn_1Mï¼Œturn_3M
    """
    cal = Calendar('China.SSE')
    end = cal.advanceDate(date, '-1B').strftime('%Y%m%d')
    pre_20 = cal.advanceDate(end, '-19B').strftime('%Y%m%d')
    pre_60 = cal.advanceDate(end, '-59B').strftime('%Y%m%d')
    # ILLIQ
    pct_chg = mkt_info[(mkt_info['tradeDate'] >= pre_20) & (mkt_info['tradeDate'] <= end)]
    pct_chg['daily'] = np.where(pct_chg['turnoverValue'] == 0, np.nan, 10e9 * np.abs(pct_chg['chgPct']) / pct_chg['turnoverValue'])
    illiq = pct_chg.groupby(by='secID').apply(lambda x: x['daily'].mean())
    illiq = illiq.reset_index(level=0)
    illiq.columns = ['code', 'ILLIQ']
    illiq['date'] = end
    # turnover_3M
    turn = mkt_info[(mkt_info['tradeDate'] >= pre_60) & (mkt_info['tradeDate'] <= end)]
    turn['turn'] = np.where(turn['turnoverRate'] == 0, np.nan, turn['turnoverRate'])
    turn_3M = turn.groupby(by='secID').apply(lambda x: x['turn'].mean())
    turn_3M = turn_3M.reset_index(level=0)
    turn_3M.columns = ['code', 'turn_3M']
    turn_3M['date'] = end
    # turnover_1M
    turn = turn[turn['tradeDate'] >= pre_20]
    turn_1M = turn.groupby(by='secID').apply(lambda x: x['turn'].mean())
    turn_1M = turn_1M.reset_index(level=0)
    turn_1M.columns = ['code', 'turn_1M']
    turn_1M['date'] = end
    
    liquidity = illiq.merge(turn_3M, on=['date', 'code']).merge(turn_1M, on=['date', 'code'])
    liquidity = liquidity[['date', 'code', 'ILLIQ', 'turn_1M', 'turn_3M']]
    return liquidity

# æ³¢åŠ¨ï¼š ç‰¹å¼‚åº¦ï¼Œä¸€ä¸ªæœˆçœŸå®æ³¢å¹…ï¼Œä¸‰ä¸ªæœˆçœŸå®æ³¢å¹…
# ç‰¹å¼‚åº¦å› å­
def cal_specificity(date, code, mkt_info):
    """
    Args:
        date: æœˆåˆæ—¥æœŸ
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
        mkt_info: è¡Œæƒ…æ•°æ®
    Returns:
        spec: ç‰¹å¼‚åº¦å› å­æ•°æ®,dataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼Œå› å­å€¼
    """
    cal = Calendar('China.SSE')
    end = cal.advanceDate(date, '-1B').strftime('%Y%m%d')
    pre_20 = cal.advanceDate(date, '-20B').strftime('%Y%m%d')
    all_data = mkt_info[(mkt_info['tradeDate'] >= pre_20) & (mkt_info['tradeDate'] <= end)]
    tdata = all_data[(all_data['tradeDate'] == end) & (all_data['secID'].isin(code))]
    # å¸‚åœºç»„åˆ
    tdata['weight'] = tdata['negMarketValue'] / tdata['negMarketValue'].sum()
    port_all = tdata[['secID', 'weight']]
    port_all.columns = ['secID', 'weight_market']    
    # å¤§å°å¸‚å€¼ç»„åˆ
    temp = tdata.copy()
    temp.sort_values(by='negMarketValue', inplace=True)
    temp.reset_index(drop=True, inplace=True)
    num = len(temp) / 3
    port_small_mkv = temp[0: num]
    port_small_mkv['weight_sm'] = port_small_mkv['negMarketValue'] / port_small_mkv['negMarketValue'].sum()
    port_small_mkv = port_small_mkv[['secID', 'weight_sm']]
    port_large_mkv = temp[-num:]
    port_large_mkv['weight_lm'] = port_large_mkv['negMarketValue'] / port_large_mkv['negMarketValue'].sum()    
    port_large_mkv = port_large_mkv[['secID', 'weight_lm']]
    # é«˜ä½PBç»„åˆ
    temp = tdata.copy()
    temp.sort_values(by='PB', inplace=True)
    temp.reset_index(drop=True, inplace=True)
    port_low_pb = temp[0: num]
    port_low_pb['weight_lp'] = port_low_pb['negMarketValue'] / port_low_pb['negMarketValue'].sum()
    port_low_pb = port_low_pb[['secID', 'weight_lp']]
    port_high_pb = temp[-num:]
    port_high_pb['weight_hp'] = port_high_pb['negMarketValue'] / port_high_pb['negMarketValue'].sum()    
    port_high_pb = port_high_pb[['secID', 'weight_hp']]
    # æ•´åˆ
    weight = pd.merge(port_all, port_small_mkv, on='secID', how='left')
    weight = pd.merge(weight, port_large_mkv, on='secID', how='left')
    weight = pd.merge(weight, port_low_pb, on='secID', how='left')
    weight = pd.merge(weight, port_high_pb, on='secID', how='left')
    weight.fillna(0, inplace=True)
    # æ”¶ç›ŠçŸ©é˜µ
    cal_dates = DataAPI.TradeCalGet(exchangeCD=u"XSHG", beginDate=pre_20, endDate=end).sort('calendarDate')
    cal_dates = cal_dates[cal_dates['isOpen']==1]
    date_list = cal_dates['calendarDate'].values.tolist()
    date_list = [time_change(x) for x in date_list]
    for day in date_list:
        pct_chg = all_data[all_data['tradeDate'] == day]
        pct_chg = pct_chg[['secID', 'chgPct']]
        pct_chg.columns = ['secID', 'chgPct__' + str(day)]
        weight = pd.merge(weight, pct_chg, on='secID')
    # æ—¥æ”¶ç›Šåºåˆ—
    w_mat = np.matrix(weight.iloc[:, 1: 6]).T
    change_mat = np.matrix(weight.iloc[:, 6: ])                 
    ret = w_mat * change_mat
    cols = ['ret_all', 'ret_small_mkv', 'ret_large_mkv', 'ret_low_pb', 'ret_high_pb']
    ret = pd.DataFrame(ret.T,columns=cols)
    ret['date'] = date_list
    ret['ret_mkv'] = ret['ret_small_mkv'] - ret['ret_large_mkv']
    ret['ret_pb'] = ret['ret_low_pb'] - ret['ret_high_pb']
    ret['constant'] = 1
    ret = ret[['date', 'constant', 'ret_all', 'ret_mkv', 'ret_pb']]
    # å›å½’
    pct_table = weight.iloc[:, 6: ]
    pct_table.columns = [str(x.split('__')[1]) for x in list(pct_table.columns)]
    pct_table = pct_table.T
    pct_table.columns = list(weight['secID'])
    reg_data = pd.merge(ret, pct_table, left_on='date', right_index=True)
    x = reg_data.iloc[:, 1:5]  # è¿™é‡ŒåŠ äº†å¸¸æ•°é¡¹constant
    col = reg_data.columns[5:]
    IV_col = []
    IVR = []
    all_code = []
    for name in col:
        y = reg_data[name]
        y = y.replace(0, np.nan)
        # åšä¸ªåˆ¤å®šï¼Œå›å½’å¤©æ•°å¤ªå°‘çš„è¦å‰”é™¤
        if len(y[y.isnull()]) < 10:
            model = sm.OLS(y, x, missing='drop')
            results = model.fit()
            IV = np.std(results.resid) * np.sqrt(252)
            R2_single = results.rsquared        
            all_code.append(name)            
            IV_col.append(IV)
            IVR.append(1 - R2_single)
        else:
            continue
    spec = pd.DataFrame({'secID': all_code, 'IVFF': IV_col, 'IVR': IVR})
    spec = pd.merge(port_all, spec, on='secID', how='left') # æ²¡æœ‰çš„è®°ä¸ºnan
    spec['date'] = end
    spec = spec[['date', 'secID', 'IVR']]
    spec.columns = ['date', 'code', 'IVR']
    return spec

def cal_atrp(date, code, mkt_info):
    """
    Args:
        date: æœˆåˆæ—¥æœŸ
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
        mkt_info: è¡Œæƒ…æ•°æ®
    Returns:
        : atrpå› å­æ•°æ®ï¼Œdataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼ŒATRP_20, ATRP_60
    """
    cal = Calendar('China.SSE')
    end = cal.advanceDate(date, '-1B').strftime('%Y%m%d')
    pre_20 = cal.advanceDate(end, '-19B').strftime('%Y%m%d')
    pre_60 = cal.advanceDate(end, '-59B').strftime('%Y%m%d')
    period_info = mkt_info[(mkt_info['tradeDate'] >= pre_60) & (mkt_info['tradeDate'] <= end)]
    period_info.rename(columns={"secID": "code", "tradeDate": "date", "preClosePrice": "pre_close", "highestPrice": "high",
                             "lowestPrice": "low", "closePrice": "close", "turnoverVol": "volume"}, inplace=True)  
    period_info['cand1'] = period_info['high'] - period_info['low']
    period_info['cand2'] = abs(period_info['high'] - period_info['pre_close'])
    period_info['cand3'] = abs(period_info['low'] - period_info['pre_close'])
    period_info['tr'] = np.maximum(period_info['cand1'], period_info['cand2'])
    period_info['tr'] = np.maximum(period_info['tr'], period_info['cand3'])
    period_info['trp'] = period_info['tr'] / period_info['close']
    period_info['trp'] = np.where(period_info['volume'] < 1e-8, np.nan, period_info['trp'])
    # ATRP_3M                         
    atrp_3M = period_info.groupby(by='code').apply(lambda x: x['trp'].mean())
    atrp_3M = atrp_3M.reset_index(level=0)
    atrp_3M.columns = ['code', 'ATRP_3M']
    # ATRP_1M
    period_info = period_info[period_info['date'] >= pre_20]
    atrp_1M = period_info.groupby(by='code').apply(lambda x: x['trp'].mean())
    atrp_1M = atrp_1M.reset_index(level=0)
    atrp_1M.columns = ['code', 'ATRP_1M']
        
    atrp = atrp_1M.merge(atrp_3M, on='code')
    atrp['date'] = end
    return atrp

# å¤šçº¿ç¨‹å–æ•°æ®å¤‡ç”¨
from multiprocessing.dummy import Pool as ThreadPool
pool = ThreadPool(processes=16)

# è·å–ç»™å®šåŒºé—´çš„è¡Œæƒ…ä¿¡æ¯
def get_mkt_info(params):
    '''
    Argsï¼š
        params = [code, date, equd_list, adj_list]
        code: è‚¡ç¥¨ä»£ç é›†åˆ
        date: èµ·å§‹æ—¶é—´å’Œç»“æŸæ—¶é—´åˆ—è¡¨ï¼Œ ['20080101', '20081231']
        equd_list: mktequdæ•°æ®åˆ—è¡¨
        adj_list: å‰å¤æƒæ•°æ®åˆ—è¡¨
    Return:
        DataFrame, è¿”å›æ—¥æœŸåŒºé—´çš„æ•°æ®å€¼
        '''
    code, date, equd_list, adj_list = params
    
    cnt = 0
    while True:
        try:
            tmp_frame1 = DataAPI.MktEqudGet(secID=code,beginDate=date[0],endDate=date[1],
                                           field=["secID","tradeDate"] + equd_list,pandas="1")
            tmp_frame2 = DataAPI.MktEqudAdjGet(secID=code,beginDate=date[0],endDate=date[1],
                                           field=["secID","tradeDate"] + adj_list,pandas="1")
            tmp_frame = tmp_frame1.merge(tmp_frame2, on=['secID', 'tradeDate'])
            return tmp_frame
        except Exception as e:
            cnt += 1
            print "get data failed in get_mkt_info, reason:%s, retry again, retry count:%s" % (e, cnt)
            if cnt >= 3:
                print "max get data retry, will exit"
                raise Exception(e)
        return

tic = time.time()
# å¤šçº¿ç¨‹å–æ•°æ®
all_code = sorted(all_stock['code'].unique())
date_list = sorted(all_stock['date'].unique())
# æ—¶é—´å‡†å¤‡ï¼Œä¸€å¹´å–ä¸€æ¬¡
calendar = DataAPI.TradeCalGet(exchangeCD='XSHG', beginDate='20070601', endDate='20180831')
calendar = calendar[calendar['isOpen'] == 1]
year_end = list(calendar[calendar['isYearEnd'] == 1]['calendarDate'])
year_end.append('2007-06-01')
year_end.append('2018-08-31')
year_end = sorted([x.replace('-', '') for x in year_end])
date_l = [[year_end[i], year_end[i + 1]] for i in range(len(year_end) - 1)]

factor_list_1 = ['chgPct', 'turnoverValue', 'turnoverRate', 'PB', 'negMarketValue']
factor_list_2 = ['preClosePrice', 'highestPrice', 'lowestPrice', 'closePrice', 'turnoverVol']
pool_args = zip([all_code] * len(date_l), date_l, [factor_list_1] * len(date_l), [factor_list_2] * len(date_l))
mkt_info_list = pool.map(get_mkt_info, pool_args)
pool.close()
pool.join()
mkt_info = pd.concat(mkt_info_list)
mkt_info['tradeDate'] = map(time_change, mkt_info['tradeDate'])
mkt_info.drop_duplicates(subset=['secID', 'tradeDate'], inplace=True)
toc = time.time()
print ("\n ----- get_mkt_data time = " + str((toc - tic)) + "s")


# ç¬¬äºŒéƒ¨åˆ†ï¼šæœˆæœ«å› å­è®¡ç®—
tic = time.time()
date_list = sorted(all_stock['date'].unique())
all_factor = []
for date in date_list:
    end = cal.advanceDate(date, '-1B').strftime('%Y%m%d') # æœˆæœ«æ—¶é—´
    stock = all_stock[all_stock['date'] == date]
    stock['date'] = end
    code = list(stock['code'])
    factor = stock.copy()
    # ä¼°å€¼å› å­
    temp = cal_value_factor(end, code)
    factor = pd.merge(factor, temp, on=['date', 'code'], how='left')
    # æŠ€æœ¯å› å­
    temp = cal_reverse(end, code)
    factor = pd.merge(factor, temp, on=['date', 'code'], how='left')
    # æµåŠ¨æ€§å› å­
    temp = cal_liquidity(date, code, mkt_info)
    factor = pd.merge(factor, temp, on=['date', 'code'], how='left')
    # ç‰¹å¼‚åº¦
    temp = cal_specificity(date, code, mkt_info)
    factor = pd.merge(factor, temp, on=['date', 'code'], how='left')    
    # atrp
    temp = cal_atrp(date, code, mkt_info)
    factor = pd.merge(factor, temp, on=['date', 'code'], how='left')     
    # å¯¹æ•°å¸‚å€¼
    temp = cal_lnmkt(end, code)
    factor = pd.merge(factor, temp, on=['date', 'code'], how='left')
    # è¡Œä¸šè™šæ‹Ÿå˜é‡
    temp = get_industry(date, code)
    factor = pd.merge(factor, temp, on=['date', 'code'], how='left')    
    all_factor.append(factor)

all_factor = pd.concat(all_factor)
# åŸå§‹å› å­æ•´åˆ
all_factor = all_factor.merge(growth, on=['date', 'code'], how='left').merge(earning, on=['date', 'code'], how='left')
# è°ƒæ•´åˆ—çš„æ¬¡åº
tmp1 = all_factor[all_factor.columns[: 13]].copy()
tmp2 = all_factor[['mkt', 'industry']].copy()
tmp3 = all_factor[all_factor.columns[15: ]].copy()
all_factor = pd.concat([tmp1, tmp3, tmp2], axis=1)
all_factor.to_csv(path + 'raw_factor.csv', index=False, encoding='gbk')
toc = time.time()
print('***********åŸå§‹å› å­ç¤ºä¾‹************')
print(all_factor.head(10).to_html())
print ("\n ----- factor Computation time = " + str((toc - tic)) + "s")

# å› å­é¢„å¤„ç†
# ç¼ºå¤±å€¼å¡«å……
def nafill_by_sw1(data, factor_name):
    """
    ç¼ºå¤±å€¼å¡«å……ï¼Œä½¿ç”¨ç”¨ç”³ä¸‡ä¸€çº§è¡Œä¸šä¸­ä½æ•°
    Argsï¼š
        data: å› å­å€¼ï¼ŒDataFrame
        factor_name: å› å­å
    Returnsï¼š
        DataFrame, å¡«å……ç¼ºå¤±å€¼åçš„å› å­å€¼
    """
    data_input = data.copy()
    data_input.loc[:, factor_name] = data_input.loc[:, factor_name].fillna(
        data_input.groupby('industry')[factor_name].transform("median"))

    return data_input

# å› å­é¢„å¤„ç†å‡½æ•°ï¼Œä¸­ä½æ•°å»æå€¼-->å¯¹å¸‚å€¼åŠè¡Œä¸šä¸­æ€§åŒ–-->æ ‡å‡†åŒ–ï¼Œå¾—åˆ°å¤„ç†å¥½çš„å› å­æ•°æ®
def factor_process(factor_name, data, mode):
    """
    Args:
        factor_name: éœ€è¦è¿›è¡Œé¢„å¤„ç†çš„å› å­å
        data: æŸæ—¥çš„åŸå§‹å› å­æ•°æ®
        mode: å¯¹å¸‚å€¼å› å­ä¸éœ€è¦æ‰§è¡Œä¸­æ€§åŒ–è¿‡ç¨‹ï¼Œéœ€è¦ä½œåŒºåˆ†ï¼Œ'yes'ä»£è¡¨ä¸­æ€§åŒ–ï¼Œ'no'ä»£è¡¨ä¸åšä¸­æ€§åŒ–
    Returns:
        data: å¯¹æŒ‡å®šfactor_nameåšå®Œå¤„ç†çš„å› å­æ•°æ®
    """    
    # ä¸­ä½æ•°å»æå€¼
    D_mad = abs(data[factor_name] - data[factor_name].median()).median()
    D_m = data[factor_name].median()
    upper = D_m + 5 * D_mad
    lower = D_m - 5 * D_mad
    temp = [max(lower, min(x, upper)) for x in list(data[factor_name])] # è¾¹ç•Œå‹ç¼© 
    data[factor_name] = temp
    n = list(data.columns).index('mkt')
    # ä¸­æ€§åŒ–
    if mode == 'yes':
        y = np.array(data[factor_name])
        x = np.array(data[data.columns[n: ]]) # å¸‚å€¼åŠ è¡Œä¸š
        x = sm.add_constant(x, has_constant='add')
        model = sm.OLS(y, x, missing='drop')
        results = model.fit()
        data[factor_name] = results.resid
    # æ ‡å‡†åŒ–
    data[factor_name] = (data[factor_name] - data[factor_name].mean()) / (data[factor_name].std())
    return data
  
# åé¢ä¼šç”¨åˆ°å› å­ICæ•°æ®ï¼Œæ‰€ä»¥éœ€è¦æ¬¡æœˆæ”¶ç›Šè®¡ç®—å› å­ICï¼Œè¿™é‡Œè®¡ç®—æ¬¡æœˆæ”¶ç›Š
def cal_month_ret(code, this_month, next_month):
    """
    Args:
        code: å½“æœˆæœ«çš„è‚¡ç¥¨åˆ—è¡¨
        this_month: å½“æœˆæœˆæœ«æ—¶é—´
        next_month: æ¬¡æœˆæœˆæœ«æ—¶é—´
    Returns:
        m_ret: æœˆæœ«è‚¡ç¥¨çš„æ¬¡æœˆæ”¶ç›Šæ•°æ®,åˆ—åä¸ºæ—¥æœŸã€è‚¡ç¥¨ä»£ç ã€æ¬¡æœˆæ”¶ç›Š
    """
    close_tm = DataAPI.MktEqudAdjAfGet(secID=code,beginDate=this_month,endDate=this_month,field=u"secID,closePrice",pandas="1")
    close_tm.columns = ['code', 'close_tm']
    close_nm = DataAPI.MktEqudAdjAfGet(secID=code,beginDate=next_month,endDate=next_month,field=u"secID,closePrice",pandas="1")
    close_nm.columns = ['code', 'close_nm']    
    close = pd.merge(close_tm, close_nm, on='code')
    close['Month_ret'] = close['close_nm'] / close['close_tm'] - 1
    close['date'] = this_month
    m_ret = close[['date', 'code', 'Month_ret']]    
    return m_ret

# å› å­é¢„å¤„ç†
tic = time.time()
factor_stand = []
factor_list = ['BP', 'EPTTM', 'SPTTM', 'ret_20', 'ret_60', 'ILLIQ', 'turn_1M', 'turn_3M', 'IVR', 'ATRP_1M', 'ATRP_3M', 'sue', 'sur',
              'profit_growth_yoy', 'sales_growth_yoy', 'roe', 'delta_roe', 'mkt'] # å› å­é›†åˆ
date_list = sorted(all_factor['date'].unique())
for date in date_list:
    tdata = all_factor[all_factor['date'] == date]
    tdata.reset_index(drop=True ,inplace=True)
    # ç¼ºå¤±å€¼å¡«å……
    for factor_name in factor_list:
        tdata = nafill_by_sw1(tdata, factor_name)
    tdata = tdata.dropna()
    # å°†è¡Œä¸šè½¬æ¢æˆè™šæ‹Ÿå˜é‡
    indu_dummies = pd.get_dummies(tdata['industry'])
    del tdata['industry']
    tdata = pd.concat([tdata, indu_dummies], axis=1)
    # å…ˆå¯¹å¸‚å€¼æ ‡å‡†åŒ–ï¼Œæ–¹ä¾¿åç»­å…¶ä»–å› å­çš„ä¸­æ€§åŒ–
    tdata = factor_process('mkt', tdata, 'no')
    # å…¶ä»–å› å­
    for factor_name in factor_list[: -1]:
        tdata = factor_process(factor_name, tdata, 'yes')
    factor_stand.append(tdata)

factor_stand = pd.concat(factor_stand)
    
month_ret = []
date_list = sorted(list(set(factor_stand['date'])))
for i in range(len(date_list) - 1):
    this_month = date_list[i]
    next_month = date_list[i + 1]
    code = list(factor_stand[factor_stand['date'] == this_month]['code'])
    ret = cal_month_ret(code, this_month, next_month)
    month_ret.append(ret)

month_ret = pd.concat(month_ret) 

# æ ‡å‡†åŒ–å› å­å­˜å‚¨  
factor_stand.sort_values(by=['date', 'code'])
factor_stand.reset_index(drop=True, inplace=True)
n = list(factor_stand.columns).index('mkt')
factor_stand = factor_stand[factor_stand.columns[: n + 1]]
factor_stand = pd.merge(factor_stand, month_ret, on=['date', 'code'], how='left')
factor_stand.fillna(0, inplace=True)    
factor_stand.to_csv(path + 'factor_stand.csv', index=False)
toc = time.time()
print('***************æ ‡å‡†åŒ–å› å­ç¤ºä¾‹***************')
print(factor_stand.head(10).to_html())
print ("\n ----- Computation time = " + str((toc - tic)) + "s")

'''
ç¬¬äºŒéƒ¨åˆ†ï¼šæ”¶ç›Šé¢„æµ‹æ¨¡å‹
è¯¥éƒ¨åˆ†è€—æ—¶ 5åˆ†é’Ÿ
è¯¥éƒ¨åˆ†å†…å®¹æ˜¯å¦‚ä½•å¯¹è‚¡ç¥¨çš„é¢„æœŸæ”¶ç›Šè¿›è¡Œé¢„æµ‹ï¼Œè€Œæˆ‘ä»¬å°†æ”¶ç›Šçš„é¢„æµ‹è½¬åŒ–ä¸ºå› å­çš„å¤åˆï¼Œå…·ä½“åŒ…æ‹¬ï¼š

2.1 å› å­çš„å¤šé‡å…±çº¿æ€§å¤„ç†ï¼šå¯¹ç§°æ­£äº¤

2.2 å› å­æƒé‡çš„åå‘å½’é›¶åŠICIRåŠ æƒæ³•

æ·±åº¦æŠ¥å‘Šç‰ˆæƒå½’ä¼˜çŸ¿æ‰€æœ‰ï¼Œç¦æ­¢ç›´æ¥è½¬è½½æˆ–ç¼–è¾‘åè½¬è½½ã€‚

   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
2.1 å› å­å¤šé‡å…±çº¿æ€§å¤„ç†ï¼šå¯¹ç§°æ­£äº¤

å› å­å…±çº¿æ€§çš„å›°æ‰°
åœ¨å¤šå› å­é€‰è‚¡æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä»è§„æ¨¡ã€æŠ€æœ¯åè½¬ã€æµåŠ¨æ€§ã€æ³¢åŠ¨æ€§ã€ä¼°å€¼ã€æˆé•¿ã€è´¨é‡ç­‰ç»´åº¦æ ¹æ®å¤šä¸ªå› å­çš„çº¿æ€§åŠ æƒæ¥ä¸ºä¸ªè‚¡è¿›è¡Œç»¼åˆæ‰“åˆ†ï¼Œè¿™ä¸ªæ‰“åˆ†æ³•æœ‰ä¸€ä¸ªéšå«å‡è®¾æ˜¯å› å­ä¹‹é—´ç›¸å…³æ€§è¾ƒä½ï¼Œä½†æ˜¯æˆ‘ä»¬ç»˜å‡ºäº†æœ¬æ–‡é€‰å–çš„å› å­åœ¨2010å¹´åˆè‡³2018å¹´8æœˆåº•ç›¸å…³ç³»æ•°çš„å‡å€¼ã€‚ä»ä¸‹å›¾å¯ä»¥çœ‹åˆ°ï¼Œè™½ç„¶æœ‰äº›å› å­å±äºä¸åŒçš„ç»´åº¦ï¼Œä½†æ˜¯ä»ç„¶å­˜åœ¨æ˜æ˜¾çš„ç›¸å…³æ€§ï¼ŒåŒä¸€ç»´åº¦å†…å› å­çš„ç›¸å…³æ€§åˆ™æ›´ä¸ºçªå‡ºï¼Œå¦‚æœç›´æ¥é‡‡ç”¨å¸¸è§çš„åŠ æƒæ–¹æ³•ç›´æ¥å¯¹è¿™äº›å› å­è¿›è¡ŒåŠ æƒï¼Œä¼šå¯¼è‡´åŠ æƒåçš„ç»„åˆæ•´ä½“åœ¨æŸäº›å› å­ä¸Šçš„é‡å¤æš´éœ²ï¼Œä»è€Œä¼šå½±å“ç»„åˆçš„é•¿æœŸè¡¨ç°ã€‚

å› å­æ­£äº¤åŒ–çš„åŸç†
å› å­æ­£äº¤åŒ–ï¼Œæœ¬è´¨ä¸Šæ˜¯å¯¹åŸå§‹å› å­ï¼ˆé€šè¿‡ä¸€ç³»åˆ—çº¿æ€§å˜æ¢ï¼‰è¿›è¡Œæ—‹è½¬ï¼Œæ—‹è½¬åå¾—åˆ°ä¸€ç»„ä¸¤ä¸¤æ­£äº¤çš„æ–°å› å­ï¼Œä»–ä»¬ä¹‹é—´çš„ç›¸å…³æ€§ä¸ºé›¶å¹¶ä¸”å¯¹äºæ”¶ç›Šçš„è§£é‡Šåº¦ï¼ˆå³æ•´ä½“çš„æ–¹å·®ï¼‰ä¿æŒä¸å˜ã€‚ç›¸å…³æ€§ä¸ºé›¶ä¿è¯äº†æ—‹è½¬åçš„å› å­ä¹‹é—´æ²¡æœ‰å…±çº¿æ€§ï¼Œè€Œè§£é‡Šåº¦ä¿æŒä¸å˜ä¿è¯äº†åŸå§‹å› å­åŒ…å«çš„ä¿¡æ¯èƒ½å¦è¢«å®Œå…¨ä¿ç•™ã€‚

æ­£äº¤åŒ–å…¬å¼

 
æ­£äº¤åŒ–æ–¹æ³•
å¸¸è§çš„æ­£äº¤åŒ–æ–¹æ³•å°±æ˜¯æ–½å¯†ç‰¹æ­£äº¤å’Œå¯¹ç§°æ­£äº¤ï¼Œè€Œå¯¹ç§°æ­£äº¤ç›¸æ¯”äºä¼ ç»Ÿçš„æ–½å¯†ç‰¹æ­£äº¤æ³•æœ‰å¦‚ä¸‹ä¼˜ç‚¹ï¼š

ç›¸æ¯”äºæ–½å¯†ç‰¹æ­£äº¤ï¼Œå¯¹ç§°æ­£äº¤ä¸éœ€è¦æä¾›æ­£äº¤æ¬¡åºï¼Œå¯¹æ¯ä¸ªå› å­å¹³ç­‰çœ‹å¾…ï¼›
æ‰€æœ‰æ­£äº¤è¿‡æ¸¡çŸ©é˜µä¸­ï¼Œå¯¹ç§°æ­£äº¤åçš„çŸ©é˜µå’ŒåŸå§‹çŸ©é˜µçš„ç›¸ä¼¼æ€§æœ€å¤§ï¼Œæˆ‘ä»¬ç”¨å˜æ¢å‰åçš„çŸ©é˜µçš„FrobeniusèŒƒæ•°ğœ‘æ¥è¡¡é‡å› å­æ­£äº¤å‰åçš„å˜åŒ–å¤§å°ã€‚åœ¨æ‰€æœ‰è¿‡æ¸¡çŸ©é˜µä¸­ï¼Œå­˜åœ¨å”¯ä¸€è§£ä½¿ğœ‘æœ€å°ï¼Œè¯¥è§£å³ä¸ºå¯¹ç§°æ­£äº¤çš„è¿‡æ¸¡çŸ©é˜µï¼›
å¯¹ç§°æ­£äº¤çš„è®¡ç®—åªéœ€è¦æˆªé¢å› å­æ•°æ®ï¼Œå¹¶ä¸ä¾èµ–å†å²æ•°æ®ï¼Œå› æ­¤è®¡ç®—æ•ˆç‡éå¸¸é«˜ã€‚ ä¸¤ç§æ­£äº¤æ–¹æ³•çš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼š

'''


# å› å­ç›¸å…³æ€§çŸ©é˜µ
begin_point = '20091231'
date_list = sorted(factor_stand['date'].unique())
date_list = [x for x in date_list if x >= begin_point]
mean_corr = pd.DataFrame()
for i in range(len(date_list)):
    tdata = factor_stand[factor_stand['date'] == date_list[i]]
    corr_frame = tdata[tdata.columns[2: -1]].corr()    
    if i == 0:
        mean_corr = corr_frame
    else:
        mean_corr += corr_frame

mean_corr = mean_corr / len(date_list)
mean_corr = mean_corr.round(2)

f, ax= plt.subplots(figsize = (20, 10))
_ = sns.heatmap(mean_corr, alpha=1.0, annot=True, center=0.0, annot_kws={"size": 8}, linewidths=0.02, 
                linecolor='white', linewidth=0,  ax=ax)
title=u'åŸå§‹å› å­çš„ç›¸å…³æ€§çŸ©é˜µ'
_ = ax.set_title(title, fontproperties=font, fontsize=16)


# å¯¹ç§°æ­£äº¤çš„å®ç°ä»£ç 
from numpy import linalg as LA
# å¯¹è¾“å…¥çš„listè¿›è¡Œlowdinæ­£äº¤ï¼ˆï¼‰
def lowdin_orthog_list(x_list):
    '''
    x_list = [x1, x2, x3, ...xk], åŒä¸€ä¸ªæ¨ªæˆªé¢ä¸Šï¼Œkä¸ªå› å­çš„å› å­é›†åˆ
    x1 = [v11, v21, v31, ...vn1], å…¶ä¸­ä¸€ä¸ªå› å­é›†åˆä¸­ï¼Œnä¸ªè‚¡ç¥¨çš„æŸä¸ªå› å­å€¼
    return: å¯¹åº”çš„np.array([x1, x2, x3, ...xn])
    '''
    # å¯¹Xè¿›è¡Œå‡å€¼å½’é›¶åŒ–ï¼Œä»¥ä¾¿äºåœ¨ç®—overlapçŸ©é˜µçš„æ—¶å€™ç›´æ¥ç”¨cov matrix
    x_list = [x-np.array(x).mean() for x in x_list]
    
    # çŸ©é˜µæ ¼å¼, æ ¼å¼ä¸º:
    '''
    [[v11, v21, v31, v41, ...vn1],
     [v21, v22, v32, v42, ...vn2],
     ...
     [v1k, v2k, v3k, v4k, ...vnk]
     ]
    (ç”±äºæ˜¯np.arrayè½¬æˆçš„matrix, æ‰€ä»¥çŸ©é˜µéƒ½æ˜¯è¡Œå‘é‡æ¨¡å¼)
    '''
    factor_array = np.array(x_list)
    cov_m = np.cov(factor_array)
    
    # overlapçŸ©é˜µ
    overlap_m = (len(x_list[0])-1)*cov_m
    
    # æ¥ä¸‹æ¥ï¼Œæ±‚overlapçŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾æ ¹å‘é‡ï¼Œä»¥æ±‚è§£è¿‡åº¦çŸ©é˜µ
    eig_d, eig_u = LA.eig(overlap_m)
    eig_d = np.power(eig_d, -0.5)
    
    # å¤„ç†åçš„ç‰¹å¾æ ¹å¯¹è§’é˜µ
    d_trans = np.diag(eig_d)
    eig_u_T = eig_u.T
    
    # è¿‡æ¸¡çŸ©é˜µ
    transfer_s = np.matrix(eig_u)*d_trans*eig_u_T
    # æœ€ç»ˆï¼Œæ­£äº¤å¤„ç†åçš„çŸ©é˜µ
    out_m = (np.matrix(factor_array).T*transfer_s)
    out_m = np.array(out_m.T)
    return out_m

# æ­£äº¤dataframe
def lowdin_orthog_frame(df, cols):
    '''
    df: åŒ…å«å› å­å€¼çš„dataframeï¼Œç¤ºä¾‹æ ¼å¼ä¸º: [ticker, tradeDate, factor1, factor2, factor3, factor4, ...], å¯ä¸ºæ¨ªæˆªé¢æˆ–è€…panelçš„å› å­æ•°æ®
    cols: éœ€è¦è¿›è¡Œæ­£äº¤çš„åˆ—ï¼Œå¦‚ cols = [factor1,factor2,factor3,factor4...]
    è¿”å›:
        å¯¹colsè¿›è¡Œäº†æ­£äº¤å¤„ç†åçš„dataframeï¼Œæ ¼å¼åŒè¾“å…¥dfå®Œå…¨ä¸€è‡´
    è¯´æ˜ï¼š å¦‚æœdfçš„tradeDateä¸æ­¢ä¸€ä¸ªå€¼ï¼Œåˆ™åˆ†åˆ«åœ¨æ¯ä¸ªtradeDate,å¯¹æ¨ªæˆªé¢çš„å¤šä¸ªå› å­å€¼è¿›è¡Œæ­£äº¤
    '''
    def orthog_tdate_frame(dframe, cols):
        dframe = dframe.copy()
        dframe[cols] = pd.DataFrame(lowdin_orthog_list(np.array(dframe[cols]).T).T, index=dframe.index, columns = [cols])
        return dframe
    
    df = df.groupby(['date']).apply(orthog_tdate_frame, cols)
    df.index = range(len(df))
    return df

# å¯¹åŸå§‹å› å­è¿›è¡Œå¯¹ç§°æ­£äº¤
using_factors = [x for x in factor_stand.columns if x not in ['date', 'code', 'Month_ret']]
all_orth_factor_df = lowdin_orthog_frame(factor_stand, using_factors)


# å¯¹ç§°æ­£äº¤åçš„å› å­ç›¸å…³æ€§çŸ©é˜µ
date_list = sorted(all_orth_factor_df['date'].unique())
date_list = [x for x in date_list if x >= begin_point]
mean_corr = pd.DataFrame()
for i in range(len(date_list)):
    tdata = all_orth_factor_df[all_orth_factor_df['date'] == date_list[i]]
    corr_frame = tdata[tdata.columns[2: -1]].corr()    
    if i == 0:
        mean_corr = corr_frame
    else:
        mean_corr += corr_frame

mean_corr = mean_corr / len(date_list)
mean_corr = mean_corr.round(2)

f, ax= plt.subplots(figsize = (20, 10))
_ = sns.heatmap(mean_corr, alpha=1.0, annot=True, center=0.0, annot_kws={"size": 8}, linewidths=0.02, 
                linecolor='white', linewidth=0,  ax=ax)
title=u'å¯¹ç§°æ­£äº¤åå› å­çš„ç›¸å…³æ€§çŸ©é˜µ'
_ = ax.set_title(title, fontproperties=font, fontsize=16)

'''


ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œå¯¹ç§°æ­£äº¤ä¹‹åçš„å› å­ä¸¤ä¸¤ç›¸å…³æ€§ä¸ºé›¶ï¼Œä¸‹é¢æˆ‘ä»¬æ£€éªŒä¸€ä¸‹å¯¹ç§°æ­£äº¤å‰åå› å­åŠ æƒå¤åˆçš„æ•ˆæœ.

   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
2.2 å› å­æƒé‡çš„åå‘å½’é›¶åŠICIRåŠ æƒæ³•

åŸç†
åœ¨å¤šå› å­æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸ä¼šç”¨ä»¥ä¸‹å‡ ç§å¸¸è§çš„åŠ æƒæ–¹å¼ï¼š

å› å­ICå‡å€¼åŠ æƒ
å› å­ICIRåŠ æƒ
æœ€ä¼˜åŒ–å¤åˆå› å­ICIRåŠ æƒ
åŠè¡°ICåŠ æƒ
é¦–å…ˆæœ¬æ–‡é‡‡ç”¨çš„å› å­åŠ æƒæ–¹æ³•æ˜¯ICIRåŠ æƒï¼Œçª—å£æœŸä¸º12ï¼ŒåŠç”¨è¯¥å› å­è¿‡å»12æœŸçš„IRå€¼ä½œä¸ºè¯¥å› å­åœ¨å½“æœŸçš„æƒé‡ã€‚
å…¶æ¬¡ï¼Œå¤šå› å­é€‰è‚¡ç­–ç•¥é€‰æ‹©çš„å› å­é€šå¸¸éƒ½æœ‰å…¶åˆç†çš„æŠ•èµ„é€»è¾‘ï¼Œä¾‹å¦‚å¯¹äºä¼°å€¼ç±»å› å­ï¼Œä¸€èˆ¬æˆ‘ä»¬éƒ½ä¼šè®¤ä¸ºä½ä¼°å€¼çš„è‚¡ç¥¨æœªæ¥çš„è¡¨ç°è¦ä¼˜äºé«˜ä¼°å€¼çš„è‚¡ç¥¨ç¥¨ï¼Œæ‰€ä»¥å½“æˆ‘ä»¬åœ¨æŸä¸ªæˆªé¢ä¸Šé¢„æœŸé«˜ä¼°å€¼è‚¡ç¥¨å ä¼˜ï¼ˆå’Œé•¿æœŸçš„æŠ•èµ„é€»è¾‘ä¸ä¸€è‡´æ—¶ï¼‰ï¼Œæˆ‘ä»¬å¹¶ä¸å»ºè®®åå‘é…ç½®è¯¥å› å­ï¼Œè€Œé€‰æ‹©å¯¹å½“æœŸçš„å› å­æƒé‡ä½œå½’é›¶å¤„ç†ã€‚

ä¾‹å¦‚ä¸‹å›¾ï¼Œè¿™æ˜¯SPTTMå› å­çš„ICåºåˆ—ä¸æ»šåŠ¨12æœŸICå‡å€¼ï¼Œä»æŠ•èµ„é€»è¾‘ä¸Šæ¥è®²SPTTMçš„ICåº”è¯¥æ˜¯æ­£çš„ï¼Œä½†æ˜¯è¯¥å› å­åœ¨2013å¹´ä¸‹åŠå¹´è‡³2014å¹´ä¸ŠåŠå¹´ä¸­ICçš„æ»šåŠ¨12æœŸå‡å€¼ä¸ºè´Ÿï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨æ”¶ç›Šé¢„æµ‹æ¨¡å‹ä¸­å°†è¯¥å› å­çš„æƒé‡é…ç½®è®¾ä¸º0ï¼Œå³ä¸åå‘ä½¿ç”¨è¯¥å› å­æ¥é¢„æµ‹æ”¶ç›Šã€‚

'''

# è®¡ç®—å› å­ICçš„å‡½æ•°
def cal_ic(factor_name, data, mode=False):
    """
    Args:
        factor_name: éœ€è¦è®¡ç®—ICçš„å› å­åç§°
        data: å› å­æ•°æ®ï¼Œè‡³å°‘éœ€è¦3åˆ—ï¼šæ—¥æœŸã€è‚¡ç¥¨å› å­å€¼ã€è‚¡ç¥¨ä¸‹æœŸæ”¶ç›Š('Month_ret')
        mode: å–å€¼ä¸ºTrueæˆ–è€…False,Trueä»£è¡¨èˆå¼ƒæœ€åä¸€æœŸè®¡ç®—ICï¼ˆæœ€åä¸€æœŸçš„æœªæ¥æœˆåº¦æ”¶ç›Šå¯èƒ½æ²¡æœ‰ï¼‰ï¼ŒFalseä¸èˆå¼ƒ
    Returns:
        IC_data: ICç»“æœï¼Œdataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼ŒICå€¼
    """
    IC = []
    date_list = sorted(list(set(data['date'])))
    if mode:
        date_list = date_list[: -1]

    IC_data = data.groupby(['date']).apply(
        lambda x: x[[factor_name, 'Month_ret']].corr(method='spearman').values[0, 1])
    IC_data.name = 'IC_'+factor_name
    IC_data = IC_data.reset_index()
    return IC_data

def cal_weight(df):
    """
    ç”¨å› å­è¿‡å»NæœŸçš„IRå€¼ä½œä¸ºå½“æœŸå› å­çš„æƒé‡
    Args:
        dfï¼šå› å­æ•°æ®ï¼Œåˆ—åä¸º['date', 'code', å› å­å1, å› å­å2,..., æ¬¡æœˆæ”¶ç›Š]
    Returns:
        all_weightï¼šDataFrameæ ¼å¼ï¼Œåˆ—åä¸ºå› å­å1,å› å­å2,... ,æ—¥æœŸ    
    """
    date_list = sorted(df['date'].unique())
    N = 12
    all_weight = []
    for i in range(N, len(date_list)):
        currentdate = date_list[i]
        period_date = date_list[i - N: i]
        period = df[(df['date'] >= period_date[0]) & (df['date'] <= period_date[-1])]
        # initå°±æ˜¯è¿‡å»Nä¸ªæœˆå„å› å­çš„ICåºåˆ—
        init = pd.DataFrame({'date': period_date})
        for factor_name in period.columns[2: -1]:
            temp = cal_ic(factor_name, period, False)
            init = pd.merge(init, temp, on='date')        
        init = init[init.columns[1: ]]
        weight = np.array(init.mean() / init.std()) # æƒé‡
        weight = pd.DataFrame(weight.reshape(1, len(weight)), columns=period.columns[2: -1])
        weight['date'] = currentdate
        all_weight.append(weight)
    all_weight = pd.concat(all_weight)
    return all_weight

def factor_compose(df, all_weight):
    """
    Args:
        dfï¼šå› å­æ•°æ®ï¼Œåˆ—åä¸º['date', 'code', å› å­å1, å› å­å2,..., æ¬¡æœˆæ”¶ç›Š]
        all_weightï¼šDataFrameæ ¼å¼ï¼Œåˆ—åä¸ºå› å­å1,å› å­å2,... ,æ—¥æœŸ
    Returns:
        frameï¼šDataFrameæ ¼å¼ï¼Œåˆ—åä¸º['date', 'code', 'compose', 'Month_ret']    
    """
    weight = all_weight.copy()
    date_list = sorted(all_weight['date'].unique())
    weight = weight.set_index('date')
    frame = []
    for date in date_list:
        tdata = df[df['date'] == date]
        factor_loading = np.array(tdata[tdata.columns[2: -1]])
        w = np.array(weight.loc[date, :])
        composed_factor = np.dot(factor_loading, w)
        tdata['compose'] = composed_factor
        frame.append(tdata)
    frame = pd.concat(frame)
    frame = frame[['date', 'code', 'compose', 'Month_ret']]
    return frame

# åŸå§‹å› å­åŠå¯¹ç§°æ­£äº¤åŒ–åçš„å› å­åˆæˆ
tic = time.time()
# æ¯æœŸæƒé‡
all_weight = cal_weight(factor_stand)
all_weight.to_csv(path + 'weight.csv', index=False)
# åŸå§‹å¤åˆå› å­
compose_raw = factor_compose(factor_stand, all_weight)
compose_orth = factor_compose(all_orth_factor_df, all_weight)
compose_raw.to_csv(path + 'compose_raw.csv', index=False)
compose_orth.to_csv(path + 'compose_orth.csv', index=False)
# è®¡ç®—æ­£äº¤å‰åçš„å¤åˆå› å­çš„ICæƒ…å†µ
ic_raw = cal_ic('compose', compose_raw, True)
ic_orth = cal_ic('compose', compose_orth, True)
# å› å­æƒé‡åå‘å½’é›¶
weight = all_weight[all_weight.columns[: -1]]
tmp = np.sign(weight * weight.mean()).replace(-1, 0)
weight_zero = np.multiply(weight, tmp)
weight_zero['date'] = all_weight['date']
compose_orth_zero = factor_compose(all_orth_factor_df, weight_zero)
compose_orth_zero.to_csv(path + 'compose_orth_zero.csv', index=False)
ic_orth_zero = cal_ic('compose', compose_orth_zero, True)
# æ•´åˆ
ic_mean = [ic_raw['IC_compose'].mean(), ic_orth['IC_compose'].mean(), ic_orth_zero['IC_compose'].mean()]
icir = [ic_raw['IC_compose'].mean() / ic_raw['IC_compose'].std(), ic_orth['IC_compose'].mean() / ic_orth['IC_compose'].std(),
       ic_orth_zero['IC_compose'].mean() / ic_orth_zero['IC_compose'].std()]
icir = [np.sqrt(12) * x for x in icir]
ic_win = [len(ic_raw[ic_raw['IC_compose'] > 0]) / (len(ic_raw) + 0.0), len(ic_orth[ic_orth['IC_compose'] > 0]) / (len(ic_orth) + 0.0),
         len(ic_orth_zero[ic_orth_zero['IC_compose'] > 0]) / (len(ic_orth_zero) + 0.0)]
ic_count = pd.DataFrame({'Method': [u'åŸå§‹', u'å¯¹ç§°æ­£äº¤', u'å¯¹ç§°æ­£äº¤å¸¦åå‘å½’é›¶'], u'å¤åˆå› å­ICå‡å€¼': ic_mean, 
                         u'å¤åˆå› å­å¹´åŒ–ICIR': icir, u'å¤åˆå› å­ICèƒœç‡': ic_win})
toc = time.time()
print('***********ICæŒ‡æ ‡************')
print(ic_count.round(4).to_html())
print ("\n ----- Computation time = " + str((toc - tic)) + "s")

'''
ICç»“è®º

å¯¹ç§°æ­£äº¤åçš„å¤åˆå› å­ICå‡å€¼ä»0.1285ä¸Šå‡åˆ°0.1312ï¼ŒICIRä»6.08æ˜¾è‘—æå‡åˆ°äº†6.74ï¼ŒICçš„èƒœç‡ä¹Ÿæœ‰æå‡ï¼Œå¯è§å¯¹ç§°æ­£äº¤å¯¹å› å­çš„å¤åˆæ•ˆæœæœ‰æ˜¾è‘—æå‡ï¼›
å¸¦åå‘å½’é›¶å¤„ç†ä½¿å¾—å¤åˆå› å­çš„ç¨³å¥å‹å¾—åˆ°è¿›ä¸€æ­¥æå‡ï¼Œå¤åˆå› å­çš„ICIRæå‡è‡³6.93ã€‚
   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
ç¬¬ä¸‰éƒ¨åˆ†ï¼šé£é™©æ§åˆ¶æ¨¡å‹ä¸ç»„åˆä¼˜åŒ–
è¯¥éƒ¨åˆ†è€—æ—¶ 60åˆ†é’Ÿ
è¯¥éƒ¨åˆ†å†…å®¹æ˜¯å¦‚ä½•åœ¨å„ç§é£é™©çº¦æŸä¸‹å®ç°ç»„åˆä¼˜åŒ–ï¼Œæ ¹æ®é£é™©çº¦æŸçš„åŒºåˆ«ï¼ŒåŒ…æ‹¬å¦‚ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š

3.1 é£é™©æ§åˆ¶æ¨¡å‹åŠé™æ€æŒ‡æ•°å¢å¼ºæ¨¡å‹

3.2 åŸºäºè‡ªé€‚åº”é£é™©æ§åˆ¶çš„æŒ‡æ•°å¢å¼ºæ¨¡å‹

3.3 ç»„åˆå›æµ‹åŠåˆ†æ

æ·±åº¦æŠ¥å‘Šç‰ˆæƒå½’ä¼˜çŸ¿æ‰€æœ‰ï¼Œç¦æ­¢ç›´æ¥è½¬è½½æˆ–ç¼–è¾‘åè½¬è½½ã€‚

   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
3.1 é£é™©æ§åˆ¶æ¨¡å‹åŠé™æ€æŒ‡æ•°å¢å¼ºæ¨¡å‹

é£é™©æ§åˆ¶æ¨¡å‹

ç¨³å¥çš„æ”¶ç›Šé¢„æµ‹æ¨¡å‹æ˜¯å¤šå› å­é€‰è‚¡ç­–ç•¥æˆåŠŸçš„åŸºçŸ³ï¼Œä½†æ˜¯å¦‚æœä»…é€‰ç”¨å¾—åˆ†æœ€é«˜çš„ä¸€ç¯®å­è‚¡ç¥¨æ„å»ºç»„åˆï¼Œåœ¨ä¸€äº›æç«¯å¸‚åœºç¯å¢ƒä¸‹å¯èƒ½ä¼šäº§ç”Ÿè¾ƒå¤§çš„å›æ’¤é£é™©ï¼Œå› æ­¤éœ€è¦å¯¹ç»„åˆè¿›è¡Œé£é™©æ§åˆ¶ï¼Œé¿å…ç»„åˆåœ¨æŸäº›é£æ ¼æˆ–è¡Œä¸šä¸Šæœ‰è¿‡å¤§çš„æš´éœ²ã€‚å¸¸è§çš„é£é™©æ§åˆ¶å½¢å¼ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ç§ï¼šé£æ ¼æš´éœ²çº¦æŸã€è¡Œä¸šæš´éœ²çº¦æŸã€ç›¸å¯¹äºåŸºå‡†çš„è·Ÿè¸ªè¯¯å·®çº¦æŸã€ä¸ªè‚¡æƒé‡çº¦æŸç­‰ã€‚è¿™äº›çº¦æŸæ¡ä»¶éƒ½èƒ½æœ‰æ•ˆæ§åˆ¶ç»„åˆç›¸å¯¹åŸºå‡†æŒ‡æ•°çš„åç¦»ï¼Œä½¿ç»„åˆèƒ½ç¨³å®šåœ°æˆ˜èƒœåŸºå‡†æŒ‡æ•°ã€‚

æœ¬æ–‡é‡‡ç”¨çš„ç»„åˆä¼˜åŒ–æ¨¡å‹å¦‚ä¸‹ï¼š


è¯¥ä¼˜åŒ–é—®é¢˜çš„ç›®æ ‡å‡½æ•°ä¸ºæœ€å¤§åŒ–ç»„åˆçš„é¢„æœŸæ”¶ç›Šï¼Œæ¨¡å‹å¯¹åº”çš„çº¦æŸæ¡ä»¶å¦‚ä¸‹ï¼š

ç¬¬ä¸€ä¸ªçº¦æŸæ¡ä»¶é™åˆ¶äº†ç»„åˆç›¸å¯¹äºåŸºå‡†æŒ‡æ•°çš„é£æ ¼åç§»ï¼›
ç¬¬äºŒä¸ªçº¦æŸæ¡ä»¶é™åˆ¶äº†ç»„åˆç›¸å¯¹åŸºå‡†æŒ‡æ•°çš„è¡Œä¸šåç¦»ï¼›
ç¬¬ä¸‰ä¸ªçº¦æŸæ¡ä»¶é™åˆ¶äº†ç»„åˆç›¸å¯¹äºåŸºå‡†æŒ‡æ•°æˆåˆ†è‚¡çš„åç¦»ï¼›
ç¬¬å››ä¸ªçº¦æŸé™åˆ¶äº†ç»„åˆåœ¨æˆä»½è‚¡åœ¨æƒé‡å æ¯”çš„ä¸Šé™åŠä¸‹é™ï¼›
ç¬¬äº”ä¸ªçº¦æŸé™åˆ¶äº†å–ç©ºï¼Œå¹¶ä¸”é™åˆ¶äº†ä¸ªè‚¡æƒé‡çš„ä¸Šçº¿ä¸ºlï¼›
ç¬¬å…­ä¸ªçº¦æŸè¦æ±‚ç»„åˆçš„æƒé‡å’Œä¸º1ï¼Œå³ç»„åˆå§‹ç»ˆæ»¡ä»“è¿ä½œã€‚
è·Ÿä»¥å¾€çš„ç»„åˆä¼˜åŒ–æ¨¡å‹æœ‰åŒºåˆ«çš„æ˜¯ï¼Œæˆ‘ä»¬æ‘’å¼ƒäº†äºŒæ¬¡é¡¹çš„è·Ÿè¸ªè¯¯å·®çº¦æŸæ¥æ§åˆ¶ç»„åˆå¯¹åŸºå‡†çš„åç¦»ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ç”¨ä¸ªè‚¡ç›¸å¯¹åŸºå‡†æŒ‡æ•°æˆä»½è‚¡çš„åç¦»åº¦ï¼Œè¿™æœ‰ä¸¤ä¸ªæ–¹é¢çš„è€ƒè™‘ï¼š

ç›´æ¥ç”¨è·Ÿè¸ªè¯¯å·®ä½œä¸ºçº¦æŸæ¡ä»¶è¿›è¡Œé£é™©æ§åˆ¶éœ€è¦ä¼°è®¡åæ–¹å·®çŸ©é˜µï¼Œå¯¹è·Ÿè¸ªè¯¯å·®çš„æ§åˆ¶æ˜¯å¦æˆåŠŸä¾èµ–äºåæ–¹å·®çŸ©é˜µçš„ä¼°è®¡å‡†ç¡®æ€§ï¼›è€Œç›´æ¥æ§åˆ¶ä¸ªè‚¡ç›¸å¯¹åŸºå‡†æŒ‡æ•°æˆåˆ†è‚¡åç¦»åº¦å¯¹ç»„åˆçš„è·Ÿè¸ªè¯¯å·®æ§åˆ¶çš„ä¼ å¯¼æœºåˆ¶æ›´ç›´æ¥ï¼Œä¸ªè‚¡åç¦»åº¦è¶Šå°ï¼Œå¯¹åŸºå‡†æŒ‡æ•°çš„è·Ÿè¸ªè¯¯å·®å°±è¶Šå°ï¼Œæç«¯æƒ…å†µä¸‹ï¼Œå°†ä¸ªè‚¡ç›¸å¯¹åŸºå‡†æˆåˆ†è‚¡æƒé‡çš„åç¦»è®¾ä¸º0æ—¶ï¼Œç»„åˆå³å®Œå…¨å¤åˆ¶åŸºå‡†æŒ‡æ•°ï¼Œæ­¤æ—¶è·Ÿè¸ªè¯¯å·®ä¸ºé›¶ï¼›
è·Ÿè¸ªè¯¯å·®çº¦æŸæ˜¯äºŒæ¬¡é¡¹çº¦æŸï¼Œéœ€è¦ç”¨äºŒé˜¶é”¥è§„åˆ’æ¥æ±‚è§£ï¼Œè€Œä¸Šè¿°æ¨¡å‹ä¸­ç›®æ ‡å‡½æ•°ã€ä¸ªè‚¡æƒé‡åç¦»çº¦æŸã€æˆåˆ†è‚¡æƒé‡å æ¯”çº¦æŸç­‰éƒ½æ˜¯çº¿æ€§çš„ï¼Œçº¿æ€§è§„åˆ’é—®é¢˜çš„æ±‚è§£æ¯”äºŒé˜¶é”¥è§„åˆ’çš„æ±‚è§£æ›´é«˜æ•ˆï¼Œå°¤å…¶åœ¨å˜é‡æ•°æ€¥å‰§å¢åŠ çš„æ—¶å€™ã€‚
æ³¨ï¼šç ”æŠ¥ä¸­çš„æŒ‡æ•°å¢å¼ºæ¨¡å‹çš†æ˜¯åœ¨å…¨Açš„æŠ•èµ„åŸŸä¸­è¿›è¡Œé€‰è‚¡çš„ï¼Œå› æ­¤ä¸ªè‚¡åç¦»åº¦åˆ°è·Ÿè¸ªè¯¯å·®çš„ä¼ å¯¼å¹¶ä¸æ˜¯å®Œå…¨ç›´æ¥çš„ï¼Œå› ä¸ºè¦è€ƒè™‘åˆ°æˆåˆ†è‚¡ä¹‹å¤–çš„è‚¡ç¥¨çš„é£é™©å½±å“ï¼ˆä¾‹å¦‚æŸåªæŒ‡æ•°æˆåˆ†è‚¡ä¹‹å¤–çš„è‚¡ç¥¨çš„æƒé‡ä¸º1%ï¼Œé‚£ä¹ˆåç¦»åº¦å°±æ˜¯1%ï¼Œä½†æ˜¯è¿™åªè‚¡ç¥¨çš„é£é™©ä¸èƒ½ç”¨æŒ‡æ•°æˆä»½è‚¡çš„é£é™©çŸ©é˜µä¼°è®¡ï¼‰ï¼Œä½†æ˜¯æ­£å‘å…³ç³»è¿˜æ˜¯å­˜åœ¨çš„ã€‚
é™æ€æŒ‡æ•°å¢å¼ºæ¨¡å‹
ä¸‹é¢æˆ‘ä»¬æ ¹æ®å‰æ–‡ä»‹ç»çš„æ”¶ç›Šé¢„æµ‹æ¨¡å‹ã€é£é™©æ§åˆ¶æ¨¡å‹ï¼Œå¯¹æ²ªæ·±300ã€ä¸­è¯500è¿™ä¸¤ä¸ªæŒ‡æ•°åˆ©ç”¨ç»„åˆä¼˜åŒ–æ¨¡å‹è¿›è¡Œå¢å¼ºå®è¯å›æµ‹ã€‚
æˆ‘ä»¬å¯¹ç»„åˆä¼˜åŒ–åŠå›æµ‹è®¾å®šçš„å‚æ•°å¦‚ä¸‹ï¼š

å›æµ‹åŒºé—´ä»2010å¹´åˆè‡³2018å¹´8æœˆ31æ—¥
åœ¨å‰”é™¤STåŠæ–°è‚¡åçš„å…¨AæŠ•èµ„åŸŸè¿›è¡Œé€‰è‚¡
äº¤æ˜“è´¹ç”¨åŒè¾¹0.2%
é£é™©ç»´åº¦æˆ‘ä»¬éœ€è¦å¸‚å€¼ä¸åŸºå‡†è¡Œä¸šå®Œå…¨ä¸€è‡´ï¼Œè¡Œä¸šæš´éœ²æ•å£æœ€å¤§ä¸º0.005ï¼ˆä¸€èˆ¬ä¼šé™åˆ¶åˆ°0.001ä»¥å†…ï¼‰
 æ³¨æ„äº‹é¡¹ 

 ç¬¬ä¸‰éƒ¨åˆ†æ¶‰åŠå›æµ‹åŠå¤šè¿›ç¨‹ä¼˜åŒ–ï¼Œæ¶ˆè€—èµ„æºè¾ƒå¤§ï¼Œéœ€è¦é‡å¯ç ”ç©¶ç¯å¢ƒä»¥é‡Šæ”¾èµ„æº

ä¹‹å‰çš„æ•°æ®éƒ½è¿›è¡Œäº†å­˜å‚¨ï¼Œä¸‹é¢çš„ä»£ç å¯ä»¥ç›´æ¥è¿è¡Œè€Œä¸éœ€è¦é‡è·‘ä¸Šé¢çš„ä»£ç 

é‡å¯ç ”ç©¶ç¯å¢ƒçš„æ­¥éª¤ä¸ºï¼š

ç½‘é¡µç‰ˆï¼šå…ˆç‚¹å‡»å·¦ä¸Šè§’çš„â€œNotebookâ€å›¾æ ‡ï¼Œç„¶åç‚¹å‡»å·¦ä¸‹è§’çš„â€œå†…å­˜å ç”¨x%â€å›¾æ ‡ï¼Œåœ¨å¼¹æ¡†ä¸­ç‚¹å‡»é‡å¯ç ”ç©¶ç¯å¢ƒ
å®¢æˆ·ç«¯ï¼šç‚¹å‡»å·¦ä¸‹è§’çš„â€œå†…å­˜x%â€, åœ¨å¼¹æ¡†ä¸­ç‚¹å‡»é‡å¯ç ”ç©¶ç¯å¢ƒ

'''

# coding: utf-8

import numpy as np
import pandas as pd
import datetime
import time
import os
import copy
import cvxpy as cvx
import matplotlib.pyplot as plt
import statsmodels.api as sm
import scipy.stats as st
import seaborn as sns
from multiprocessing import Pool
import cPickle as pickle
from CAL.PyCAL import *    # CAL.PyCALä¸­åŒ…å«font
universe = set_universe('A')
cal = Calendar('China.SSE')

# æ—¶é—´æ ¼å¼è½¬å˜å‡½æ•°
def time_change(x):
    y = datetime.datetime.strptime(x, '%Y-%m-%d')
    y = y.strftime('%Y%m%d')
    return y

# è·å–çº¦æŸæ‰€éœ€æ•°æ®
def get_size_con_data(df, date, index_ticker):
    """
    Args:
        df: æœˆæœ«è‚¡ç¥¨æ± æ•°æ®
        date: æœˆæœ«æ—¶é—´
        index_ticker: æŒ‡æ•°ä»£ç 
    Returns:
        mkt: å¯¹æ•°å¸‚å€¼æ•°æ®ï¼Œdataframeï¼Œåˆ—åä¸ºè‚¡ç¥¨ä»£ç ï¼Œå¯¹æ•°æµé€šå¸‚å€¼
        sh: åŸºå‡†æŒ‡æ•°çš„å¯¹æ•°å¸‚å€¼æš´éœ²
    """ 
    # å¸‚å€¼æš´éœ²å‘é‡
    code = list(df['code'])
    mkt = DataAPI.MktEqudGet(tradeDate=date,secID=code,field=u"tradeDate,secID,marketValue",pandas="1")
    mkt['marketValue'] = np.log(mkt['marketValue'])
    mkt = mkt[['secID', 'marketValue']]
    mkt.columns = ['code', 'mkt']
    mkt = mkt.sort_values(by=['code'])
    mkt.reset_index(drop=True, inplace=True)
    #ã€€æŒ‡æ•°å¸‚å€¼æš´éœ²
    bench = DataAPI.IdxCloseWeightGet(ticker=index_ticker, beginDate=date, endDate=date, field=u"consID,weight",pandas="1")
    bench.columns = ['code', 'weight']
    bench['weight'] = bench['weight'] / 100
    mkt_b = DataAPI.MktEqudGet(tradeDate=date,secID=list(bench['code']),field=u"tradeDate,secID,marketValue",pandas="1")
    mkt_b['marketValue'] = np.log(mkt_b['marketValue'])
    mkt_b = mkt_b[['secID', 'marketValue']]
    mkt_b.columns = ['code', 'mkt_bench']
    tmp = pd.merge(mkt, bench, on=['code'])
    sh = (tmp['mkt'] * tmp['weight']).sum() # æŒ‡æ•°çš„å¸‚å€¼æš´éœ²
    return mkt, sh

# è·å–æŸä¸ªæ—¶ç‚¹è‚¡ç¥¨æ‰€å±è¡Œä¸š
def get_industry_end(date, code):
    """
    Args:
        date: æœˆæœ«æ—¶é—´
        code: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚['000001.XSHE', '000002.XSHE']
    Returns:
        indu: ç”³ä¸‡ä¸€çº§è¡Œä¸šå› å­æ•°æ®,dataframeæ ¼å¼ï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼Œè¡Œä¸šå
    """  
    indu = DataAPI.MdSwBackGet(secID=code,field=u"secID,isNew,oldTypeName,industryName1,intoDate,outDate",pandas="1")
    indu['outDate'].fillna('2050-01-01', inplace=True)
    indu['intoDate'] = map(time_change, indu['intoDate'])
    indu['outDate'] = map(time_change, indu['outDate'])
    indu = indu[(indu['intoDate'] <= date) & (indu['outDate'] > date)]
    indu.drop_duplicates(subset=['secID'], inplace=True)
    indu['date'] = date
    indu = indu[['date', 'secID', 'industryName1']]
    indu.columns = ['date', 'code', 'industry']
    return indu

def get_indu_con_data(df, date, index_ticker):
    """
    Args:
        df: æœˆæœ«è‚¡ç¥¨æ± æ•°æ®
        date: æœˆæœ«æ—¶é—´
        index_ticker: æŒ‡æ•°ä»£ç 
    Returns:
        all_indu: è¡Œä¸šè™šæ‹Ÿå˜é‡æ•°æ®
    """ 
    # è¡Œä¸šæš´éœ²å‘é‡
    code = list(df['code'])
    indu = get_industry_end(date, code)
    indu.set_index('code', inplace=True)
    indu_dummies = pd.get_dummies(indu['industry'])
    H = indu_dummies.T
    # æŒ‡æ•°è¡Œä¸šæš´éœ²
    bench = DataAPI.IdxCloseWeightGet(ticker=index_ticker, beginDate=date, endDate=date, field=u"consID,weight",pandas="1")
    bench.columns = ['code', 'weight']
    bench['weight'] = bench['weight'] / 100
    indu_b = get_industry_end(date, list(bench['code']))
    data = pd.merge(bench, indu_b, on='code')
    index_indu = data.groupby(by='industry').sum()
    all_indu = pd.merge(H, index_indu, left_index=True, right_index=True, how='left').fillna(0)
    return all_indu

def get_stock_diff(df, date, index_ticker):
    """
    Args:
        df: æœˆæœ«è‚¡ç¥¨æ± æ•°æ®
        date: æœˆæœ«æ—¶é—´
        index_ticker: æŒ‡æ•°ä»£ç 
    Returns:
        w: ç»„åˆæœˆåŸºå‡†æŒ‡æ•°çš„ä¸ªè‚¡æƒé‡å·®å¼‚
    """ 
    bench = DataAPI.IdxCloseWeightGet(ticker=index_ticker, beginDate=date, endDate=date, field=u"consID,weight",pandas="1")
    bench.columns = ['code', 'weight']
    bench['weight'] = bench['weight'] / 100
    w = pd.merge(df[['code']], bench, on='code', how='left')
    w.fillna(0, inplace=True)
    return w

# åˆ©ç”¨çº¿æ€§è§„åˆ’æ±‚è§£ç»„åˆ
# ä¸ºäº†åº”ç”¨å¤šè¿›ç¨‹ï¼Œæˆ‘ä»¬å°†å•æœŸçš„ç»„åˆä¼˜åŒ–å‡½æ•°çš„å–æ•°æ®ä¸è®¡ç®—æ¨¡å—æ‹†å¼€
def prepare_optimal_data(df, date, index_ticker):
    """
    çº¿æ€§è§„åˆ’å–æ•°æ®å‡½æ•°
    Argsï¼š
        dfï¼šå…¨ä½“å› å­æ•°æ®ï¼ŒDataFrameæ ¼å¼ï¼Œåˆ—åä¸º['date', 'code', 'compose', 'Month_ret']
        dateï¼šæœˆæœ«æ—¥æœŸ
        index_tickerï¼šæŒ‡æ•°ç¼–å·ï¼Œæ¯”å¦‚æ²ªæ·±300å°±æ˜¯'000300'
    Returnsï¼š
        rï¼šé¢„æœŸæ”¶ç›Šå‘é‡
        tmpï¼šå¸‚å€¼çº¦æŸåŠæƒé‡å’Œä¸º1çº¦æŸæ•°æ®
        induï¼šè¡Œä¸šçº¦æŸæ•°æ®
        wï¼šä¸ªè‚¡åç¦»åº¦çº¦æŸæ•°æ®
    """ 
    factor = df[df['date'] == date]
    factor = factor.sort_values(by=['code'])
    factor.reset_index(drop=True, inplace=True)    
    # é¢„æœŸæ”¶ç›Šå‘é‡
    r = np.array(factor['compose'])
    # é£æ ¼æš´éœ²çº¦æŸï¼ˆçº¦æŸå¸‚å€¼ä¸­æ€§ï¼‰
    size, sh = get_size_con_data(factor, date, index_ticker)
    size['cosntant'] = 1
    tmp = size.set_index('code').T
    tmp['weight'] = [sh, 1]  
    # è¡Œä¸šä¸­æ€§çº¦æŸ
    indu = get_indu_con_data(factor, date, index_ticker)    
    # ä¸ªè‚¡åç¦»åº¦çº¦æŸ
    w = get_stock_diff(factor, date, index_ticker)    
    return r, tmp, indu, w

def single_period_allocation(arg):
    """
    ç»„åˆä¼˜åŒ–å‡½æ•°
    Argsï¼š
        argï¼šå‚æ•°é›†åˆï¼Œåˆ†åˆ«ä¸ºtmp, indu, w, dev,å…·ä½“å¦‚ä¸‹
        rï¼šé¢„æœŸæ”¶ç›Šå‘é‡
        tmpï¼šå¸‚å€¼çº¦æŸåŠæƒé‡å’Œä¸º1çº¦æŸæ•°æ®
        induï¼šè¡Œä¸šçº¦æŸæ•°æ®
        w_conï¼šä¸ªè‚¡åç¦»åº¦çº¦æŸæ•°æ®
        devï¼šä¸ªè‚¡åç¦»åº¦
        dateï¼šæ—¥æœŸ
    Returnsï¼š
        wï¼šæ¯æœŸç»„åˆæƒé‡æ•°æ®
        res.successï¼šæ¯æœŸä¼˜åŒ–çŠ¶æ€
    """ 
    r, tmp, indu, w_con, dev, date = arg
    w = cvx.Variable(len(r))
    size = np.array(tmp.iloc[0, : -1]).reshape(1, tmp.shape[1] - 1)
    w_con['upper'] = w_con['weight'] + dev
    w_con['lower'] = np.where(w_con['weight'] - dev < 0, 0, w_con['weight'] - dev)    
    # ç›®æ ‡å‡½æ•°
    obj = cvx.Maximize(r.reshape(1, len(r)) * w)
    # å¾ªç¯æ”¾å®½çº¦æŸ    
    delta = 0.001
    indu_expo_init = 0.001
    for i in range(5): # è¡Œä¸šåç§»ä¸Šçº¿æš‚æ—¶è®¾å®šä¸º0.005
        indu_expo = indu_expo_init + i * delta
        constraint = [cvx.sum(w) == 1.0,
                      size * w <= tmp['weight'][0],
                      size * w >= tmp['weight'][0],
                      np.array(indu[indu.columns[: -1]]) * w <= np.array(indu['weight'] + indu_expo),
                      np.array(indu[indu.columns[: -1]]) * w >= np.array(indu['weight'] - indu_expo),
                      w <= np.array(w_con['upper'].values),
                      w >= np.array(w_con['lower'].values)
                      ]
        prob = cvx.Problem(obj, constraint)
        prob.solve(solver=cvx.ECOS, verbose=False, max_iters=3000)
        if prob.status == 'optimal':
            w_con['optimal_weight'] = w.value    
            w_con['date'] = date
            w_con = w_con[['date', 'code', 'optimal_weight']]
            break
        if (prob.status != 'optimal') & (i == 4):
            break
    
    return w_con, dev, prob.status
    
def get_mkt_data(df, date, index_ticker):
    """
    Argsï¼š
        dfï¼šå…¨ä½“å› å­æ•°æ®ï¼ŒDataFrameæ ¼å¼ï¼Œåˆ—åä¸º['date', 'code', 'compose', 'Month_ret']
        dateï¼šæœˆæœ«æ—¥æœŸ
        index_tickerï¼šæŒ‡æ•°ç¼–å·ï¼Œæ¯”å¦‚æ²ªæ·±300å°±æ˜¯'000300'
    Returnsï¼š
        pctï¼šè‚¡ç¥¨æ¯æ—¥æ¶¨è·Œå¹…æ•°æ®
        benchï¼šåŸºå‡†æ¶¨è·Œå¹…åºåˆ—æ•°æ®
    """ 
    date_list = sorted(df['date'].unique())
    pre_date = date_list[date_list.index(date) - 2] # T-2
    factor = df[df['date'] == date]
    code = list(factor['code'])
    pct = DataAPI.MktEqudGet(secID=code,beginDate=pre_date,endDate=date,field=u"secID,tradeDate,chgPct",pandas="1")
    pct = pct[pct['secID'].str[0].isin(['0', '3', '6'])]
    pct.columns = ['code', 'date', 'ret']
    pct = pct.pivot(index='code', columns='date', values='ret')
    pct = pct[pct.columns[1: ]]
    pct.fillna(0,inplace=True)
    # ç»„åˆ
    pct = (1 + pct).cumprod(axis=1)
    pct['code'] = pct.index
    # åŸºå‡†
    bench = DataAPI.MktIdxdGet(ticker=index_ticker,beginDate=pre_date,endDate=date,field=u"tradeDate,closeIndex",pandas="1")
    bench['ret_bench'] = bench['closeIndex'] / bench['closeIndex'].shift(1) - 1
    bench = bench[1: ]
    bench.columns = ['date', 'close', 'ret_bench']
    return pct, bench

def cal_tracking_error(pct, bench, weight):
    """
    Argsï¼š
        pctï¼šè‚¡ç¥¨æ¯æ—¥æ¶¨è·Œå¹…æ•°æ®
        benchï¼šåŸºå‡†æ¶¨è·Œå¹…åºåˆ—æ•°æ®
        weightï¼šä¼˜åŒ–å¥½çš„ç»„åˆæƒé‡
    Returnsï¼š
        teï¼šç»„åˆçš„å¹´åŒ–è·Ÿè¸ªè¯¯å·®
    """ 
    
    pct = pd.merge(pct, weight, on='code')
    days = [x for x in pct.columns if x not in ['date', 'code', 'optimal_weight']]
    capital = []
    for day in days:
        capital.append((pct[day] * pct['optimal_weight']).sum())
    portfolio = pd.DataFrame({'date': days, 'capital': capital})
    portfolio['temp'] = portfolio['capital'].shift(1).fillna(1)
    portfolio['ret'] = portfolio['capital'] / portfolio['temp'] - 1
    portfolio = pd.merge(portfolio, bench, on='date')
    portfolio['excess'] = portfolio['ret'] - portfolio['ret_bench']
    te = portfolio['excess'].std() * np.sqrt(252)
    return te

def portfolio_get(pickle_data, static_dev, target_te):
    """
    ä»pickleæ–‡ä»¶ä¸­æå–ä¿¡æ¯
    Argsï¼š
        pickle_dataï¼šä¼˜åŒ–ç»“æœpickleæ–‡ä»¶
        static_devï¼šé»˜è®¤çš„é™æ€çš„ä¸ªè‚¡åç¦»åº¦ï¼Œstræ ¼å¼ï¼Œä¾‹å¦‚'0.02'
        target_teï¼š é¢„æœŸè·Ÿè¸ªè¯¯å·®ä¸Šé™ï¼Œä¾‹å¦‚0.03
    Returnsï¼š
        static_portï¼šé™æ€ç»„åˆï¼ŒDataFrameï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼Œä¼˜åŒ–æƒé‡
        dynamic_portï¼šåŠ¨æ€ç»„åˆï¼ŒDataFrameï¼Œåˆ—åä¸ºæ—¥æœŸï¼Œè‚¡ç¥¨ä»£ç ï¼Œä¼˜åŒ–æƒé‡
    """
    date_list = sorted(pickle_data.keys())
    static_port = pd.DataFrame()
    dynamic_port = pd.DataFrame()
    for date in date_list:
        single_data = pickle_data[date]
        dev_list = sorted(single_data.keys())
        status = []
        tracking_error = []
        for dev in dev_list:
            status.append(single_data[dev]['status'])
            tracking_error.append(single_data[dev]['tracking_error'])
        df = pd.DataFrame({'dev': dev_list, 'status': status, 'tracking_error': tracking_error})
        df['date'] = date
        df = df[['date', 'dev', 'status', 'tracking_error']]
        # é™æ€ç»„åˆ
        static_port = static_port.append(single_data[static_dev]['weight'])
        # åŠ¨æ€ç»„åˆ
        temp = df[df['status'] == 'optimal']
        if float(temp['tracking_error'].min()) > target_te:
            dynamic_dev = temp['dev'].min()
        else:
            temp = temp[temp['tracking_error'] <= target_te]
            dynamic_dev = temp['dev'].max()
        dynamic_port = dynamic_port.append(single_data[dynamic_dev]['weight'])
    static_port.rename(columns={'code': 'secID'}, inplace=True)
    dynamic_port.rename(columns={'code': 'secID'}, inplace=True)
    static_port['date'] = static_port['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y%m%d').strftime('%Y-%m-%d'))
    dynamic_port['date'] = dynamic_port['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y%m%d').strftime('%Y-%m-%d'))
    return static_port, dynamic_port


begin_point = '20091231'
path = 'enhance_strategy_data/'
compose_orth_zero = pd.read_csv(path + 'compose_orth_zero.csv', dtype={"date": np.str})


# å¤šè¿›ç¨‹å®ç°ä¸åŒä¸ªè‚¡åç¦»åº¦ä¸‹çš„ç»„åˆä¼˜åŒ–ï¼šæ²ªæ·±300å¢å¼ºç»„åˆ
if __name__ == '__main__':
    tic = time.time()
    date_list = sorted(compose_orth_zero['date'].unique())
    date_list = [x for x in date_list if x >= begin_point]
    # æ²ªæ·±300å¢å¼º
    index_ticker = '000300'
    results = {}
    for date in date_list:
        # æ•°æ®å‡†å¤‡
        r, tmp, indu, w = prepare_optimal_data(compose_orth_zero, date, index_ticker)
        # åç¦»åº¦é›†åˆä¸‹çš„å¤šè¿›ç¨‹è®¡ç®—
        dev_list = list(np.linspace(0.005, 0.02, 16))
        arg_list = []
        for dev in dev_list:
            arg_list.append((r, tmp, indu, w, dev, date))
        pool = Pool(processes=16)
        res = pool.map(single_period_allocation, arg_list)
        pool.close()
        pool.join()
        pct, bench = get_mkt_data(compose_orth_zero, date, index_ticker)
        temp = {}
        for k in range(len(res)):
            weight = res[k][0]
            if 'optimal_weight' in list(weight.columns):
                te = cal_tracking_error(pct, bench, weight) # è·Ÿè¸ªè¯¯å·®è®¡ç®—
                weight = weight[weight['optimal_weight'] > 1e-8]
            else:
                te = np.nan
            temp[str(res[k][1])] = {"weight": weight, "status": res[k][2], "tracking_error": te}
        results[date] = temp

    # å­˜å‚¨
    with open(path + 'HS300_weight.pkl', 'wb') as f:
        pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)
    toc = time.time()
    print ("\n ----- Computation time = " + str((toc - tic)) + "s")
    
    
# å¤šè¿›ç¨‹å®ç°ä¸åŒä¸ªè‚¡åç¦»åº¦ä¸‹çš„ç»„åˆä¼˜åŒ–ï¼šä¸­è¯500å¢å¼ºç»„åˆ
if __name__ == '__main__':
    tic = time.time()
    date_list = sorted(compose_orth_zero['date'].unique())
    date_list = [x for x in date_list if x >= begin_point]
    # ä¸­è¯500å¢å¼º
    index_ticker = '000905'
    results = {}
    for date in date_list:
        # æ•°æ®å‡†å¤‡
        r, tmp, indu, w = prepare_optimal_data(compose_orth_zero, date, index_ticker)
        # åç¦»åº¦é›†åˆä¸‹çš„å¤šè¿›ç¨‹è®¡ç®—
        dev_list = list(np.linspace(0.001, 0.005, 5))
        arg_list = []
        for dev in dev_list:
            arg_list.append((r, tmp, indu, w, dev, date))
        pool = Pool(processes=16)
        res = pool.map(single_period_allocation, arg_list)
        pool.close()
        pool.join()
        pct, bench = get_mkt_data(compose_orth_zero, date, index_ticker)
        temp = {}
        for k in range(len(res)):
            weight = res[k][0]
            if 'optimal_weight' in list(weight.columns):
                te = cal_tracking_error(pct, bench, weight) # è·Ÿè¸ªè¯¯å·®è®¡ç®—
                weight = weight[weight['optimal_weight'] > 1e-8]
            else:
                te = np.nan
            temp[str(res[k][1])] = {"weight": weight, "status": res[k][2], "tracking_error": te}
        toc = time.time()
        results[date] = temp
        
    # å­˜å‚¨
    with open(path + 'ZZ500_weight.pkl', 'wb') as f:
        pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)
    toc = time.time()
    print ("\n ----- Computation time = " + str((toc - tic)) + "s")
    
with open(path + 'HS300_weight.pkl') as f:
    pickle_300 = pickle.load(f)
with open(path + 'ZZ500_weight.pkl') as f:
    pickle_500 = pickle.load(f)

# æ²ªæ·±300
static_dev = '0.02'
target_te = 0.03
static_port_300, dynamic_port_300 = portfolio_get(pickle_300, static_dev, target_te)
# ä¸­è¯500
static_dev = '0.005'
target_te = 0.035
static_port_500, dynamic_port_500 = portfolio_get(pickle_500, static_dev, target_te)


# è·Ÿè¸ªè¯¯å·®è¯´æ˜ï¼Œä»¥é™æ€ä¸­è¯500ä¸ºä¾‹
factor = static_port_500.copy()
factor = factor.pivot_table(index='date', columns='secID', values='optimal_weight') # é™æ€ä¸­è¯500


# é™æ€ä¸­è¯500ç»„åˆå›æµ‹
start = '2009-12-31'                       # å›æµ‹èµ·å§‹æ—¶é—´
end = '2018-08-31'                         # å›æµ‹ç»“æŸæ—¶é—´

benchmark = 'ZZ500'                        # ç­–ç•¥å‚è€ƒæ ‡å‡†
universe = DynamicUniverse('A')        # è¯åˆ¸æ± ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒåŸºé‡‘
capital_base = 10000000                    # èµ·å§‹èµ„é‡‘
freq = 'd'                                 # ç­–ç•¥ç±»å‹ï¼Œ'd'è¡¨ç¤ºæ—¥é—´ç­–ç•¥ä½¿ç”¨æ—¥çº¿å›æµ‹
refresh_rate = 1                          # è°ƒä»“é¢‘ç‡ï¼Œè¡¨ç¤ºæ‰§è¡Œhandle_dataçš„æ—¶é—´é—´éš”

factor_dates = factor.index.values
  
commission = Commission(0.001, 0.001)     # äº¤æ˜“è´¹ç‡è®¾ä¸ºåŒè¾¹åƒåˆ†ä¹‹äºŒ

def initialize(account):                   # åˆå§‹åŒ–è™šæ‹Ÿè´¦æˆ·çŠ¶æ€
    pass

def handle_data(account):                  # æ¯ä¸ªäº¤æ˜“æ—¥çš„ä¹°å…¥å–å‡ºæŒ‡ä»¤
    pre_date = account.previous_date.strftime("%Y-%m-%d")
    if pre_date not in factor_dates:            # å› å­åªåœ¨æ¯ä¸ªæœˆåº•è®¡ç®—ï¼Œæ‰€ä»¥è°ƒä»“ä¹Ÿåœ¨æ¯æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥è¿›è¡Œ
        return
    
    # æ‹¿å–è°ƒä»“æ—¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„å› å­ï¼Œå¹¶æŒ‰ç…§ç›¸åº”ååˆ†ä½é€‰æ‹©è‚¡ç¥¨
    wts = pd.Series(dict(factor.ix[pre_date, account.universe].dropna()))
    wts = dict(wts)
        
    # äº¤æ˜“éƒ¨åˆ†
    sell_list = [stk for stk in account.security_position if stk not in wts]
    for stk in sell_list:
        account.order_to(stk, 0)

    c = account.reference_portfolio_value
    change = {}
    for stock, w in wts.iteritems():
        p = account.reference_price[stock]
        if not np.isnan(p) and p > 0:
            change[stock] = int(c * w / p) - account.security_position.get(stock, 0)

    for stock in sorted(change, key=change.get):
        account.order(stock, change[stock])
        
        
# é™æ€ä¸­è¯500å¢å¼ºç»„åˆçš„æ»šåŠ¨ä¸‰ä¸ªæœˆå¹´åŒ–è·Ÿè¸ªè¯¯å·®å›¾
df = bt[['tradeDate', 'portfolio_value', 'benchmark_return']]
df['tradeDate'] = df['tradeDate'].apply(lambda x: x.strftime('%Y%m%d'))
df['return'] = df['portfolio_value'].pct_change()
df = df[1: ]
df['excess_return'] = df['return'] - df['benchmark_return']
date_list = sorted(results.keys())
actual_te = []
for i in range(3, len(date_list)):
    start = date_list[i - 3]
    end = date_list[i]
    temp =  df[(df['tradeDate'] >= start) & (df['tradeDate'] <= end)]
    actual_te.append(temp['excess_return'].std() * np.sqrt(252))
actual_te = pd.DataFrame({'date': date_list[3: ], 'actual_te': actual_te})
actual_te['date'] = pd.to_datetime(actual_te['date'])
ax = plt.plot(actual_te['date'], actual_te['actual_te'])

'''


è·Ÿè¸ªè¯¯å·®çº¦æŸç»“æœ
ä¸Šå›¾æ˜¯ä¸Šè¿°ä¸­è¯500é™æ€å¢å¼ºç»„åˆåœ¨ä¸ªè‚¡åç¦»åº¦ä¸º0.5%æ¡ä»¶ä¸‹çš„çš„å®é™…æ»šåŠ¨ä¸‰ä¸ªæœˆå¹´åŒ–è·Ÿè¸ªè¯¯å·®ï¼Œè®¡ç®—å¯çŸ¥ï¼Œç»„åˆæ€»ä½“çš„è·Ÿè¸ªè¯¯å·®ä¸º4.45%ï¼Œ2011å¹´çš„è·Ÿè¸ªè¯¯å·®ä»…æœ‰2.93%ï¼Œä½†2015å¹´çš„è·Ÿè¸ªè¯¯å·®å´è¾¾åˆ°7.14%ï¼Œå› æ­¤åœ¨ä¸åŒçš„å¸‚åœºçŠ¶æ€ä¸‹è®¾ç½®ç›¸åŒçš„è·Ÿè¸ªè¯¯å·®çº¦æŸå‚æ•°ä¼šå¯¼è‡´ç»„åˆçš„å®é™…è·Ÿè¸ªè¯¯å·®åŠ¨æ€å˜åŒ–ï¼Œè¿™ä¼šä½¿å¾—ç»„åˆåœ¨å¤§éƒ¨åˆ†çš„æ—¶é—´å†…æ»¡è¶³è·Ÿè¸ªè¯¯å·®å°äºTEçš„çº¦æŸï¼Œä½†æ˜¯åœ¨æç«¯è¡Œæƒ…ä¸‹çš„é£é™©æ§åˆ¶å°±æ— æ³•ä¿è¯äº†ã€‚

   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
3.2 åŸºäºè‡ªé€‚åº”é£é™©æ§åˆ¶çš„æŒ‡æ•°å¢å¼ºæ¨¡å‹

è‡ªé€‚åº”é£é™©æ§åˆ¶
ä¸Šè¿°çš„é™æ€æ¨¡å‹åœ¨ç›¸åŒçš„çº¦æŸä¸‹ï¼Œåœ¨ä¸åŒçš„å¸‚åœºç¯å¢ƒä¸‹å®ç°çš„è·Ÿè¸ªè¯¯å·®æ˜¯ä¸å°½ç›¸åŒçš„ï¼Œé™æ€çš„ä¸ªè‚¡åç¦»åº¦çº¦æŸå¹¶ä¸èƒ½å®Œç¾åœ°é€‚åº”å¸‚åœºæ³¢åŠ¨çš„å˜åŒ–ã€‚å› æ­¤æˆ‘ä»¬å‚è€ƒå¤©é£è¯åˆ¸çš„åšæ³•ï¼Œé‡‡å–äº†ä¸€ç§è‡ªé€‚åº”çš„è·Ÿè¸ªè¯¯å·®çº¦æŸæ–¹æ³•ï¼Œæ ¹æ®ç»„åˆè¿‡å»ä¸€æ®µæ—¶é—´å†…ä»¥ä¸åŒçš„ä¸ªè‚¡æƒé‡åç¦»çº¦æŸå¾—åˆ°çš„ç»„åˆå®é™…è·Ÿè¸ªè¯¯å·®ä¸é¢„æœŸè·Ÿè¸ªè¯¯å·®çš„å…³ç³»æ¥åŠ¨æ€åœ°è‡ªé€‚åº”åœ°ç¡®å®šæ¯æœŸè°ƒä»“æ—¶çš„ä¸ªè‚¡æƒé‡åç¦»åº¦çº¦æŸï¼Œå…·ä½“è€Œè¨€ï¼š

åœ¨Tæœˆåº•å»ºä»“æ—¶ï¼Œé¦–å…ˆè®¡ç®—[T-3, T]æœˆæ—¶é—´å†…ä»¥ä¸ªè‚¡æƒé‡åç¦»åº¦w_iä¼˜åŒ–å¾—åˆ°çš„ç»„åˆçš„å¹´åŒ–è·Ÿè¸ªè¯¯å·®TE_iï¼›
å¯¹äºç»™å®šçš„ç›®æ ‡è·Ÿè¸ªè¯¯å·®TE_targetï¼Œæ‰¾åˆ°æ»¡è¶³TE_k <= TE_targetçš„ä¸ªè‚¡æƒé‡åç¦»åº¦çš„æœ€å¤§å€¼w_kä½œä¸ºTæœˆåº•çš„ä¸ªè‚¡æƒé‡åç¦»åº¦çº¦æŸæ¡ä»¶ã€‚

ä¸Šå›¾æ˜¯æ–‡ä¸­çš„é™æ€ä¸­è¯500å¢å¼ºç»„åˆåœ¨0.1%-0.5%åˆ†äº”æ¡£ä¸ªè‚¡æƒé‡åç¦»åº¦çº¦æŸä¸‹çš„å®é™…å¹´åŒ–è·Ÿè¸ªè¯¯å·®å›¾ï¼Œç»™å®šçš„è·Ÿè¸ªè¯¯å·®çº¦æŸä¸º3.5%ï¼Œæˆ‘ä»¬ä»¥ä¸Šå›¾åˆ†èŠ‚ç‚¹è¯´æ˜ï¼š

æ•´ä½“æ¥çœ‹ï¼Œä¸ªè‚¡æƒé‡åç¦»åº¦è¶Šå®½ï¼Œåˆ™ç»„åˆçš„å®é™…è·Ÿè¸ªè¯¯å·®è¶Šå¤§ï¼›
åœ¨20131231æ—¶ï¼Œä»¥æœ€å¤§åç¦»0.2%çš„ç»„åˆè¿‡å»3ä¸ªæœˆçš„å¹´åŒ–è·Ÿè¸ªè¯¯å·®ä¸º3.06%ï¼Œä»¥0.3%ä¸ºçº¦æŸçš„ç»„åˆè¿‡å»3ä¸ªæœˆçš„å¹´åŒ–è·Ÿè¸ªè¯¯å·®ä¸º3.88%ï¼Œå› æ­¤åœ¨å½“æœŸçº¦æŸè·Ÿè¸ªè¯¯å·®æ—¶ï¼Œæˆ‘ä»¬ä»¥0.2%ä½œä¸ºä¸ªè‚¡æƒé‡æœ€å¤§åç¦»çš„çº¦æŸæ¥æ±‚è§£ä¸‹ä¸€æœŸç»„åˆï¼›
åœ¨20150731æ—¶ï¼Œä»¥æœ€å¤§åç¦»0.1%çš„ç»„åˆè¿‡å»3ä¸ªæœˆçš„å¹´åŒ–è·Ÿè¸ªè¯¯å·®ä¸º3.76%ï¼Œå…¶ä»–çº¦æŸä¸‹çš„è·Ÿè¸ªè¯¯å·®éƒ½é«˜äº4%ï¼Œå› æ­¤åœ¨å½“æœŸæˆ‘ä»¬ä»¥0.1%ä½œä¸ºä¸ªè‚¡æƒé‡æœ€å¤§åç¦»çº¦æŸï¼›
åœ¨20170630æ—¶ï¼Œä»¥0.5%ä¸ºçº¦æŸçš„ç»„åˆè¿‡å»3ä¸ªæœˆçš„å¹´åŒ–è·Ÿè¸ªè¯¯å·®ä¸º3.39%ï¼Œå…¶ä»–çº¦æŸä¸‹çš„è·Ÿè¸ªè¯¯å·®éƒ½ä½äº3%ï¼Œå› æ­¤åœ¨å½“æœŸæˆ‘ä»¬ä»¥0.5%ä½œä¸ºä¸ªè‚¡æƒé‡æœ€å¤§åç¦»çº¦æŸã€‚
   è°ƒè¯• è¿è¡Œ
æ–‡æ¡£
 ä»£ç   ç­–ç•¥  æ–‡æ¡£
3.3 ç»„åˆå›æµ‹åŠåˆ†æ

'''


with open(path + 'HS300_weight.pkl') as f:
    pickle_300 = pickle.load(f)
with open(path + 'ZZ500_weight.pkl') as f:
    pickle_500 = pickle.load(f)

# æ²ªæ·±300
static_dev = '0.02'
target_te = 0.03
static_port_300, dynamic_port_300 = portfolio_get(pickle_300, static_dev, target_te)
# ä¸­è¯500
static_dev = '0.005'
target_te = 0.035
static_port_500, dynamic_port_500 = portfolio_get(pickle_500, static_dev, target_te)


def plot_under_water(bt, title):
    """
    ç»˜åˆ¶å›æ’¤åŠæ”¶ç›Šç‡æ›²çº¿å›¾ï¼Œè¾“å‡ºç­–ç•¥æŒ‡æ ‡
    è¾“å…¥ï¼š
        btï¼šquartzå›æµ‹ç»“æŸè‡ªåŠ¨ç”Ÿæˆçš„dict
        titleï¼šstr
    è¿”å›ï¼š
        axï¼šmatplotlib figure å¯¹è±¡
        df_ratioï¼šç­–ç•¥æŒ‡æ ‡
    """
    bt_quantile_ten = bt.copy()
    data = bt_quantile_ten[[u'tradeDate',u'portfolio_value',u'benchmark_return']]
    data['portfolio_return'] = data.portfolio_value/data.portfolio_value.shift(1) - 1.0
    data['portfolio_return'].ix[0] = data['portfolio_value'].ix[0]/	10000000.0 - 1.0
    data['excess_return'] = data.portfolio_return - data.benchmark_return
    data['excess'] = data.excess_return + 1.0
    data['excess'] = data.excess.cumprod()
    # æŒ‡æ ‡è®¡ç®—
    df = data.copy()
    df = df[['tradeDate', 'excess_return', 'excess']]
    df.columns = ['tradeDate', 'rtn', 'capital']
    df['tradeDate'] = df['tradeDate'].apply(lambda x: x.strftime('%Y%m%d'))
    df.sort_values(by='tradeDate', inplace=True)
    df.reset_index(drop=True, inplace=True)
    annual = pow((df.ix[len(df.index) - 1, 'capital']) / df.ix[0, 'capital'], 250.0 / len(df)) - 1 # å¹´åŒ–æ”¶ç›Š
    volatility = df['rtn'].std() * np.sqrt(250)
    df['max2here'] = df['capital'].expanding(min_periods=1).max()
    df['dd2here'] = df['capital'] / df['max2here'] - 1
    temp = df.sort_values(by='dd2here').iloc[0][['tradeDate', 'dd2here']]
    max_dd = temp['dd2here'] # æœ€å¤§å›æ’¤
    end_date = temp['tradeDate']
    df = df[df['tradeDate'] <= end_date]
    start_date = df.sort_values(by='capital', ascending=False).iloc[0]['tradeDate']
    sharpe = annual / volatility # å¤æ™®æ¯”ç‡
    rtn_ratio = annual / np.abs(max_dd) # æ”¶ç›Šå›æ’¤æ¯”
    df_ratio = pd.DataFrame({u'ç­–ç•¥': [title], u'å¹´åŒ–è¶…é¢æ”¶ç›Š': [annual], u'ç›¸å¯¹æœ€å¤§å›æ’¤': [max_dd], u'æ”¶ç›Šå›æ’¤æ¯”': rtn_ratio,
                             u'æœ€å¤§å›æ’¤èµ·å§‹': start_date, u'æœ€å¤§å›æ’¤ç»“æŸ': end_date, u'è·Ÿè¸ªè¯¯å·®': volatility, u'å¤æ™®æ¯”ç‡': sharpe})
    # ç”»å›¾
    df_cum_rets = data['excess']
    running_max = np.maximum.accumulate(df_cum_rets)
    underwater = -((running_max - df_cum_rets) / running_max)
    underwater.index = data['tradeDate']

    fig = plt.figure(figsize=(12, 5))
    ax1 = fig.add_subplot(111)
    ax2 = ax1.twinx()
    x = range(len(underwater))
    ax2.grid(False)
    ax1.set_ylim(-0.30, 0)
    ax1.set_ylabel(u'å›æ’¤', fontproperties=font, fontsize=16)
    ax1.fill_between(underwater.index, 0, np.array(underwater), color='#000066', alpha=1)
    ax2.set_ylabel(u'å‡€å€¼', fontproperties=font, fontsize=16)
    ax2.plot(data['tradeDate'], data[['excess']], label='hedged(right)', color='r')
    ax2.set_ylim(bottom=0.9, top=5)
    s = ax1.set_title(title, fontproperties=font, fontsize=16)
    return fig, df_ratio

# å°†æ•°æ®å¤„ç†æˆä¼˜çŸ¿å›æµ‹æ‰€éœ€æ ¼å¼
factor1 = static_port_300.copy()
factor1 = factor1.pivot_table(index='date', columns='secID', values='optimal_weight') # é™æ€æ²ªæ·±300
factor2 = dynamic_port_300.copy()
factor2 = factor2.pivot_table(index='date', columns='secID', values='optimal_weight') # åŠ¨æ€æ²ªæ·±300
factor3 = static_port_500.copy()
factor3 = factor3.pivot_table(index='date', columns='secID', values='optimal_weight') # é™æ€ä¸­è¯500
factor4 = dynamic_port_500.copy()
factor4 = factor4.pivot_table(index='date', columns='secID', values='optimal_weight') # åŠ¨æ€ä¸­è¯500


# é™æ€æ²ªæ·±300ç»„åˆå›æµ‹
start = '2009-12-31'                       # å›æµ‹èµ·å§‹æ—¶é—´
end = '2018-08-31'                         # å›æµ‹ç»“æŸæ—¶é—´

benchmark = 'HS300'                        # ç­–ç•¥å‚è€ƒæ ‡å‡†
universe = DynamicUniverse('A')        # è¯åˆ¸æ± ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒåŸºé‡‘
capital_base = 10000000                    # èµ·å§‹èµ„é‡‘
freq = 'd'                                 # ç­–ç•¥ç±»å‹ï¼Œ'd'è¡¨ç¤ºæ—¥é—´ç­–ç•¥ä½¿ç”¨æ—¥çº¿å›æµ‹
refresh_rate = 1                          # è°ƒä»“é¢‘ç‡ï¼Œè¡¨ç¤ºæ‰§è¡Œhandle_dataçš„æ—¶é—´é—´éš”

factor_dates = factor1.index.values
  
commission = Commission(0.001, 0.001)     # äº¤æ˜“è´¹ç‡è®¾ä¸ºåŒè¾¹åƒåˆ†ä¹‹äºŒ

def initialize(account):                   # åˆå§‹åŒ–è™šæ‹Ÿè´¦æˆ·çŠ¶æ€
    pass

def handle_data(account):                  # æ¯ä¸ªäº¤æ˜“æ—¥çš„ä¹°å…¥å–å‡ºæŒ‡ä»¤
    pre_date = account.previous_date.strftime("%Y-%m-%d")
    if pre_date not in factor_dates:            # å› å­åªåœ¨æ¯ä¸ªæœˆåº•è®¡ç®—ï¼Œæ‰€ä»¥è°ƒä»“ä¹Ÿåœ¨æ¯æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥è¿›è¡Œ
        return
    
    # æ‹¿å–è°ƒä»“æ—¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„å› å­ï¼Œå¹¶æŒ‰ç…§ç›¸åº”ååˆ†ä½é€‰æ‹©è‚¡ç¥¨
    wts = pd.Series(dict(factor1.ix[pre_date, account.universe].dropna()))
    wts = dict(wts)
    
    # äº¤æ˜“éƒ¨åˆ†
    sell_list = [stk for stk in account.security_position if stk not in wts]
    for stk in sell_list:
        account.order_to(stk, 0)

    c = account.reference_portfolio_value
    change = {}
    for stock, w in wts.iteritems():
        p = account.reference_price[stock]
        if not np.isnan(p) and p > 0:
            change[stock] = int(c * w / p) - account.security_position.get(stock, 0)

    for stock in sorted(change, key=change.get):
        account.order(stock, change[stock])
        
        


bt1 = bt.copy()
bt1.to_csv(path + 'static_300.csv', index=False)
figure1, ratio1 = plot_under_water(bt1, u'é™æ€æ²ªæ·±300')

# åŠ¨æ€æ²ªæ·±300ç»„åˆå›æµ‹ï¼Œè·Ÿè¸ªè¯¯å·®ä¸Šé™è®¾ç½®ä¸º3%
start = '2009-12-31'                       # å›æµ‹èµ·å§‹æ—¶é—´
end = '2018-08-31'                         # å›æµ‹ç»“æŸæ—¶é—´

benchmark = 'HS300'                        # ç­–ç•¥å‚è€ƒæ ‡å‡†
universe = DynamicUniverse('A')        # è¯åˆ¸æ± ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒåŸºé‡‘
capital_base = 10000000                    # èµ·å§‹èµ„é‡‘
freq = 'd'                                 # ç­–ç•¥ç±»å‹ï¼Œ'd'è¡¨ç¤ºæ—¥é—´ç­–ç•¥ä½¿ç”¨æ—¥çº¿å›æµ‹
refresh_rate = 1                          # è°ƒä»“é¢‘ç‡ï¼Œè¡¨ç¤ºæ‰§è¡Œhandle_dataçš„æ—¶é—´é—´éš”

factor_dates = factor2.index.values
  
commission = Commission(0.001, 0.001)     # äº¤æ˜“è´¹ç‡è®¾ä¸ºåŒè¾¹åƒåˆ†ä¹‹äºŒ

def initialize(account):                   # åˆå§‹åŒ–è™šæ‹Ÿè´¦æˆ·çŠ¶æ€
    pass

def handle_data(account):                  # æ¯ä¸ªäº¤æ˜“æ—¥çš„ä¹°å…¥å–å‡ºæŒ‡ä»¤
    pre_date = account.previous_date.strftime("%Y-%m-%d")
    if pre_date not in factor_dates:            # å› å­åªåœ¨æ¯ä¸ªæœˆåº•è®¡ç®—ï¼Œæ‰€ä»¥è°ƒä»“ä¹Ÿåœ¨æ¯æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥è¿›è¡Œ
        return
    
    # æ‹¿å–è°ƒä»“æ—¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„å› å­ï¼Œå¹¶æŒ‰ç…§ç›¸åº”ååˆ†ä½é€‰æ‹©è‚¡ç¥¨
    wts = pd.Series(dict(factor2.ix[pre_date, account.universe].dropna()))
    wts = dict(wts)

    # äº¤æ˜“éƒ¨åˆ†
    sell_list = [stk for stk in account.security_position if stk not in wts]
    for stk in sell_list:
        account.order_to(stk, 0)

    c = account.reference_portfolio_value
    change = {}
    for stock, w in wts.iteritems():
        p = account.reference_price[stock]
        if not np.isnan(p) and p > 0:
            change[stock] = int(c * w / p) - account.security_position.get(stock, 0)

    for stock in sorted(change, key=change.get):
        account.order(stock, change[stock])
        
        

'''
ç”±æ­¤ï¼Œæˆ‘ä»¬åŸºäºé™æ€æ–¹æ³•ä¸è‡ªé€‚åº”é£é™©æ§åˆ¶æ–¹æ³•åˆ†åˆ«å¯¹æ²ªæ·±300ã€ä¸­è¯500æŒ‡æ•°æ„å»ºæŒ‡æ•°å¢å¼ºç»„åˆï¼Œç»„åˆå›æµ‹ç»“æœå¦‚ä¸‹

'''

bt2 = bt.copy()
bt2.to_csv(path + 'dynamic_300.csv', index=False)
figure2, ratio2 = plot_under_water(bt2, u'åŠ¨æ€æ²ªæ·±300')

# é™æ€ä¸­è¯500ç»„åˆå›æµ‹
start = '2009-12-31'                       # å›æµ‹èµ·å§‹æ—¶é—´
end = '2018-08-31'                         # å›æµ‹ç»“æŸæ—¶é—´

benchmark = 'ZZ500'                        # ç­–ç•¥å‚è€ƒæ ‡å‡†
universe = DynamicUniverse('A')        # è¯åˆ¸æ± ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒåŸºé‡‘
capital_base = 10000000                    # èµ·å§‹èµ„é‡‘
freq = 'd'                                 # ç­–ç•¥ç±»å‹ï¼Œ'd'è¡¨ç¤ºæ—¥é—´ç­–ç•¥ä½¿ç”¨æ—¥çº¿å›æµ‹
refresh_rate = 1                          # è°ƒä»“é¢‘ç‡ï¼Œè¡¨ç¤ºæ‰§è¡Œhandle_dataçš„æ—¶é—´é—´éš”

factor_dates = factor3.index.values
  
commission = Commission(0.001, 0.001)     # äº¤æ˜“è´¹ç‡è®¾ä¸ºåŒè¾¹åƒåˆ†ä¹‹äºŒ

def initialize(account):                   # åˆå§‹åŒ–è™šæ‹Ÿè´¦æˆ·çŠ¶æ€
    pass

def handle_data(account):                  # æ¯ä¸ªäº¤æ˜“æ—¥çš„ä¹°å…¥å–å‡ºæŒ‡ä»¤
    pre_date = account.previous_date.strftime("%Y-%m-%d")
    if pre_date not in factor_dates:            # å› å­åªåœ¨æ¯ä¸ªæœˆåº•è®¡ç®—ï¼Œæ‰€ä»¥è°ƒä»“ä¹Ÿåœ¨æ¯æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥è¿›è¡Œ
        return
    
    # æ‹¿å–è°ƒä»“æ—¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„å› å­ï¼Œå¹¶æŒ‰ç…§ç›¸åº”ååˆ†ä½é€‰æ‹©è‚¡ç¥¨
    wts = pd.Series(dict(factor3.ix[pre_date, account.universe].dropna()))
    wts = dict(wts)

    # äº¤æ˜“éƒ¨åˆ†
    sell_list = [stk for stk in account.security_position if stk not in wts]
    for stk in sell_list:
        account.order_to(stk, 0)

    c = account.reference_portfolio_value
    change = {}
    for stock, w in wts.iteritems():
        p = account.reference_price[stock]
        if not np.isnan(p) and p > 0:
            change[stock] = int(c * w / p) - account.security_position.get(stock, 0)

    for stock in sorted(change, key=change.get):
        account.order(stock, change[stock])
        
        
bt3 = bt.copy()
bt3.to_csv(path + 'static_500.csv', index=False)
figure3, ratio3 = plot_under_water(bt3, u'é™æ€ä¸­è¯500')


# åŠ¨æ€ä¸­è¯500ç»„åˆå›æµ‹ï¼Œè·Ÿè¸ªè¯¯å·®ä¸Šé™è®¾ç½®ä¸º3.5%
start = '2009-12-31'                       # å›æµ‹èµ·å§‹æ—¶é—´
end = '2018-08-31'                         # å›æµ‹ç»“æŸæ—¶é—´

benchmark = 'ZZ500'                        # ç­–ç•¥å‚è€ƒæ ‡å‡†
universe = DynamicUniverse('A')        # è¯åˆ¸æ± ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒåŸºé‡‘
capital_base = 10000000                    # èµ·å§‹èµ„é‡‘
freq = 'd'                                 # ç­–ç•¥ç±»å‹ï¼Œ'd'è¡¨ç¤ºæ—¥é—´ç­–ç•¥ä½¿ç”¨æ—¥çº¿å›æµ‹
refresh_rate = 1                          # è°ƒä»“é¢‘ç‡ï¼Œè¡¨ç¤ºæ‰§è¡Œhandle_dataçš„æ—¶é—´é—´éš”

factor_dates = factor4.index.values
  
commission = Commission(0.001, 0.001)     # äº¤æ˜“è´¹ç‡è®¾ä¸ºåŒè¾¹åƒåˆ†ä¹‹äºŒ

def initialize(account):                   # åˆå§‹åŒ–è™šæ‹Ÿè´¦æˆ·çŠ¶æ€
    pass

def handle_data(account):                  # æ¯ä¸ªäº¤æ˜“æ—¥çš„ä¹°å…¥å–å‡ºæŒ‡ä»¤
    pre_date = account.previous_date.strftime("%Y-%m-%d")
    if pre_date not in factor_dates:            # å› å­åªåœ¨æ¯ä¸ªæœˆåº•è®¡ç®—ï¼Œæ‰€ä»¥è°ƒä»“ä¹Ÿåœ¨æ¯æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥è¿›è¡Œ
        return
    
    # æ‹¿å–è°ƒä»“æ—¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„å› å­ï¼Œå¹¶æŒ‰ç…§ç›¸åº”ååˆ†ä½é€‰æ‹©è‚¡ç¥¨
    wts = pd.Series(dict(factor4.ix[pre_date, account.universe].dropna()))
    wts = dict(wts)

    # äº¤æ˜“éƒ¨åˆ†
    sell_list = [stk for stk in account.security_position if stk not in wts]
    for stk in sell_list:
        account.order_to(stk, 0)

    c = account.reference_portfolio_value
    change = {}
    for stock, w in wts.iteritems():
        p = account.reference_price[stock]
        if not np.isnan(p) and p > 0:
            change[stock] = int(c * w / p) - account.security_position.get(stock, 0)

    for stock in sorted(change, key=change.get):
        account.order(stock, change[stock])
        
bt4 = bt.copy()
bt4.to_csv(path + 'dynamic_500.csv', index=False)
figure4, ratio4 = plot_under_water(bt4, u'åŠ¨æ€ä¸­è¯500')

# å›æµ‹ç»“æœæ•´åˆ
ratio = pd.concat([ratio1, ratio2, ratio3, ratio4], axis=0)
ratio = ratio[[u'ç­–ç•¥', u'å¹´åŒ–è¶…é¢æ”¶ç›Š', u'è·Ÿè¸ªè¯¯å·®', u'å¤æ™®æ¯”ç‡', u'æ”¶ç›Šå›æ’¤æ¯”', u'ç›¸å¯¹æœ€å¤§å›æ’¤', u'æœ€å¤§å›æ’¤èµ·å§‹', u'æœ€å¤§å›æ’¤ç»“æŸ']]
print('***********ç»„åˆç»“æœå¯¹æ¯”************')
print(ratio.round(4).to_html())

'''
ç»“è®º

ç”±ä¸Šè¿°æŒ‡æ•°å¢å¼ºç»„åˆçš„è¡¨ç°å¾—åˆ°å¦‚ä¸‹ç»“è®ºï¼š

åœ¨è‡ªé€‚åº”é£é™©æ§åˆ¶ä¸‹ï¼Œæ²ªæ·±300æŒ‡æ•°å¢å¼ºç»„åˆçš„æ”¶ç›Šç•¥å¾®ä¸‹é™ï¼Œå¹´åŒ–è¶…é¢æ”¶ç›Šä»10.7%é™è‡³8.2%ï¼Œä½†æ˜¯è·Ÿè¸ªè¯¯å·®ä»åŸå…ˆçš„4.55%é™è‡³3.31%ï¼Œæ€»ä½“çš„è·Ÿè¸ªè¯¯å·®è¾ƒå¥½åœ°çº¦æŸåœ¨ç›®æ ‡è·Ÿè¸ªè¯¯å·®èŒƒå›´å·¦å³ï¼ˆç›®æ ‡è·Ÿè¸ªè¯¯å·®3%ï¼‰ï¼Œç»„åˆçš„å¤æ™®æ¯”ç‡ç”±2.36ä¸Šå‡è‡³2.48ï¼Œç›¸å¯¹åŸºå‡†çš„æœ€å¤§å›æ’¤å¤§å¹…åº¦é™ä½ï¼ˆ7.36% -> 4.01%ï¼‰ï¼Œè€Œä¸”æˆ‘ä»¬ä»å‡€å€¼æ›²çº¿ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œè‡ªé€‚åº”é£é™©æ§åˆ¶çš„ç»„åˆåœ¨ä»»ä½•å¸‚åœºè¡Œæƒ…ä¸­éƒ½éå¸¸ç¨³å¥ï¼›
åŒç†ï¼Œåœ¨è‡ªé€‚åº”é£é™©æ§åˆ¶ä¸‹ï¼Œä¸­è¯500æŒ‡æ•°å¢å¼ºç»„åˆçš„æ”¶ç›Šä¹Ÿæ˜¯ä¸‹é™çš„ï¼Œè¿™æ˜¯å¼ºåŒ–é£é™©çº¦æŸå¸¦æ¥çš„å¿…ç„¶ç»“æœï¼Œä½†æ˜¯è·Ÿè¸ªè¯¯å·®ä»åŸå…ˆçš„4.44%é™è‡³3.32%ï¼Œèƒ½å¤Ÿå®Œå…¨åœ°çº¦æŸåœ¨ç›®æ ‡è·Ÿè¸ªè¯¯å·®3.5%çš„èŒƒå›´å†…ï¼ŒåŒæ—¶ç›¸å¯¹åŸºå‡†çš„æœ€å¤§å›æ’¤ä¹Ÿé™ä½äº†ï¼ˆ3.53% -> 3.23%ï¼‰ï¼Œæ§åˆ¶äº†åœ¨è¯¸å¦‚2015å¹´æç«¯è¡Œæƒ…ä¸‹çš„è·Ÿè¸ªè¯¯å·®ï¼Œè‡ªé€‚åº”é£é™©æ§åˆ¶ç»„åˆèƒ½é€‚åº”ä»»ä½•è¡Œæƒ…å¹¶è·å–ç¨³å®šè¶…é¢æ”¶ç›Šï¼›
å› æ­¤ï¼Œè‡ªé€‚åº”çš„é£é™©æ§åˆ¶çº¦æŸèƒ½æœ‰æ•ˆåœ°æ§åˆ¶ç»„åˆçš„é£é™©ï¼Œåœ¨ç‰ºç‰²ä¸€å°éƒ¨åˆ†æ”¶ç›Šçš„æƒ…å†µä¸‹å¤§å¹…åº¦æå‡ç»„åˆçš„ç¨³å¥æ€§ï¼Œæ›´å¥½åœ°é€‚åº”å„ç§å¸‚åœºé£æ ¼ã€‚

'''

